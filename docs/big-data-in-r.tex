\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Exploring, Visualizing, and Modeling Big Data with R},
            pdfauthor={Okan Bulut; Christopher Desjardins},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Exploring, Visualizing, and Modeling Big Data with R}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Okan Bulut \\ Christopher Desjardins}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2019-04-04}

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preface}{%
\chapter{Preface}\label{preface}}

\includegraphics{images/Cover2.jpg}

\hypertarget{summary}{%
\section{Summary}\label{summary}}

Working with \textbf{BIG DATA} requires a particular suite of data analytics tools and advanced techniques, such as machine learning (ML). Many of these tools are readily and freely available in \href{https://cran.r-project.org/}{R}. This full-day session will provide participants with a hands-on training on how to use data analytics tools and machine learning methods available in R to explore, visualize, and model big data.

The first half of our training session will focus on organizing (manipulating and summarizing) and visualizing (both statically and dynamically) big data in R. The second half will involve a series of short lectures on ML techniques (decision trees, random forests, and support vector machines), as well as hands-on demonstrations applying these methods in R. Examples will be drawn from the OECD's \href{http://www.oecd.org/pisa/}{Programme for International Student Assessment (PISA)}. Participants will get opportunities to work through several hands-on lab sessions throughout the day.

\hypertarget{who-we-are}{%
\section{Who we are}\label{who-we-are}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Okan Bulut -- University of Alberta}

  \begin{itemize}
  \tightlist
  \item
    Associate Professor of educational measurement and psychometrics at the University of Alberta
  \item
    10+ years using the R programming language for data analysis and visualization
  \item
    Specialized in the analysis and visualization of big data (mostly from large-scale assessments)
  \item
    6+ years teaching courses and workshops on statistics, psychometrics, and programming with R
  \item
    \url{https://github.com/okanbulut} and \url{https://sites.ualberta.ca/~bulut/}
  \item
    \textbf{E-mail:} \href{mailto:bulut@ualberta.ca}{\nolinkurl{bulut@ualberta.ca}}
  \end{itemize}
\item
  \textbf{Christopher D. Desjardins -- University of Minnesota}

  \begin{itemize}
  \tightlist
  \item
    Research Associate at the Research Methodology Consulting Center.
  \item
    R user since 2006 and contributer since 2009.
  \item
    \url{https://github.com/cddesja} and \url{https://cddesja.github.io/}
  \item
    \textbf{E-mail:} \href{mailto:cdesjard@umn.edu}{\nolinkurl{cdesjard@umn.edu}}
  \end{itemize}
\end{enumerate}

We also co-authored:

\begin{itemize}
\tightlist
\item
  Two R packages -- \href{https://cran.r-project.org/web/packages/profileR/index.html}{profileR} for profile analysis of multivariate data and \href{https://github.com/cddesja/hemp}{hemp} for psychometric analysis of assessment data
\item
  A recent book titled \href{https://www.crcpress.com/Handbook-of-Educational-Measurement-and-Psychometrics-Using-R/Desjardins-Bulut/p/book/9781498770132}{Handbook of Educational Measurement and Psychometrics Using R}
\end{itemize}

\hypertarget{intro}{%
\chapter{Introduction}\label{intro}}

\hypertarget{what-is-big-data}{%
\section{What is big data?}\label{what-is-big-data}}

When we handle a data-related problem, how do we know that we are actually dealing with ``big data''? What is ``big data''? What characteristics make a dataset big? The following three characteristics (three Vs of big data, source: \href{https://whatis.techtarget.com/definition/3Vs}{TechTarget}) can help us define the size of data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Volume}: The number of rows or cases (e.g., students) and the number of columns or variables (e.g., age, gender, student responses, response times)
\item
  \textbf{Variety}: Whether there are secondary sources or data that expand the existing data even further
\item
  \textbf{Velocity}: Whether real-time data are being used
\end{enumerate}

\begin{figure}
\includegraphics[width=1\linewidth]{images/BigData} \caption{Three Vs of big data}\label{fig:fig1-1}
\end{figure}

\hypertarget{why-is-big-data-important}{%
\section{Why is big data important?}\label{why-is-big-data-important}}

Nowadays nearly every private and public sector of industry, commerce, health, education, and so forth are talking about big data. Data is a strategic and valuable asset when we know which questions we want to answer (see Bernard Marr's article titled \href{https://www.forbes.com/sites/bernardmarr/2015/08/25/big-data-too-many-answers-not-enough-questions/\#527635fb1361}{Big Data: Too Many Answers, Not Enough Questions}). Therefore, it is very important to identify the right questions at the beginning of data collection. More data with appropriate questions can yield quality answers that we can use for better decision-making. However, too much data without any purpose may obfuscate the truth.

Currently big data is used to better understand customers and their behaviors and preferences. Consider \href{https://www.netflix.com}{Netflix} -- one of the world's leading subscription services for watching movies and TV shows online. They use big data -- such as customers' ratings for each movie and TV show and when customers subscribe/unsubscribe -- to make better recommendations for existing customers and to convince more customers to subscribe. Target, a big retailer in the US, implements data mining techniques to predict pregnancies of their shoppers and send them a sale booklet for baby clothes, cribs, and diapers (see this interesting \href{https://www.driveresearch.com/single-post/2016/12/06/How-Target-Used-Data-Analytics-to-Predict-Pregnancies}{article}). Car insurance companies analyze big data from their customers to understand how well their customers actually drive and how much they need to charge each customer to make a profit.

In education, there is no shortage of big data. Student records, teacher observations, assessment results, and other student-related databases make tons of information available to researchers and practitioners. With the advent of new technologies such as \href{https://www.edweek.org/ew/articles/2016/01/13/the-future-of-big-data-and-analytics.html}{facial recognition software} and \href{https://www.smartdatacollective.com/jay-z-kanye-west-used-biometrics-beat-album-leaks/}{biometric signals}, now we get access to a variety of visual and audio data on students. In the context of educational testing and psychometrics, big data can help us to assess students more accurately, while continuously monitoring their progress via \href{https://isit.arts.ubc.ca/learning-analytics-examples/}{learning analytics}. We can use \href{https://link.springer.com/content/pdf/10.1007\%2Fs41237-018-0063-y.pdf}{log data} and response times to understand students' engagement with the test, whether they were cheating, and whether they had pre-knowledge of the items presented on the test.

\hypertarget{how-do-we-analyze-big-data}{%
\section{How do we analyze big data?}\label{how-do-we-analyze-big-data}}

Big data analysis often begins with reading and then extracting the data. First, we need to read the data into a software program -- such as R -- and then manage it properly. Second, we need to extract a subset, sample, or summary from the big data. Due to its size, even a subset of the big data might itself be quite large. Third, we need to repeat computation (e.g., fitting a model) for many subgroups of the data (e.g., for each individual or by larger groups that combine individuals based on a particular characteristic). Therefore, we need to use the right tools for our data operations. For example, we may need to store big data in a data warehouse (either a local database or a cloud system) and then pass subsets of data from the warehouse to the local machine where we are analyzing the data.

R, maintained by the \href{https://www.r-project.org/contributors.html}{R Core Team}, has its packages (collect of R functions) available on this \href{https://cran.r-project.org/}{The Comprehensive R Archive Network (CRAN)}. It used to be considered an \emph{inadequate} programming language for big data (see Douglass Merril's \href{https://www.forbes.com/sites/douglasmerrill/2012/05/01/r-is-not-enough-for-big-data/\#59c7ad9b5924}{article} from 2012). Fortunately, today's R, with the help of \href{https://www.rstudio.com/}{RStudio} and many data scientists, is capable of running most analytic tasks for big data either alone or with the help of other programs and programming languages, such as \href{https://spark.apache.org/docs/latest/sparkr.html}{Spark}, \href{https://hadoop.apache.org/}{Hadoop}, \href{https://en.wikipedia.org/wiki/SQL}{SQL}, and \href{http://www.cplusplus.com/}{C++} (see Figure \ref{fig:fig1-2}). R is an amazing data science programming tool, it has a myriad statistical techniques available, and can readily translate the results of our analyses into colourful graphics. There is no doubt that R is one of the most preferred programming tool for statisticians, data scientists, and data analysts who deal with big data on a daily basis.

\begin{figure}
\includegraphics[width=1\linewidth]{images/r_and_others} \caption{Other big data programs integrated with R}\label{fig:fig1-2}
\end{figure}

Some general suggestions on big data analysis include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Obtain a strong computer (multiple and faster CPUs, more memory)
\item
  If memory is a problem, access the data differently or split up the data
\item
  Preview a subset of big data using a program, \textbf{not} the entire raw data.
\item
  Visualize either a subset of data or a summary of the big data, \textbf{not} the entire raw data.
\item
  Calculate necessary summary statistics manually, \textbf{not} for all variables in big data.
\item
  Delay computationally expensive operations (e.g., those that require large memory) until you actually need them.
\item
  Consider using parallel computing -- \href{https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf}{parallel} and \href{https://cran.r-project.org/web/packages/foreach/vignettes/foreach.pdf}{foreach} packages and \href{https://rstudio.cloud/}{cloud computing}
\item
  Profile big tasks (in R) to cut down on computational time
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{start_time <-}\StringTok{ }\KeywordTok{proc.time}\NormalTok{()}

\CommentTok{# Do all of your coding here}

\NormalTok{end_time <-}\StringTok{ }\KeywordTok{proc.time}\NormalTok{()}
\NormalTok{end_time }\OperatorTok{-}\StringTok{ }\NormalTok{start_time}

\CommentTok{# Alternatively,}

\KeywordTok{system.time}\NormalTok{(\{}
  
 \CommentTok{# Do all of your coding here }
  
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

During this training session, we will follow these steps and demonstrate how each one helps us explore, visualize, and model big data in R.

\hypertarget{additional-resources}{%
\section{Additional resources}\label{additional-resources}}

There are dozens of online resources and books on big data analysis. Here are a few of them that we recommend you check out:

\begin{itemize}
\item
  James, G., Witten, D., Hastie, T., \& Tibshirani, R. (2017). An introduction to statistical learning with applications in R. New York, NY: Springer. (Freely available from the authors' website: \url{http://www-bcf.usc.edu/~gareth/ISL/index.html})
\item
  Grolemund, G., \& Wickham, H. (2016). R for data science. Sebastopol, CA: O'Reilly Media, Inc. (Freely available from the authors' website: \url{http://r4ds.had.co.nz/})
\item
  Baumer, B. S., Kaplan, D. T., \& Horton, N. J. (2017). \href{https://mdsr-book.github.io/}{Modern data science with R}. Boca Raton, FL: CRC Press.
\item
  Romero, C., Ventura, S., Pechenizkiy, M., \& Baker, de, R. S. J. (Eds.) (2011). Handbook of educational data mining. (Chapman and Hall/CRC data mining and knowledge discovery series). Boca Raton: CRC Press.
\item
  DataCamp: \url{https://www.datacamp.com/tracks/big-data-with-r}
\item
  RStudio: \url{https://www.rstudio.com/resources/webinars/working-with-big-data-in-r/}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{pisa-dataset}{%
\section{PISA dataset}\label{pisa-dataset}}

In this training session, we will use the 2015 administration of the OECD's \href{http://www.oecd.org/pisa/}{Programme for International Student Assessment (PISA)}. PISA is a large-scale, international assessment that involves students, parents, teachers, and school principals from all over the world as participants. Every three years, PISA tests 15-year-old students from all over the world in reading, mathematics and science. The tests are designed to gauge how well the students master key subjects in order to be prepared for real-life situations in the adult world.

In addition to assessing students' competencies, PISA also aims to inform educational policies and practices for the participating countries and economies by providing additional information obtained from students, parents, teachers, and school principals through the questionnaires. Students complete a background questionnaire with questions about themselves, their family and home, and their school and learning experiences. School principals complete a questionnaire about the system and learning environment in schools. In some countries, teachers and parents also complete optional questionnaires to provide more information on their perceptions and expectations regarding students. In this training session, we specifically focus on the assessment data and the background questionnaire that all participating students are required to complete.

The 2015 administration of PISA involves approximately 540,000 15-year-old students from 72 participating countries and economies. During this training session, we will sometimes use the entire dataset or take a subset of the PISA dataset to demonstrate the methods used for exploring, visualizing, and modeling big data. For more details about the PISA dataset and its codebooks, please see \href{http://www.oecd.org/pisa/data/2015database/}{the PISA website}.

The three data files that we will use in this training session can be downloaded using the following links. Please download and unzip the files to follow the examples that we will demonstrate in this training session.

\begin{itemize}
\tightlist
\item
  \url{http://bit.ly/2VleDPZ} (all PISA records -- 331.65 MB)
\item
  \url{http://bit.ly/2Uf2mQA} (only 6 regions with 17 countries -- 103.76 MB)
\item
  \url{http://bit.ly/2YNzei0} (randomly selected cases from 6 regions -- 22.92 MB)
\end{itemize}

\hypertarget{eda}{%
\chapter{Exploratory data analysis}\label{eda}}

\hypertarget{what-is-exploratory-data-analysis}{%
\section{What is exploratory data analysis?}\label{what-is-exploratory-data-analysis}}

\begin{figure}
\includegraphics[width=0.4\linewidth]{images/tukey} \caption{The EDA classic.}\label{fig:tukey}
\end{figure}

\begin{quote}
Exploratory data analysis is detective work -- numerical detective work -- or counting detective work -- or graphical detective work.
\end{quote}

\begin{quote}
To learn about data analysis, it is right that each of us try many things that do not work
\end{quote}

Exploratory data analysis (EDA) is an \textbf{iterative, hypothesis-generating framework}. Through EDA, we hope to \textbf{uncover new relationships} among the variables in our data. In EDA, our job is the accumulation of evidence, preferably novel evidence and the widespread availability of powerful computers and able statistical tools means accumulating evidence is much easier now than ever before. But everything we find may not be meaningful. Nonetheless, our job is not to evaluate what we've learned but rather discovery and EDA ``is an attitude, a state of flexibility, a willingness to look for those things that we believe are not there, as well as those we believe to be there.'' It is for researchers building on our sleuthing to evaluate whether what we've found is real or not, but it's not our focus in EDA. Regardless of the ultimate fate of what we've found, EDA is vital to science as it challenges our dogmas about the relationships among variables and it forces us to face the fact that our theories may be wrong. It pushes science forwards and provides a framework for designing new studies and experiments to confirm/refute what we've found. We're the detectives, not the judges.

\hypertarget{confirmatory-data-analysis}{%
\section{Confirmatory data analysis}\label{confirmatory-data-analysis}}

\begin{quote}
You can only use an observation once to confirm a hypothesis. As soon as you use it more than once you're back to doing exploratory analysis. This means to do hypothesis confirmation you need to ``preregister'' (write out in advance) your analysis plan, and not deviate from it even when you have seen the data. Grolemund \& Wickham, (2017)
\end{quote}

\hypertarget{a-framework-for-eda}{%
\section{A framework for EDA}\label{a-framework-for-eda}}

Figure \ref{fig:edamod} shows a framework for data science presented in Grolemund \& Wickham (2017). This model is equally applicable for EDA.

\begin{figure}
\includegraphics[width=0.8\linewidth]{images/datascienceexplore} \caption{Grolemund and Wickham (2017) model of data science.}\label{fig:edamod}
\end{figure}

The first step in any analysis involves \textbf{importing} the data into the software. Depending on how you import data into R, this step can be relatively instantaneous or it can take minutes.

The second step is \textbf{tidying} or \textbf{reshaping} your data. Grolemund \& Wickham's criteria for tidy data are

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each variable must have its own column.
\item
  Each observation must have its own row.
\item
  Each value must have its own cell.
\end{enumerate}

Note that this does not explicit refer to data being in a wide or long format. Table 11 in Wickham (2014) in the Journal of Statistical Software, shown below in Figure \ref{fig:tidypic}, highlights this issue. In this particular example, without knowing what X and Y are (e.g., do they code for 1) measurements on two occasions? 2) two items on the same assessment? 3) the height and weight of the participants?) we do not know which format is the tidy one.

\begin{figure}
\includegraphics[width=0.6\linewidth]{images/whichtidy} \caption{Table 11 from Wickham (2014). Which data set is tidy?}\label{fig:tidypic}
\end{figure}

The next steps in EDA are iterative and they involve \textbf{transforming} variables (e.g., changing type, rescaling, creating new ones, etc), \textbf{visualizing} these variables (e.g., marginal, bivariate, multivariate plots), and \textbf{modeling} the relationships (can these variables predict/classify our outcomes).

Finally, once an interesting relationship has been discovered it must be \textbf{communicated} and any, arguably all, of these three would be communicated.

\hypertarget{eda-tools}{%
\section{EDA tools}\label{eda-tools}}

\begin{figure}
\includegraphics[width=0.6\linewidth]{images/allfour} \end{figure}

\hypertarget{wrangling-big-data}{%
\chapter{Wrangling big data}\label{wrangling-big-data}}

Data wrangling is a general term that refers to transforming data. Wrangling could involve subsetting, recoding, and transforming variables. For the workshop, we'll also include summarizing data as wrangling as it fits within our discussion of the \texttt{data.table} and \texttt{sparklyr} packages. However, summarizing might more appropriately occur during data exploration/initial data analysis.

\hypertarget{what-is-data.table}{%
\section{\texorpdfstring{What is \texttt{data.table}?}{What is data.table?}}\label{what-is-data.table}}

From the \texttt{data.table} wiki

\begin{quote}
It is a high-performance version of base R's \texttt{data.frame} with syntax and feature enhancements for ease of use, convenience and programming speed.
\end{quote}

Its syntax is designed to be concise and consistent. It's somewhat similar to base R, but arguably less intuitive than \texttt{tidyverse}. We, and many others, would say that \texttt{data.table} is one of the most underrated package out there.

If you're familiar with SQL, then working with a \texttt{data.table} (DT) is conceptually similar to querying.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DT[i, j, by]}

\NormalTok{  R}\OperatorTok{:}\StringTok{                 }\NormalTok{i                 j        by}
\NormalTok{SQL}\OperatorTok{:}\StringTok{  }\NormalTok{where }\OperatorTok{|}\StringTok{ }\NormalTok{order by   select }\OperatorTok{|}\StringTok{ }\NormalTok{update  group by}
\end{Highlighting}
\end{Shaded}

This should be read as take \texttt{DT}, subset ( or order) rows using \texttt{i}, then calculate \texttt{j}, and group by \texttt{by}. A graphical depiction of this ``grammar'', created by one of the developers of \texttt{data.table}, is shown in Figure \ref{fig:dtvis}.

\begin{figure}
\includegraphics[width=0.8\linewidth]{images/dtvis} \caption{Source: https://tinyurl.com/yyepwjpt.}\label{fig:dtvis}
\end{figure}

The \texttt{data.table} package needs to be installed and loaded throughout the workshop.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"data.table"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(data.table)}
\end{Highlighting}
\end{Shaded}

Throughout the workshop, we will write DT code as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DT[i,}
\NormalTok{   j,}
\NormalTok{   by]}
\end{Highlighting}
\end{Shaded}

That is, we will use write separate lines for the i, j, and by DT statements.

\hypertarget{why-use-data.table-over-tidyverse}{%
\subsection{\texorpdfstring{Why use \texttt{data.table} over \texttt{tidyverse}?}{Why use data.table over tidyverse?}}\label{why-use-data.table-over-tidyverse}}

If you're familiar with R, then you might wonder why we are using DT and not \texttt{tidyverse}? This has to do with memory management and speed.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#}
\CommentTok{# Benchmark #1 - Reading in data}
\CommentTok{#}

\KeywordTok{system.time}\NormalTok{(\{}\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/pisa2015.csv"}\NormalTok{)\})}
\KeywordTok{system.time}\NormalTok{(\{}\KeywordTok{fread}\NormalTok{(}\StringTok{"data/pisa2015.csv"}\NormalTok{, }\DataTypeTok{na.strings =} \StringTok{""}\NormalTok{)\})}
\KeywordTok{system.time}\NormalTok{(\{}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"data/pisa2015.csv"}\NormalTok{)\})}

\CommentTok{#}
\CommentTok{# Benchmark #2 - Calculating a conditional mean}
\CommentTok{#}

\CommentTok{#' Calculate proportion that strongly agreed to an item}
\CommentTok{#' @param x likert-type item as a numeric vector}
\NormalTok{getSA <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, ...) }\KeywordTok{mean}\NormalTok{(x }\OperatorTok{==}\StringTok{ "Strongly agree"}\NormalTok{, ...)}

\CommentTok{# read in data using fread()}
\NormalTok{pisa <-}\StringTok{ }\KeywordTok{fread}\NormalTok{(}\StringTok{"data/pisa2015.csv"}\NormalTok{, }\DataTypeTok{na.strings =} \StringTok{""}\NormalTok{)}

\CommentTok{# calculate conditional means}
\CommentTok{# This is the proportion of students in each country that }
\CommentTok{# strongly agree that }
\CommentTok{# "I want top grades in most or all of my courses."}
\KeywordTok{benchmark}\NormalTok{(}
  \StringTok{"baseR"}\NormalTok{ =}\StringTok{ }\NormalTok{\{}
\NormalTok{    X <-}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(ST119Q01NA }\OperatorTok{~}\StringTok{ }\NormalTok{CNTRYID, }\DataTypeTok{data =}\NormalTok{ pisa, getSA, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{  \},}
  \StringTok{"data.table"}\NormalTok{ =}\StringTok{ }\NormalTok{\{}
\NormalTok{    X <-}\StringTok{ }\NormalTok{pisa[, }
              \KeywordTok{getSA}\NormalTok{(ST119Q01NA, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{), }
\NormalTok{              by =}\StringTok{ }\NormalTok{CNTRYID]}
\NormalTok{  \},}
  \StringTok{"tidyverse"}\NormalTok{ =}\StringTok{ }\NormalTok{\{}
\NormalTok{    X <-}\StringTok{ }\NormalTok{pisa }\OperatorTok{%>%}\StringTok{ }
\StringTok{      }\KeywordTok{group_by}\NormalTok{(CNTRYID) }\OperatorTok{%>%}\StringTok{ }
\StringTok{      }\KeywordTok{summarize}\NormalTok{(}\KeywordTok{getSA}\NormalTok{(ST119Q01NA, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\NormalTok{  \},}
  \DataTypeTok{replications =} \DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Table \ref{tab:benchresults} shows the results of this (relatively) unscientific minibenchmark. The first column is the method, the second column is elapsed time (in seconds) to read in the \textbf{pisa} data set (only once, though similar results/pattern is found if repeated), and the third column is the elapsed time (in seconds) to calculate the conditional mean 1000 times. We see that \texttt{data.table} is substantially faster than base R and the \texttt{tidyverse}.

\begin{table}

\caption{\label{tab:benchresults}Comparing base R, data.table, and tidyverse.}
\centering
\begin{tabular}[t]{l|r|r}
\hline
Method & Reading in data & Conditional mean (1000 times)\\
\hline
base R & 225.51 & 196.59\\
\hline
data.table & 46.80 & 27.73\\
\hline
tidyverse & 233.72 & 159.22\\
\hline
\end{tabular}
\end{table}

This extends to other data wrangling procedures (e.g., reshaping, recoding). Importantly, \texttt{tidyverse} is not designed for big data but instead for data science, more generally. From Grolemund \& Wickham (2017)

\begin{quote}
``This book (R for Data Science) proudly focuses on small, in-memory datasets. This is the right place to start because you cannot tackle big data unless you have experience with small data. The tools you learn in this book will easily handle hundreds of megabytes of data, and with a little care you can typically use them to work with 1-2 Gb of data. If you are routinely working with larger data (10-100 Gb, say), you should learn more about data.table. This book does not teach data.table because it has a very concise interface which makes it harder to learn since it offers fewer linguistic cues. But if you are working with large data, the performance payoff is worth the extra effort required to learn it.''
\end{quote}

\hypertarget{readingwriting-data-with-data.table}{%
\section{\texorpdfstring{Reading/writing data with \texttt{data.table}}{Reading/writing data with data.table}}\label{readingwriting-data-with-data.table}}

The \texttt{fread} function should always be used when reading in large data sets and arguably when ever you read in a CSV file. As shown above, \texttt{read.csv} and \texttt{readr::read\_csv} are painfully slow with big data.

Throughout the workshop we'll be using the \textbf{pisa} data set. Therefore, we begin by reading in (or importing) the data set

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa <-}\StringTok{ }\KeywordTok{fread}\NormalTok{(}\StringTok{"data/pisa2015.csv"}\NormalTok{, }\DataTypeTok{na.strings =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To see the \textbf{class} the object \texttt{pisa} is and how big it is in R

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(pisa)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "data.table" "data.frame"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\KeywordTok{object.size}\NormalTok{(pisa), }\DataTypeTok{unit =} \StringTok{"GB"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 3.5 Gb
\end{verbatim}

We see that objects that are read in with \texttt{fread} are of class \texttt{data.table} and \texttt{data.frame}. That means that methods for data.tables and data.frames will work on these objects. We also see this data set uses up 3.5 Gb of memory and this is all in the memory (RAM) not on the disk and allocated to memory dynamically (this is what SAS does).

If we wanted to write \texttt{pisa} back to a CSV to share with a colleague or to use in another program after some wrangling, then we should use the \texttt{fwrite} function instead of \texttt{write.csv}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fwrite}\NormalTok{(pisa, }\DataTypeTok{file =} \StringTok{"pisa2015.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The following image (Figure \ref{fig:dtcomp}), taken from Matt Dowle's blog, shows the speed difference using common ways to save R objects and the differences in sizes of these files.

\begin{figure}
\includegraphics[width=0.8\linewidth]{images/dtcomp} \caption{Time to write an R object to a file. Source: https://tinyurl.com/y366kvfx.}\label{fig:dtcomp}
\end{figure}

In the event that you did \textbf{just} want to read the data in using the \texttt{fread()} function but then wanted to work with a tibble (tidyverse) or a data.frame, you can convert the data set after its been read in:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa.tib <-}\StringTok{ }\NormalTok{tibble}\OperatorTok{::}\KeywordTok{as_tibble}\NormalTok{(pisa)}
\NormalTok{pisa.df <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(pisa)}
\end{Highlighting}
\end{Shaded}

However, I strongly recommend against this approach unless you have done some amount of subsetting. If your data set is large enough to benefit appreciably by \texttt{fread} then you should try and use the \texttt{data.table} package.

For the workshop, we have created two smaller versions of the \textbf{pisa} data set for those of you with less beefy computers. The first is a file called \texttt{region6.csv} and it was created by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{region6 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(pisa, CNT }\OperatorTok{%in%}\StringTok{  }\KeywordTok{c}\NormalTok{(}\StringTok{"United States"}\NormalTok{, }\StringTok{"Canada"}\NormalTok{, }\StringTok{"Mexico"}\NormalTok{,}
                                    \StringTok{"B-S-J-G (China)"}\NormalTok{, }\StringTok{"Japan"}\NormalTok{, }\StringTok{"Korea"}\NormalTok{,}
                                    \StringTok{"Germany"}\NormalTok{, }\StringTok{"Italy"}\NormalTok{, }\StringTok{"France"}\NormalTok{, }\StringTok{"Brazil"}\NormalTok{,}
                                    \StringTok{"Colombia"}\NormalTok{, }\StringTok{"Uruguay"}\NormalTok{, }\StringTok{"Australia"}\NormalTok{,}
                                    \StringTok{"New Zealand"}\NormalTok{, }\StringTok{"Jordan"}\NormalTok{, }\StringTok{"Israel"}\NormalTok{, }\StringTok{"Lebanon"}\NormalTok{))}
\KeywordTok{fwrite}\NormalTok{(region6, }\DataTypeTok{file =} \StringTok{"region6.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

These are the 6 regions that will be covered during data visualization and can be used for the exercises and labs. The other file is a random sample of one country from each regions for even less powerful computers, which can also be used.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{random6 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(pisa, CNT }\OperatorTok{%in%}\StringTok{  }\KeywordTok{c}\NormalTok{(}\StringTok{"Mexico"}\NormalTok{, }\StringTok{"Uruguay"}\NormalTok{, }\StringTok{"Japan"}\NormalTok{,}
                                    \StringTok{"Germany"}\NormalTok{, }\StringTok{"New Zealand"}\NormalTok{, }\StringTok{"Lebanon"}\NormalTok{))}
\KeywordTok{fwrite}\NormalTok{(random6, }\DataTypeTok{file =} \StringTok{"random6.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{exercises}{%
\subsection{Exercises}\label{exercises}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Read in the \textbf{pisa} data set. Either the full data set (recommended to have \textgreater{} 8 Gb of RAM) or one of the smaller data sets.
\end{enumerate}

\hypertarget{using-the-i-in-data.table}{%
\section{\texorpdfstring{Using the i in \texttt{data.table}}{Using the i in data.table}}\label{using-the-i-in-data.table}}

One of the first things we need to do when data wrangling is subsetting. Subsetting with \texttt{data.table} is very similar to base R but not identical. For example, if we wanted to subset all the students from Mexico who are currently taking Physics, i.e., they checked the item ``Which course did you attend? Physics: This year'' (ST063Q01NA) we would do the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[CNTRYID }\OperatorTok{==}\StringTok{ "Mexico"} \OperatorTok{&}\StringTok{ }\NormalTok{ST063Q01NA }\OperatorTok{==}\StringTok{ "Checked"}\NormalTok{]}

\CommentTok{# or (identical to base R)}
\KeywordTok{subset}\NormalTok{(pisa, CNTRYID }\OperatorTok{==}\StringTok{ "Mexico"} \OperatorTok{&}\StringTok{ }\NormalTok{ST063Q01NA }\OperatorTok{==}\StringTok{ "Checked"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note that with \texttt{data.table} we do not need to use the \texttt{\$} operator to access a variable in a \texttt{data.table} object. This is one improvement to the syntax of a \texttt{data.frame}.

Typing the name of a \texttt{data.table} won't print all the rows by default like a \texttt{data.frame}. Instead it prints just the first and last 5 rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa}
\end{Highlighting}
\end{Shaded}

This is extremely helpful because when we have a object in R, it often defaults to printing the entire object and this has the negative consequence of endless output if we type just the name of a very large object.

Because we have 921 variables, \texttt{data.table} will still truncate this output. If we want to view just the rows 10 through 25.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[}\DecValTok{10}\OperatorTok{:}\DecValTok{25}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

However, with this many columns it is useless to print all of them and instead we should focus on examining just the columns we're interested in and we will see how to do this when we examine the \texttt{j} operator.

Often when data wrangling we would like to perform multiple steps without needing to create intermediate variables. This is known as \textbf{chaining}. Chaining can be done in \texttt{data.table} via

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DT[ ...}
\NormalTok{   ][ ...}
\NormalTok{     ][ ...}
\NormalTok{       ]}
\end{Highlighting}
\end{Shaded}

For example, if we wanted to just see rows 17 through 20 after we've done previous subset, we can chain together these commands:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[CNTRYID }\OperatorTok{==}\StringTok{ "Mexico"} \OperatorTok{&}\StringTok{ }\NormalTok{ST063Q01NA }\OperatorTok{==}\StringTok{ "Checked"}
\NormalTok{     ][}\DecValTok{17}\OperatorTok{:}\DecValTok{20}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

When we're wrangling data, it's common and quite helpful to reorder rows. This can be done using the \texttt{order()} function. First, we print the first 6 six elements of the CNTRYID using the default ordering in the \textbf{pisa} data. Then we reorder the data by country name in a descending order and then print the first 6 six elements again using chaining.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(pisa}\OperatorTok{$}\NormalTok{CNTRYID)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Albania" "Albania" "Albania" "Albania" "Albania" "Albania"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[}\KeywordTok{order}\NormalTok{(CNTRYID, }\DataTypeTok{decreasing =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{     ][,}
       \KeywordTok{head}\NormalTok{(CNTRYID)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Vietnam" "Vietnam" "Vietnam" "Vietnam" "Vietnam" "Vietnam"
\end{verbatim}

\hypertarget{exercises-1}{%
\subsection{Exercises}\label{exercises-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Subset all the Female students (ST004D01T) in Germany
\item
  How many female students are there in Germany?
\item
  The \texttt{.N} function returns the length of a vector/number of rows. Use chaining with the \texttt{.N} function to answer Exercise 2.
\end{enumerate}

\hypertarget{using-the-j-in-data.table}{%
\section{\texorpdfstring{Using the j in \texttt{data.table}}{Using the j in data.table}}\label{using-the-j-in-data.table}}

Using j we can select columns, summarize variables by performing actions on the variables, and create new variables. If we wanted to just select the country identifier:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[,}
\NormalTok{     CNTRYID]}
\end{Highlighting}
\end{Shaded}

However, this returns a vector not a \texttt{data.table}. If we wanted instead to return a \texttt{data.table}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[, }
     \KeywordTok{list}\NormalTok{(CNTRYID)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                                       CNTRYID
##      1:                               Albania
##      2:                               Albania
##      3:                               Albania
##      4:                               Albania
##      5:                               Albania
##     ---                                      
## 519330: Argentina (Ciudad Autónoma de Buenos)
## 519331: Argentina (Ciudad Autónoma de Buenos)
## 519332: Argentina (Ciudad Autónoma de Buenos)
## 519333: Argentina (Ciudad Autónoma de Buenos)
## 519334: Argentina (Ciudad Autónoma de Buenos)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[,}
\NormalTok{     .(CNTRYID)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                                       CNTRYID
##      1:                               Albania
##      2:                               Albania
##      3:                               Albania
##      4:                               Albania
##      5:                               Albania
##     ---                                      
## 519330: Argentina (Ciudad Autónoma de Buenos)
## 519331: Argentina (Ciudad Autónoma de Buenos)
## 519332: Argentina (Ciudad Autónoma de Buenos)
## 519333: Argentina (Ciudad Autónoma de Buenos)
## 519334: Argentina (Ciudad Autónoma de Buenos)
\end{verbatim}

The \texttt{.()} is \texttt{data.table} shorthand for \texttt{list()}. To subset more than one variable, we can just add another variable within the \texttt{.()}. For example, if we also wanted to select the science self-efficacy scale (SCIEEFF) as well, we do the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[,}
\NormalTok{     .(CNTRYID, SCIEEFF)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                                       CNTRYID SCIEEFF
##      1:                               Albania      NA
##      2:                               Albania      NA
##      3:                               Albania      NA
##      4:                               Albania      NA
##      5:                               Albania      NA
##     ---                                              
## 519330: Argentina (Ciudad Autónoma de Buenos) -0.8799
## 519331: Argentina (Ciudad Autónoma de Buenos)  0.9802
## 519332: Argentina (Ciudad Autónoma de Buenos) -0.5696
## 519333: Argentina (Ciudad Autónoma de Buenos) -0.7065
## 519334: Argentina (Ciudad Autónoma de Buenos) -0.3609
\end{verbatim}

If we wanted see how many students took physics in Japan and Mexico, we would do the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[CNTRYID }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Mexico"}\NormalTok{, }\StringTok{"Japan"}\NormalTok{),}
     \KeywordTok{table}\NormalTok{(ST063Q01NA)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## ST063Q01NA
##     Checked Not checked 
##        4283        9762
\end{verbatim}

Because \texttt{data.table} treats string variables as character variables by default we see that when they are printed they are printed alphabetically, which in this case is fine but is often unhelpful. We can chain together variables and create an intermediate tense variable to get this in the correct format. However, when we want to know how students in Mexico and Japan responded to ``I get very tense when I study for a test.''

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[CNTRYID }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Mexico"}\NormalTok{, }\StringTok{"Japan"}\NormalTok{),}
     \KeywordTok{table}\NormalTok{(ST118Q04NA)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## ST118Q04NA
##             Agree          Disagree    Strongly agree Strongly disagree 
##              4074              5313              1760              2904
\end{verbatim}

We see that the output is unhelpful. Instead, we should convert the character vector into a factor and we will create an intermediate variable called \texttt{tense}, which we won't add to our data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[CNTRYID }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Mexico"}\NormalTok{, }\StringTok{"Japan"}\NormalTok{),}
\NormalTok{     .(}\DataTypeTok{tense =} \KeywordTok{factor}\NormalTok{(ST118Q04NA, }\DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Strongly disagree"}\NormalTok{, }\StringTok{"Disagree"}\NormalTok{, }\StringTok{"Agree"}\NormalTok{, }\StringTok{"Strongly agree"}\NormalTok{)))}
\NormalTok{     ][,}
       \KeywordTok{table}\NormalTok{(tense)}
\NormalTok{     ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## tense
## Strongly disagree          Disagree             Agree    Strongly agree 
##              2904              5313              4074              1760
\end{verbatim}

Quick digression, in case you were wondering why base R reads strings in as factors and not characters by default (which \texttt{data.table} and \texttt{readr::read\_csv} do),

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[, .(}\DataTypeTok{tense.as.char =}\NormalTok{ ST118Q04NA,}
         \DataTypeTok{tense.as.fac =} \KeywordTok{factor}\NormalTok{(ST118Q04NA, }\DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Strongly disagree"}\NormalTok{, }\StringTok{"Disagree"}\NormalTok{, }\StringTok{"Agree"}\NormalTok{, }\StringTok{"Strongly agree"}\NormalTok{)))}
\NormalTok{     ][,}
\NormalTok{       .(}\DataTypeTok{character =} \KeywordTok{object.size}\NormalTok{(tense.as.char),}
         \DataTypeTok{factor =} \KeywordTok{object.size}\NormalTok{(tense.as.fac))}
\NormalTok{     ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        character        factor
## 1: 4154984 bytes 2078064 bytes
\end{verbatim}

Returning to the science self-efficacy scale, we can request summary information for just these two countries:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[CNTRYID }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Mexico"}\NormalTok{,}\StringTok{"Japan"}\NormalTok{),}
\NormalTok{     .(}\DataTypeTok{xbar =} \KeywordTok{mean}\NormalTok{(SCIEEFF, }\DataTypeTok{na.rm =}\NormalTok{ T),}
       \DataTypeTok{sigma =} \KeywordTok{sd}\NormalTok{(SCIEEFF, }\DataTypeTok{na.rm =}\NormalTok{ T),}
       \DataTypeTok{minimum =} \KeywordTok{min}\NormalTok{(SCIEEFF, }\DataTypeTok{na.rm =}\NormalTok{ T),}
       \DataTypeTok{med =} \KeywordTok{median}\NormalTok{(SCIEEFF, }\DataTypeTok{na.rm =}\NormalTok{ T),}
       \DataTypeTok{maximum =} \KeywordTok{max}\NormalTok{(SCIEEFF, }\DataTypeTok{na.rm =}\NormalTok{ T))]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           xbar    sigma minimum     med maximum
## 1: -0.08693672 1.216052 -3.7565 -0.0541  3.2775
\end{verbatim}

We can create a quick plot this way, too. For example, if we wanted a create a scatter plot of the science self-efficacy scale against the enjoyment of science scale (JOYSCIE) for just these two countries and print the mean of the enjoyment of science scale, we can do the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[CNTRYID }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Mexico"}\NormalTok{,}\StringTok{"Japan"}\NormalTok{),}
\NormalTok{     .(}\KeywordTok{plot}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ SCIEEFF, }\DataTypeTok{x =}\NormalTok{ JOYSCIE, }
            \DataTypeTok{col =} \KeywordTok{rgb}\NormalTok{(}\DataTypeTok{red =} \DecValTok{0}\NormalTok{, }\DataTypeTok{green =} \DecValTok{0}\NormalTok{, }\DataTypeTok{blue =} \DecValTok{0}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{)), }
       \DataTypeTok{xbar.joyscie =} \KeywordTok{mean}\NormalTok{(JOYSCIE, }\DataTypeTok{na.rm =}\NormalTok{ T))]}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/scatjoy-1.pdf}

\begin{verbatim}
##    xbar.joyscie
## 1:   0.06140002
\end{verbatim}

This example is kind of silly but it shows that j is incredibly flexible and that we can string together a bunch of commands using j without even needing to do chaining.

Let's say we need to recode ``After leaving school did you: Eat dinner'' from a character variable to a numeric variable. We can do this with a series of if else statements

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(pisa}\OperatorTok{$}\NormalTok{ST078Q01NA)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##     No    Yes 
##  23617 373131
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[,}
     \StringTok{"eat.dinner"} \OperatorTok{:}\ErrorTok{=}\StringTok{ }\KeywordTok{sapply}\NormalTok{(ST078Q01NA,}
                            \ControlFlowTok{function}\NormalTok{(x) \{}
                              \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{is.na}\NormalTok{(x)) }\OtherTok{NA}
                              \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (x }\OperatorTok{==}\StringTok{ "No"}\NormalTok{) 0L}
                              \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (x }\OperatorTok{==}\StringTok{ "Yes"}\NormalTok{) 1L}
\NormalTok{                            \})}
\NormalTok{     ][,}
       \KeywordTok{table}\NormalTok{(eat.dinner)}
\NormalTok{       ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## eat.dinner
##      0      1 
##  23617 373131
\end{verbatim}

In this example we created a new variable called \texttt{eat.dinner} using \texttt{:=} the function. The \texttt{:=} syntax adds this variable directly to the DT. We also specified the \texttt{L} to ensure the variable was treated as an integer and not a double, which uses less memory.

We should create a function to do this recoding as there are lots of dichotomous items in the \textbf{pisa} data set.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#' Convert a dichtomous item (yes/no) to numeric scoring}
\CommentTok{#' @param x a character vector containing "Yes" and "No" responses.}
\NormalTok{bin.to.num <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x)\{}
  \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{is.na}\NormalTok{(x)) }\OtherTok{NA}
  \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (x }\OperatorTok{==}\StringTok{ "Yes"}\NormalTok{) 1L}
  \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (x }\OperatorTok{==}\StringTok{ "No"}\NormalTok{) 0L}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Then use this function to create some variables as well as recoding gender to give it a more intuitive variable name.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[, }\StringTok{`}\DataTypeTok{:=}\StringTok{`} 
\NormalTok{     (}\DataTypeTok{female =} \KeywordTok{ifelse}\NormalTok{(ST004D01T }\OperatorTok{==}\StringTok{ "Female"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
       \DataTypeTok{sex =}\NormalTok{ ST004D01T,}
       
       \CommentTok{# At my house we have ...}
       \DataTypeTok{desk =} \KeywordTok{sapply}\NormalTok{(ST011Q01TA, bin.to.num),}
       \DataTypeTok{own.room =} \KeywordTok{sapply}\NormalTok{(ST011Q02TA, bin.to.num),}
       \DataTypeTok{quiet.study =} \KeywordTok{sapply}\NormalTok{(ST011Q03TA, bin.to.num),}
       \DataTypeTok{computer =} \KeywordTok{sapply}\NormalTok{(ST011Q04TA, bin.to.num),}
       \DataTypeTok{software =} \KeywordTok{sapply}\NormalTok{(ST011Q05TA, bin.to.num),}
       \DataTypeTok{internet =} \KeywordTok{sapply}\NormalTok{(ST011Q06TA, bin.to.num),}
       \DataTypeTok{lit =} \KeywordTok{sapply}\NormalTok{(ST011Q07TA, bin.to.num),}
       \DataTypeTok{poetry =} \KeywordTok{sapply}\NormalTok{(ST011Q08TA, bin.to.num),}
       \DataTypeTok{art =} \KeywordTok{sapply}\NormalTok{(ST011Q09TA, bin.to.num),}
       \DataTypeTok{book.sch =} \KeywordTok{sapply}\NormalTok{(ST011Q10TA, bin.to.num),}
       \DataTypeTok{tech.book =} \KeywordTok{sapply}\NormalTok{(ST011Q11TA, bin.to.num),}
       \DataTypeTok{dict =} \KeywordTok{sapply}\NormalTok{(ST011Q12TA, bin.to.num),}
       \DataTypeTok{art.book =} \KeywordTok{sapply}\NormalTok{(ST011Q16NA, bin.to.num))]}
\end{Highlighting}
\end{Shaded}

Similarly, we can create new variables by combining pre-existing ones. In the later data visualization section, we will use the following variables, so we will create them now. The \texttt{rowMeans} function takes a data.frame, so we need to subset the variables from the \textbf{pisa} data set and then convert it to a data.frame. This is what the brackets are doing.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[, }\StringTok{`}\DataTypeTok{:=}\StringTok{`}
\NormalTok{     (}\DataTypeTok{math =} \KeywordTok{rowMeans}\NormalTok{(pisa[, }\KeywordTok{c}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"PV"}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{, }\StringTok{"MATH"}\NormalTok{))], }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
       \DataTypeTok{reading =} \KeywordTok{rowMeans}\NormalTok{(pisa[, }\KeywordTok{c}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"PV"}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{, }\StringTok{"READ"}\NormalTok{))], }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
       \DataTypeTok{science =} \KeywordTok{rowMeans}\NormalTok{(pisa[, }\KeywordTok{c}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"PV"}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{, }\StringTok{"SCIE"}\NormalTok{))], }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))]}
\end{Highlighting}
\end{Shaded}

\hypertarget{exercises-2}{%
\subsection{Exercises}\label{exercises-2}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The computer and software variables that were created above ask a student whether they had a computer in their home that they can use for school work (computer) and whether they had educational software in their home (software). Find the proportion of students in the Germany and Uruguay that have a computer in their home or have educational software.
\item
  For just female students, find the proportion of students who have their own room (own.room) or a quiet place to study (quiet.study).
\end{enumerate}

\hypertarget{summarizing-using-the-by-in-data.table}{%
\section{\texorpdfstring{Summarizing using the by in \texttt{data.table}}{Summarizing using the by in data.table}}\label{summarizing-using-the-by-in-data.table}}

With the by argument, we can now get conditional responses without the need to subset. If we want to know the proportion of students in each country that have their own room at home.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[,}
\NormalTok{     .(}\KeywordTok{mean}\NormalTok{(own.room, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)),}
\NormalTok{     by =}\StringTok{ }\NormalTok{.(CNTRYID)}
\NormalTok{     ][}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{,}
\NormalTok{     ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      CNTRYID        V1
## 1:   Albania       NaN
## 2:   Algeria 0.5187970
## 3: Australia 0.9216078
## 4:   Austria 0.9054462
## 5:   Belgium 0.9153612
## 6:    Brazil 0.7497861
\end{verbatim}

Again, we can reorder this using chaining:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[,}
\NormalTok{     .(}\DataTypeTok{own.room =} \KeywordTok{mean}\NormalTok{(own.room, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)),}
\NormalTok{     by =}\StringTok{ }\NormalTok{.(}\DataTypeTok{country =}\NormalTok{ CNTRYID)}
\NormalTok{     ][}\KeywordTok{order}\NormalTok{(own.room, }\DataTypeTok{decreasing =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{       ][}\DecValTok{1}\OperatorTok{:}\DecValTok{6}
\NormalTok{         ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        country  own.room
## 1:     Iceland 0.9862721
## 2: Netherlands 0.9750188
## 3:      Norway 0.9737686
## 4:      Sweden 0.9558879
## 5:     Finland 0.9440994
## 6:     Germany 0.9379274
\end{verbatim}

What if we want to compare just the Canada and Iceland on the proportion of students that have books of poetry at home (poetry) or and their mean on the enjoyment of science by student's biological sex?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[CNTRYID }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Canada"}\NormalTok{, }\StringTok{"Iceland"}\NormalTok{),}
\NormalTok{     .(}\DataTypeTok{poetry =} \KeywordTok{mean}\NormalTok{(poetry, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
       \DataTypeTok{enjoy =} \KeywordTok{mean}\NormalTok{(JOYSCIE, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)),}
\NormalTok{     by =}\StringTok{ }\NormalTok{.(}\DataTypeTok{country =}\NormalTok{ CNTRYID, }\DataTypeTok{sex =}\NormalTok{ sex)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    country    sex    poetry      enjoy
## 1:  Canada Female 0.3632105 0.29635781
## 2:  Canada   Male 0.3123878 0.40950018
## 3: Iceland Female 0.7280806 0.03583745
## 4: Iceland   Male 0.7011494 0.30316273
\end{verbatim}

We see a strong country effect on poetry at home, with \textgreater{} 70\% of Icelandic students reporting poetry books at home and just above 30\% in Canadian students and we see that Canadian students enjoy science more than Icelandic students and, male students, overall, enjoy science more than females.

Let's examine books of poetry at home by countries and sort it in descending order.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[,}
\NormalTok{     .(}\DataTypeTok{poetry =} \KeywordTok{mean}\NormalTok{(poetry, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)),}
\NormalTok{     by =}\StringTok{ }\NormalTok{.(}\DataTypeTok{country =}\NormalTok{ CNTRYID)}
\NormalTok{     ][}\KeywordTok{order}\NormalTok{(poetry, }\DataTypeTok{decreasing =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{       ][}\DecValTok{1}\OperatorTok{:}\DecValTok{6}
\NormalTok{         ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               country    poetry
## 1:             Kosovo 0.8352507
## 2: Russian Federation 0.8045568
## 3:            Romania 0.8019434
## 4:            Georgia 0.7495615
## 5:    B-S-J-G (China) 0.7442052
## 6:            Estonia 0.7422718
\end{verbatim}

Iceland is in the top 10, while Canada is 59.

We can also write more complex functions and provide these to \texttt{data.table}. For example, if wanted to fit a regression model to predict a student's score on science self-efficacy scale given their score on the enjoyment of science scale and their sex for just the G7 countries (Canada, France, Germany, Italy, Japan, the United Kingdom, and the United States), we can fit a multiple regression model and return the intercept and slope terms.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get.params <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(cntry)\{}
\NormalTok{  mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(SCIEEFF }\OperatorTok{~}\StringTok{ }\NormalTok{JOYSCIE }\OperatorTok{+}\StringTok{ }\NormalTok{sex, cntry)}
\NormalTok{  est.params <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{int =} \KeywordTok{coef}\NormalTok{(mod)[[}\DecValTok{1}\NormalTok{]], }\DataTypeTok{enjoy.slope =} \KeywordTok{coef}\NormalTok{(mod)[[}\DecValTok{2}\NormalTok{]], }\DataTypeTok{sex.slope =} \KeywordTok{coef}\NormalTok{(mod)[[}\DecValTok{3}\NormalTok{]])}
  \KeywordTok{return}\NormalTok{(est.params)}
\NormalTok{\}}

\NormalTok{g7.params <-}\StringTok{ }\NormalTok{pisa[CNTRYID }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Canada"}\NormalTok{, }\StringTok{"France"}\NormalTok{, }\StringTok{"Germany"}\NormalTok{, }\StringTok{"Italy"}\NormalTok{,}
                                 \StringTok{"Japan"}\NormalTok{, }\StringTok{"United Kingdom"}\NormalTok{, }\StringTok{"United States"}\NormalTok{),}
               \KeywordTok{get.params}\NormalTok{(.SD), }
\NormalTok{               by =}\StringTok{ }\NormalTok{.(CNTRYID)]}
\NormalTok{g7.params}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           CNTRYID          int enjoy.slope  sex.slope
## 1:         Canada  0.009803357   0.4370945 0.21489577
## 2:         France -0.208698984   0.4760903 0.17743126
## 3:        Germany -0.019150031   0.4316565 0.17971821
## 4:          Italy -0.030880063   0.3309990 0.18831666
## 5:          Japan -0.353806055   0.3914385 0.04912039
## 6: United Kingdom  0.009711647   0.5182592 0.18981965
## 7:  United States  0.096920721   0.3907848 0.15022008
\end{verbatim}

We see a fair bit of variability in these estimated parameters

\hypertarget{exercises-3}{%
\subsection{Exercises}\label{exercises-3}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Calculate the proportion of students who have art in their home (art) and the average age (AGE) of the students by gender.
\item
  Within a by argument you can discretize a variable to create a grouping variable. Perform a median split for age within the by argument and assess whether there are age difference associated with having your own room (own.room) or a desk (desk).
\end{enumerate}

\hypertarget{reshaping-data}{%
\section{Reshaping data}\label{reshaping-data}}

The \texttt{data.table} package provides some very fast methods to reshape data from wide (the current format) to long format. In long format, a single test taker will correspond to multiple rows of data. Some software and \texttt{R} packages require data to be in long format (e.g., \texttt{lme4} and \texttt{nlme}).

Let's begin by creating a student ID and then subsetting this ID and the at-home variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa}\OperatorTok{$}\NormalTok{id <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(pisa)}
\NormalTok{athome <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(pisa, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(id, desk}\OperatorTok{:}\NormalTok{art.book))}
\end{Highlighting}
\end{Shaded}

To transform the data to long format we \emph{melt} the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{athome.l <-}\StringTok{ }\KeywordTok{melt}\NormalTok{(athome, }
                 \DataTypeTok{id.vars =} \StringTok{"id"}\NormalTok{,}
                 \DataTypeTok{measure.vars =} \KeywordTok{c}\NormalTok{(}\StringTok{"desk"}\NormalTok{, }\StringTok{"own.room"}\NormalTok{, }\StringTok{"quiet.study"}\NormalTok{, }\StringTok{"lit"}\NormalTok{,}
                                  \StringTok{"poetry"}\NormalTok{, }\StringTok{"art"}\NormalTok{, }\StringTok{"book.sch"}\NormalTok{, }\StringTok{"tech.book"}\NormalTok{,}
                                  \StringTok{"dict"}\NormalTok{, }\StringTok{"art.book"}\NormalTok{))}
\NormalTok{athome.l}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              id variable value
##       1:      1     desk    NA
##       2:      2     desk    NA
##       3:      3     desk    NA
##       4:      4     desk    NA
##       5:      5     desk    NA
##      ---                      
## 5193336: 519330 art.book     1
## 5193337: 519331 art.book     0
## 5193338: 519332 art.book     1
## 5193339: 519333 art.book     0
## 5193340: 519334 art.book     0
\end{verbatim}

We could have also allowed \texttt{melt()} to guess the format:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{athome.guess <-}\StringTok{ }\KeywordTok{melt}\NormalTok{(athome)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in melt.data.table(athome): To be consistent with reshape2's melt,
## id.vars and measure.vars are internally guessed when both are 'NULL'. All
## non-numeric/integer/logical type columns are considered id.vars, which
## in this case are columns []. Consider providing at least one of 'id' or
## 'measure' vars in future.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{athome.guess}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          variable value
##       1:       id     1
##       2:       id     2
##       3:       id     3
##       4:       id     4
##       5:       id     5
##      ---               
## 7270672: art.book     1
## 7270673: art.book     0
## 7270674: art.book     1
## 7270675: art.book     0
## 7270676: art.book     0
\end{verbatim}

It guessed incorrectly. If id was set as a character vector, then it would have guessed correctly this time. However, you should not allow it to guess the names of the variables.

To go back to wide format we use the \texttt{dcast()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{athome.w <-}\StringTok{ }\KeywordTok{dcast}\NormalTok{(athome.l,}
\NormalTok{                  id }\OperatorTok{~}\StringTok{ }\NormalTok{variable)}
\end{Highlighting}
\end{Shaded}

Unlike other reshaping packages, \texttt{data.table} can also handle reshaping multiple outcomes variables. More about reshaping with \texttt{data.table} is available \href{https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reshape.html}{here}.

\hypertarget{the-sparklyr-package}{%
\section{\texorpdfstring{The \texttt{sparklyr} package}{The sparklyr package}}\label{the-sparklyr-package}}

The \texttt{sparklyr} package provides an R interface to Apache Spark and a complete \texttt{dplyr} backend. Apache Spark ``is a unified analytics engine for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing''. Apache Spark can also be interfaced using the \texttt{sparkR} package provided by Apache. See \href{https://spark.apache.org/docs/2.4.0/}{here} and \href{https://spark.apache.org/docs/2.4.0/api/R/index.html}{here} for more details.

To use Apache Spark you will need Java 8 JDK installed. It can be installed \href{https://www.oracle.com/technetwork/java/jdk8-downloads-2133151.html}{here}. To begin with you need to install \texttt{sparklyr} and \texttt{dplyr}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"sparklyr"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"dplyr"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"sparklyr"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"dplyr"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We then need to install Spark, which we can do from R.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{spark_install}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Next, we need to setup a connection with Spark and we'll be connecting to a local install of Spark.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sc <-}\StringTok{ }\KeywordTok{spark_connect}\NormalTok{(}\DataTypeTok{master =} \StringTok{"local"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Re-using existing Spark connection to local
\end{verbatim}

Then we need to copy the \textbf{pisa} data set to the Spark cluster. However, with this large of a data set, this is a bad idea. We will run into memory issues during the copying process. So, we'll first subset the data before we do this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa_sub <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(pisa, CNTRYID }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Canada"}\NormalTok{, }\StringTok{"France"}\NormalTok{, }\StringTok{"Germany"}\NormalTok{,}
                                        \StringTok{"Italy"}\NormalTok{, }\StringTok{"Japan"}\NormalTok{, }\StringTok{"United Kingdom"}\NormalTok{,}
                                        \StringTok{"United States"}\NormalTok{),}
                   \DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\StringTok{"DISCLISCI"}\NormalTok{, }\StringTok{"TEACHSUP"}\NormalTok{, }\StringTok{"IBTEACH"}\NormalTok{, }\StringTok{"TDTEACH"}\NormalTok{,}
                              \StringTok{"ENVAWARE"}\NormalTok{, }\StringTok{"JOYSCIE"}\NormalTok{, }\StringTok{"INTBRSCI"}\NormalTok{, }\StringTok{"INSTSCIE"}\NormalTok{,}
                              \StringTok{"SCIEEFF"}\NormalTok{, }\StringTok{"EPIST"}\NormalTok{, }\StringTok{"SCIEACT"}\NormalTok{, }\StringTok{"BSMJ"}\NormalTok{, }\StringTok{"MISCED"}\NormalTok{,}
                              \StringTok{"FISCED"}\NormalTok{, }\StringTok{"OUTHOURS"}\NormalTok{, }\StringTok{"SMINS"}\NormalTok{, }\StringTok{"TMINS"}\NormalTok{,}
                              \StringTok{"BELONG"}\NormalTok{, }\StringTok{"ANXTEST"}\NormalTok{, }\StringTok{"MOTIVAT"}\NormalTok{, }\StringTok{"COOPERATE"}\NormalTok{,}
                              \StringTok{"PERFEED"}\NormalTok{, }\StringTok{"unfairteacher"}\NormalTok{, }\StringTok{"HEDRES"}\NormalTok{, }\StringTok{"HOMEPOS"}\NormalTok{,}
                              \StringTok{"ICTRES"}\NormalTok{, }\StringTok{"WEALTH"}\NormalTok{, }\StringTok{"ESCS"}\NormalTok{, }\StringTok{"math"}\NormalTok{, }\StringTok{"reading"}\NormalTok{,}
                              \StringTok{"CNTRYID"}\NormalTok{, }\StringTok{"sex"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We will use the selected variables in the labs and a description of these variables can be seen below.

Now the data are ready to be copied into Spark.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa_tbl <-}\StringTok{ }\KeywordTok{copy_to}\NormalTok{(sc, pisa_sub, }\DataTypeTok{overwrite =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In tidyverse, you can use the \texttt{\%\textgreater{}\%} to chain together commands or to pass data to functions. With \texttt{sparklyr}, we can use the \texttt{filter} function instead of subset. For example, if we just want to see the female students' scores on these scales for Germany, we would do the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa_tbl }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(CNTRYID }\OperatorTok{==}\StringTok{ "Germany"} \OperatorTok{&}\StringTok{ }\NormalTok{sex }\OperatorTok{==}\StringTok{ "Female"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # Source: spark<?> [?? x 32]
##    DISCLISCI TEACHSUP IBTEACH TDTEACH ENVAWARE JOYSCIE INTBRSCI INSTSCIE
##        <dbl>    <dbl>   <dbl>   <dbl>    <dbl>   <dbl>    <dbl>    <dbl>
##  1   -0.234    -0.804  -0.608 -0.867   -0.536   -0.821  -0.550   NaN    
##  2    0.283     0.488  -0.157 -0.685   -0.805   -2.12   -1.13     -1.93 
##  3    0.700     1.45    0.988  0.525    0.171   -1.72   -0.225    -0.718
##  4    0.0039    0.568   0.209 -0.0742  -0.234   -0.821  -0.0831   -0.826
##  5    0.763    -0.450   0.535 -0.0057  -0.479    0.613   0.198    -0.304
##  6    0.660    -0.461   0.647  0.450   -0.706   -0.631  -0.551    -1.93 
##  7    0.288    -1.82    0.430 -1.32    -0.0217  -0.576  -0.566    -0.670
##  8    0.835    -1.07    0.89  -0.610    0.256    2.16    0.341     1.33 
##  9    1.32     -0.246   0.257 -0.867   -0.685   -0.152  -0.509    -0.778
## 10    1.32     -1.29    0.308 -0.790   -0.385   -1.61   -0.399    -1.93 
## # ... with more rows, and 24 more variables: SCIEEFF <dbl>, EPIST <dbl>,
## #   SCIEACT <dbl>, BSMJ <int>, MISCED <chr>, FISCED <chr>, OUTHOURS <int>,
## #   SMINS <int>, TMINS <int>, BELONG <dbl>, ANXTEST <dbl>, MOTIVAT <dbl>,
## #   COOPERATE <dbl>, PERFEED <dbl>, unfairteacher <int>, HEDRES <dbl>,
## #   HOMEPOS <dbl>, ICTRES <dbl>, WEALTH <dbl>, ESCS <dbl>, math <dbl>,
## #   reading <dbl>, CNTRYID <chr>, sex <chr>
\end{verbatim}

You'll notice the at the top it says \texttt{\#Source:\ spark\textless{}?\textgreater{}}

If we wanted to calculate the average disciplinary climate in science classes (DISCLISCI) by country and by sex and have it reorder by country than sex, we can do the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa_tbl }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(CNTRYID, sex) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{ave_disclip =} \KeywordTok{mean}\NormalTok{(DISCLISCI, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(CNTRYID, sex)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # Source:     spark<?> [?? x 3]
## # Groups:     CNTRYID
## # Ordered by: CNTRYID, sex
##    CNTRYID sex    ave_disclip
##    <chr>   <chr>        <dbl>
##  1 Canada  Female      0.0110
##  2 Canada  Male       -0.0205
##  3 France  Female     -0.236 
##  4 France  Male       -0.297 
##  5 Germany Female      0.0915
##  6 Germany Male        0.0162
##  7 Italy   Female      0.0708
##  8 Italy   Male       -0.137 
##  9 Japan   Female      0.916 
## 10 Japan   Male        0.788 
## # ... with more rows
\end{verbatim}

We can also create new variables using the \texttt{mutate} function. If we want to get a measure of home affluence, we could add home educational resources (HEDRES) and home possessions (HOMEPOS)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa_tbl }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{totl_home =}\NormalTok{ HEDRES }\OperatorTok{+}\StringTok{ }\NormalTok{HOMEPOS) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(CNTRYID) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{xbar =} \KeywordTok{mean}\NormalTok{(totl_home, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # Source: spark<?> [?? x 2]
##   CNTRYID          xbar
##   <chr>           <dbl>
## 1 United Kingdom  0.370
## 2 United States   0.113
## 3 Italy           0.324
## 4 Japan          -1.30 
## 5 France         -0.332
## 6 Germany         0.279
## 7 Canada          0.430
\end{verbatim}

On my computer, the Spark code is slightly faster than \texttt{data.table}, but not by much. The real power of using Spark is that we can use its machine learning functions. However, if you're familiar with \texttt{tidyverse} (\texttt{dplyr}) syntax, then \texttt{sparklyr} is a package that is worth investigating for data wrangling with big data sets.

\hypertarget{lab}{%
\section{Lab}\label{lab}}

This afternoon when we discuss supervised learning, we'll ask you to develop some models to predict the response to the question Do you expect your child will go into a ?" (PA032Q03TA).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Recode this variable so that a ``Yes'' is 1 and a ``No'' is a -1 and save the variable as \texttt{sci\_car}.
\item
  Calculate descriptives for this variable by sex and country. Specifically, the proportion of test takers whose parents said ``Yes'' or 1.
\end{enumerate}

After you've done this, spend some time investigating the following variables

\begin{longtable}[]{@{}ll@{}}
\toprule
Label & Description\tabularnewline
\midrule
\endhead
DISCLISCI & Disciplinary climate in science classes (WLE)\tabularnewline
TEACHSUP & Teacher support in a science classes of students choice (WLE)\tabularnewline
IBTEACH & Inquiry-based science teaching an learning practices (WLE)\tabularnewline
TDTEACH & Teacher-directed science instruction (WLE)\tabularnewline
ENVAWARE & Environmental Awareness (WLE)\tabularnewline
JOYSCIE & Enjoyment of science (WLE)\tabularnewline
INTBRSCI & Interest in broad science topics (WLE)\tabularnewline
INSTSCIE & Instrumental motivation (WLE)\tabularnewline
SCIEEFF & Science self-efficacy (WLE)\tabularnewline
EPIST & Epistemological beliefs (WLE)\tabularnewline
SCIEACT & Index science activities (WLE)\tabularnewline
BSMJ & Student's expected occupational status (SEI)\tabularnewline
MISCED & Mother's Education (ISCED)\tabularnewline
FISCED & Father's Education (ISCED)\tabularnewline
OUTHOURS & Out-of-School Study Time per week (Sum)\tabularnewline
SMINS & Learning time (minutes per week) - \tabularnewline
TMINS & Learning time (minutes per week) - in total\tabularnewline
BELONG & Subjective well-being: Sense of Belonging to School (WLE)\tabularnewline
ANXTEST & Personality: Test Anxiety (WLE)\tabularnewline
MOTIVAT & Student Attitudes, Preferences and Self-related beliefs: Achieving motivation (WLE)\tabularnewline
COOPERATE & Collaboration and teamwork dispositions: Enjoy cooperation (WLE)\tabularnewline
PERFEED & Perceived Feedback (WLE)\tabularnewline
unfairteacher & Teacher Fairness (Sum)\tabularnewline
HEDRES & Home educational resources (WLE)\tabularnewline
HOMEPOS & Home possessions (WLE)\tabularnewline
ICTRES & ICT Resources (WLE)\tabularnewline
WEALTH & Family wealth (WLE)\tabularnewline
ESCS & Index of economic, social and cultural status (WLE)\tabularnewline
math & Students' math score in PISA 2015\tabularnewline
reading & Students' reading score in PISA 2015\tabularnewline
\bottomrule
\end{longtable}

and then do the following using \texttt{data.table} and/or \texttt{sparklyr}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Means and standard deviations (\texttt{sd}) for the variables that you think will be most predictive of \texttt{sci\_car}.
\item
  Calculate these same descriptives by groups (by \texttt{sci\_car} and by \texttt{sex}).
\item
  Calculate correlations between these variables and \texttt{sci\_car},
\item
  Create new variables

  \begin{itemize}
  \tightlist
  \item
    Discretize the math and reading variables using the OECD means (490 for math and 493) and code them as 1 (at or above the mean) and -1 (below the mean), but do in the \texttt{data.table} way without using the \texttt{\$} operator.
  \item
    Calculate the correlation between these variables and the list of variables above.
  \end{itemize}
\item
  Chain together a set of operations

  \begin{itemize}
  \tightlist
  \item
    For example, create an intermediate variable that is the average of JOYSCIE and INTBRSCI, and then calculate the mean by country by \texttt{sci\_car} through chaining.
  \end{itemize}
\item
  Transform variables, specifically recode MISCED and FISCED from characters to numeric variables.
\item
  Examine other variables in the \textbf{pisa} data set that you think might be predictive of \texttt{PA032Q03TA}.
\end{enumerate}

\hypertarget{visualizing-big-data}{%
\chapter{Visualizing big data}\label{visualizing-big-data}}

One of the most effective ways to explore big data, interpret variables, and communicate results obtained from big data analyses to varied audiences is through \textbf{data visualization}. When we deal with big data, we can benefit from data visualizations in many ways, such as:

\begin{itemize}
\tightlist
\item
  understanding the distributional characteristics of variables,
\item
  detecting data entry issues,
\item
  identifying outliers in the data,
\item
  understanding relationships among variables,
\item
  selecting suitable variables for data analysis (a.k.a., feature extraction),
\item
  examining the outcomes of predictive models (e.g., accuracy and overfit), and
\item
  communicating the results to various audiences.
\end{itemize}

Developing effective visualizations requires identifying the goals and design of data analysis clearly. Sometimes we may already know the answers for some questions about the data; in other cases, we may want to explore further and understand the data in order to generate better insights into the next steps of data analysis. In this process, we need to consider many elements, such as types of variables to be used, axes, labels, legends, colors, and so on. Furthermore, if we aim to present the visualization to a particular audience, then we also need to consider the usability and interpretability of the visualization for the target audience.

The development of an effective data visualization typically includes the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Determine the goal of data visualization (e.g., exploring data, relationships, model outcomes)
\item
  Prepare the data (e.g., clean, organize, and transform data)
\item
  Identify the ideal visualization tool based on the goal of data visualization
\item
  Produce the visualization
\item
  Interpret the information in the visualization and present it to your target audience
\end{enumerate}

\begin{figure}
\includegraphics[width=1\linewidth]{images/chart} \caption{Chart suggestions (Source: <https://extremepresentation.com/>)}\label{fig:fig1}
\end{figure}

Figure \ref{fig:fig1} shows some suggestions for visualizing data based on the type of variables and the purpose of the visualization. In R, almost all of these visualizations can be created very easily, although preparing the data for these visualizations is sometimes quite tedious.

In this section of our session, we will review data visualization tools in R that can help us organize big data, interpret variables, and identify potential variables for predictive models. The first part will focus on data visualizations using the \href{https://ggplot2.tidyverse.org/}{ggplot2} package. Furthermore, we will use other R packages (e.g., \href{http://ggobi.github.io/ggally/\#ggally}{GGally}, \href{https://www.ggplot2-exts.org/ggExtra.html}{ggExtra}, and \href{http://corybrunson.github.io/ggalluvial/}{ggalluvial}) that expand the capabilities of \texttt{ggplot2} even further (also see \url{http://www.ggplot2-exts.org/gallery/} for more extensions of \texttt{ggplot2}). In the second part, we will discuss web-based, interactive visualizations and dashboards using \href{https://plot.ly/r/}{plotly}.

As we review data visualization tools, we will also demonstrate how to use each visualization tool in R and produce sample plots and graphics using the \textbf{pisa} dataset. Furthermore, we will ask you to work on short exercises where you will need to use the functions and packages presented in this section in order to generate your own plots and visualizations using the \textbf{pisa} dataset.

Before we begin, let's install and load all of the R packages that we will use in this section:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Install and load the packages one by one.}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"GGally"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"ggExtra"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"ggalluvial"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"plotly"}\NormalTok{)}

\KeywordTok{library}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"GGally"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"ggExtra"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"ggalluvial"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"plotly"}\NormalTok{)}

\CommentTok{# Or, just simply run the following to install and load all packages:}
\NormalTok{dataviz_packages <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{, }\StringTok{"GGally"}\NormalTok{, }\StringTok{"ggExtra"}\NormalTok{, }\StringTok{"ggalluvial"}\NormalTok{, }\StringTok{"plotly"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(dataviz_packages)}
\KeywordTok{lapply}\NormalTok{(packages, require, }\DataTypeTok{character.only =} \OtherTok{TRUE}\NormalTok{)}

\CommentTok{# Load already installed packages}
\KeywordTok{library}\NormalTok{(}\StringTok{"data.table"}\NormalTok{)}

\CommentTok{# we will also use cowplot later in this session.}
\CommentTok{# Please install it but do not load it for now.}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"cowplot"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{introduction-to-ggplot2}{%
\section{\texorpdfstring{Introduction to \texttt{ggplot2}}{Introduction to ggplot2}}\label{introduction-to-ggplot2}}

This section will demonstrate how to visualise your big data using \texttt{ggplot2} and other R packages that rely on \texttt{ggplot2}. We use `\texttt{ggplot2} because it is the most elegant and versatile visualization package in R. Also, it implements a simple grammar of graphics for building a variety of visualizations for either small or large data. This enables creating high-quality plots for publications and presentations easily, with minimal amounts of adjustments and tweaking.

A typical \texttt{ggplot2} template ranges from a few layers to many layers, depending on the complexity of the visualization of interest. Layers generate a plot and plot transformations within the plot. We can combine multiple layers using the + operator. Therefore, plots are built step by step by adding new elements in each layer. A simple \texttt{ggplot2} template is shown below:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ my_data,}
       \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ var1, }\DataTypeTok{y =}\NormalTok{ var2)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_function}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

where the \texttt{ggplot} function uses the two variables (\textbf{var1} and \textbf{var2}) from a dataset (\textbf{my\_data}), and draws a new plot based on a particular geom function (\textbf{geom\_function}). Selecting the variables to be plotted is done through the aesthetic mapping (via the \texttt{aes} function). Depending on the aesthetic mapping of interest, we can split the plot, add colors by a group variable, change the labels for each axis, change the font size, and so on. The `\texttt{ggplot2} package offers many geom functions to draw different types of plots:

\begin{itemize}
\tightlist
\item
  \texttt{geom\_point} for scatter plots, dot plots, etc.
\item
  \texttt{geom\_boxplot} for boxplots
\item
  \texttt{geom\_line} for trend lines, time series, etc.
\end{itemize}

In addition, functions such as \texttt{theme\_bw()} and \texttt{theme()} enable adjusting the theme elements (e.g., font size, font type, background colors) for a given plot. As we create plots in our examples, we will use some of these theme elements to make our plots look nicer.

An important caveat in visualizing big data is that the size of the dataset (\emph{especially the number of rows}) and complexity level of the plot (e.g., additional lines, colors, facets) will influence how quickly and successfully \texttt{ggplot2} can render the desired plot. Nobody can absorb the meaning of thousands of data points presented on a single visualization. Therefore, in some cases we will need to find a way to cluster or reduce the magnitude of items to visualize before we render the visualization. Typically we can achieve this by:

\begin{itemize}
\tightlist
\item
  taking smaller, sometimes random, samples from our big data, or
\item
  summarizing our big data using categorical, group variables (e.g., gender, grade, year).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{marginal-plots}{%
\section{Marginal plots}\label{marginal-plots}}

We can use marginal plots to examine the distributions of individual variables in a large dataset. A typical marginal plot is a scatter plot that also has histograms or boxplots in the margins of the x- and y-axes. In this section, first we will create histograms and boxplots for the variables in the \textbf{pisa} dataset. Then, we will review other options where we will combine multiple variables and different types of plots in a single visualization.

To demonstrate data visualizations, we will first take a subset of the \textbf{pisa} dataset by selecting some countries and some variables of interest. The selected variables are shown below.

\begin{table}

\caption{\label{tab:tab1}Variables to be used in the data visualizations}
\centering
\begin{tabular}[t]{l|l|l|l}
\hline
Variable & Description & Variable & Description\\
\hline
CNT & Country & BELONG & Sense of belonging to school\\
\hline
OECD & OECD membership & EMOSUPS & Parents emotional support\\
\hline
CNTSTUID & Student ID & HOMESCH & ICT use outside of school for schoolwork\\
\hline
W\_FSTUWT & Student weight in the PISA database & ENTUSE & ICT use outside of school leisure\\
\hline
ST001D01T & Grade level & ICTHOME & ICT available at home\\
\hline
ST004D01T & Gender (female/male) & ICTSCH & ICT availability at school\\
\hline
ST011Q04TA & Possessing a computer at home & WEALTH & Family wealth\\
\hline
ST011Q05TA & Possessing educational software at home & PARED & Highest parental education in years of schooling\\
\hline
ST011Q06TA & Having internet access at home & TMINS & Total learning time per week\\
\hline
ST071Q02NA & Additional time spent for learning math & ESCS & Index of economic, social and cultural status\\
\hline
ST071Q01NA & Additional time spent for learning science & TDTEACH & Teacher-directed science instruction\\
\hline
ST123Q02NA & Whether parents support educational efforts and achievements & IBTEACH & Inquiry based science instruction\\
\hline
ST082Q01NA & Prefering working as part of a team to working alone & TEACHSUP & Teacher support in science classes\\
\hline
ST119Q01NA & Wanting top grades in most or all courses & SCIEEFF & Science self-efficacy\\
\hline
ST119Q05NA & Wanting to the best student in class & math & Students math scores in PISA 2015\\
\hline
ANXTEST & Test anxiety & reading & Students reading scores in PISA 2015\\
\hline
COOPERATE & Enjoying cooperation & science & Students science scores in PISA 2015\\
\hline
\end{tabular}
\end{table}

Here we filter our big data based on a list of countries (we called \texttt{country}), select the variables that we have just identified in Table \ref{tab:tab1} and the reading, math, and science scales we created earlier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{country <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"United States"}\NormalTok{, }\StringTok{"Canada"}\NormalTok{, }\StringTok{"Mexico"}\NormalTok{, }\StringTok{"B-S-J-G (China)"}\NormalTok{, }\StringTok{"Japan"}\NormalTok{,}
             \StringTok{"Korea"}\NormalTok{, }\StringTok{"Germany"}\NormalTok{, }\StringTok{"Italy"}\NormalTok{, }\StringTok{"France"}\NormalTok{, }\StringTok{"Brazil"}\NormalTok{, }\StringTok{"Colombia"}\NormalTok{, }\StringTok{"Uruguay"}\NormalTok{,}
             \StringTok{"Australia"}\NormalTok{, }\StringTok{"New Zealand"}\NormalTok{, }\StringTok{"Jordan"}\NormalTok{, }\StringTok{"Israel"}\NormalTok{, }\StringTok{"Lebanon"}\NormalTok{)}

\NormalTok{dat <-}\StringTok{ }\NormalTok{pisa[CNT }\OperatorTok{%in%}\StringTok{ }\NormalTok{country,}
\NormalTok{            .(CNT, OECD, CNTSTUID, W_FSTUWT, sex, female,}
\NormalTok{              ST001D01T, computer, software, internet,}
\NormalTok{              ST011Q05TA, ST071Q02NA, ST071Q01NA, ST123Q02NA,}
\NormalTok{              ST082Q01NA, ST119Q01NA, ST119Q05NA, ANXTEST,}
\NormalTok{              COOPERATE, BELONG,  EMOSUPS, HOMESCH, ENTUSE,}
\NormalTok{              ICTHOME, ICTSCH, WEALTH, PARED, TMINS, ESCS,}
\NormalTok{              TEACHSUP, TDTEACH, IBTEACH, SCIEEFF,}
\NormalTok{              math, reading, science)}
\NormalTok{            ]}
\end{Highlighting}
\end{Shaded}

Next, we create additional variables by recoding some of the existing variables. The goal is to create some numerical variables out of the character variables in case we want to use them in the modeling stage.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Let's create additional variables that we will use for visualizations}
\NormalTok{dat <-}\StringTok{ }\NormalTok{dat[, }\StringTok{`}\DataTypeTok{:=}\StringTok{`}\NormalTok{ (}
  \CommentTok{# New grade variable}
  \DataTypeTok{grade =}\NormalTok{ (}\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{sapply}\NormalTok{(ST001D01T, }\ControlFlowTok{function}\NormalTok{(x) \{}
  \ControlFlowTok{if}\NormalTok{(x}\OperatorTok{==}\StringTok{"Grade 7"}\NormalTok{) }\StringTok{"7"}
  \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (x}\OperatorTok{==}\StringTok{"Grade 8"}\NormalTok{) }\StringTok{"8"}
  \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (x}\OperatorTok{==}\StringTok{"Grade 9"}\NormalTok{) }\StringTok{"9"}
  \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (x}\OperatorTok{==}\StringTok{"Grade 10"}\NormalTok{) }\StringTok{"10"}
  \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (x}\OperatorTok{==}\StringTok{"Grade 11"}\NormalTok{) }\StringTok{"11"}
  \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (x}\OperatorTok{==}\StringTok{"Grade 12"}\NormalTok{) }\StringTok{"12"}
  \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (x}\OperatorTok{==}\StringTok{"Grade 13"}\NormalTok{) }\OtherTok{NA_character_}
  \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (x}\OperatorTok{==}\StringTok{"Ungraded"}\NormalTok{) }\OtherTok{NA_character_}\NormalTok{\}))),}
  \CommentTok{# Total learning time as hours}
  \DataTypeTok{learning =} \KeywordTok{round}\NormalTok{(TMINS}\OperatorTok{/}\DecValTok{60}\NormalTok{, }\DecValTok{0}\NormalTok{),}
  \CommentTok{# Regions for selected countries}
  \DataTypeTok{Region =}\NormalTok{ (}\KeywordTok{sapply}\NormalTok{(CNT, }\ControlFlowTok{function}\NormalTok{(x) \{}
  \ControlFlowTok{if}\NormalTok{(x }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Canada"}\NormalTok{, }\StringTok{"United States"}\NormalTok{, }\StringTok{"Mexico"}\NormalTok{)) }\StringTok{"N. America"}
    \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (x }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Colombia"}\NormalTok{, }\StringTok{"Brazil"}\NormalTok{, }\StringTok{"Uruguay"}\NormalTok{)) }\StringTok{"S. America"}
    \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (x }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Japan"}\NormalTok{, }\StringTok{"B-S-J-G (China)"}\NormalTok{, }\StringTok{"Korea"}\NormalTok{)) }\StringTok{"Asia"}
    \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (x }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Germany"}\NormalTok{, }\StringTok{"Italy"}\NormalTok{, }\StringTok{"France"}\NormalTok{)) }\StringTok{"Europe"}
    \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (x }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Australia"}\NormalTok{, }\StringTok{"New Zealand"}\NormalTok{)) }\StringTok{"Australia"}
    \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (x }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Israel"}\NormalTok{, }\StringTok{"Jordan"}\NormalTok{, }\StringTok{"Lebanon"}\NormalTok{)) }\StringTok{"Middle-East"}
\NormalTok{    \}))}
\NormalTok{  )]}
\end{Highlighting}
\end{Shaded}

Now, let's see the number of rows in the final dataset and print the first few rows of the selected variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# N count for the final dataset}
\NormalTok{dat[,.N] }\CommentTok{# 158,061 rows}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 158061
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Let's preview the final data}
\KeywordTok{head}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          CNT OECD CNTSTUID W_FSTUWT    sex female ST001D01T computer
## 1: Australia  Yes  3610676 28.19991 Female      1  Grade 10        1
## 2: Australia  Yes  3611874 28.19991 Female      1  Grade 10        1
## 3: Australia  Yes  3601769 28.19991 Female      1  Grade 10        1
## 4: Australia  Yes  3605996 28.19991 Female      1  Grade 10        1
## 5: Australia  Yes  3608147 33.44862   Male      0  Grade 10        1
## 6: Australia  Yes  3610012 33.44862   Male      0  Grade 10        1
##    software internet ST011Q05TA ST071Q02NA ST071Q01NA     ST123Q02NA
## 1:        1        1        Yes          0          1       Disagree
## 2:        1        1        Yes          1          1          Agree
## 3:        1        1        Yes         NA         NA          Agree
## 4:        1        1        Yes          5          7 Strongly agree
## 5:        1        1        Yes          1          1          Agree
## 6:        1        1        Yes          2          2          Agree
##           ST082Q01NA     ST119Q01NA     ST119Q05NA ANXTEST COOPERATE
## 1:          Disagree          Agree Strongly agree -0.1522    0.2085
## 2:             Agree          Agree       Disagree  0.2594   -0.2882
## 3: Strongly disagree Strongly agree       Disagree  2.5493   -1.2109
## 4: Strongly disagree Strongly agree Strongly agree  0.2563    0.3950
## 5:             Agree          Agree       Disagree  0.4517   -1.3606
## 6:             Agree          Agree          Agree  0.5175    0.4252
##     BELONG EMOSUPS HOMESCH  ENTUSE ICTHOME ICTSCH  WEALTH PARED TMINS
## 1:  0.5073 -2.2547 -0.1686 -0.7369       4      5  0.0592    12  1400
## 2: -0.8021 -0.2511  0.0302 -0.1047       9      6  0.7605    12  1100
## 3: -2.4078 -1.9895  1.2836 -1.5403      11     10 -0.1220    11  1960
## 4: -0.3381  1.0991 -0.0498  0.0342      10      7  0.9314    15  2450
## 5: -0.5050 -1.3298 -0.3355  0.2309      NA      7  0.7905    15  1400
## 6: -0.0099 -0.4263  0.1567  0.6896      10      5  0.7054    15  1400
##       ESCS TEACHSUP TDTEACH IBTEACH SCIEEFF     math  reading  science
## 1:  0.4078       NA      NA      NA      NA 545.8999 586.5175 589.5787
## 2:  0.4500   0.3574  0.0615  0.2208 -0.4041 511.6101 570.8238 557.2042
## 3: -0.5889  -1.0718 -0.6102 -0.2198 -0.9003 478.6052 570.0345 569.4709
## 4:  0.6498   0.6375  0.7979 -0.0282  1.2395 506.0904 531.0690 529.0353
## 5:  0.7675   0.8213  0.1990  1.1477 -0.0746 481.8569 506.4988 504.2148
## 6:  1.1151       NA      NA      NA      NA 455.0202 456.4882 472.6377
##    grade learning    Region
## 1:    10       23 Australia
## 2:    10       18 Australia
## 3:    10       33 Australia
## 4:    10       41 Australia
## 5:    10       23 Australia
## 6:    10       23 Australia
\end{verbatim}

We want to see the distributions of the science scores across the 17 countries in our final dataset. The first line with \texttt{ggplot} creates a layout for our figure, the second line draws a box plot using \texttt{geom\_boxplot}, the fourth line with \texttt{labs} creates labels of the axes, and the last line with \texttt{theme\_bw} removes the default theme with a grey background and activates the dark-on-light \texttt{ggplot2} theme -- which is much better for publications and presentations (see \url{https://ggplot2.tidyverse.org/reference/ggtheme.html} for a complete list of themes available in \texttt{ggplot2}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CNT, }\DataTypeTok{y =}\NormalTok{ science)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x=}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{y=}\StringTok{"Science Scores"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-7-1.pdf}

The resulting plot is not necessarily nice because all the country names on the x-axis seem to be squeezed together and thus some of the country names are not visible on the x-axis. To correct this, we may want to flip the coordinates of the plot and use country names on the y-axis instead. The \texttt{coord\_flip()} function allows us to achieve that very easily.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat,}
       \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CNT, }\DataTypeTok{y =}\NormalTok{ science)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x=}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{y=}\StringTok{"Science Scores"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-8-1.pdf}

Next, I want to show the mean values in the boxplots since the line in the middle represents the median, not the mean. To achieve this, we first calculate the means by countries.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{means <-}\StringTok{ }\NormalTok{dat[,}
\NormalTok{             .(}\DataTypeTok{science =} \KeywordTok{mean}\NormalTok{(science)),}
\NormalTok{             by =}\StringTok{ }\NormalTok{CNT]}
\end{Highlighting}
\end{Shaded}

Now we can use \texttt{means} to add a point into each boxplot to show the mean score by countries. We will use \texttt{stat\_summary()} along with the options \texttt{colour\ =\ "blue",\ geom\ =\ "point"} to create a blue point for the mean. In addition, given that the average science score in PISA 2015 was 493 across all participating countries (see \href{https://www.oecd.org/pisa/pisa-2015-results-in-focus.pdf}{PISA 2015 Results in Focus} for more details), we can add a reference line into our plot to identify the average score, which would then allow us to visually examine which countries are above or below the average score. To achieve this, we use \texttt{geom\_hline} function and specify where it should intersect the plot (i.e., \texttt{yintercept\ =\ 493}). We also want the reference line to be a red, dashed-line with a thickness level of 1 -- to make it more visible in the plot. Finally, to facilitate the interpretation of the plot, we want the boxplots to be ordered based on the average scores for each country and thus we add \texttt{reorder(CNT,\ science)} into the mapping.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat,}
       \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{reorder}\NormalTok{(CNT, science), }\DataTypeTok{y =}\NormalTok{ science)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{stat_summary}\NormalTok{(}\DataTypeTok{fun.y =}\NormalTok{ mean, }\DataTypeTok{colour =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{geom =} \StringTok{"point"}\NormalTok{, }
               \DataTypeTok{shape =} \DecValTok{18}\NormalTok{, }\DataTypeTok{size =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x=}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{y=}\StringTok{"Science Scores"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =} \DecValTok{493}\NormalTok{, }\DataTypeTok{linetype=}\StringTok{"dashed"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-9b-1.pdf}

Now let's add some colors to our figure based on the region where each country is located. In order to do this, we use the region variable to fill the boxplots with color, using \texttt{fill\ =\ Region}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat,}
       \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{reorder}\NormalTok{(CNT, science), }\DataTypeTok{y =}\NormalTok{ science, }\DataTypeTok{fill =}\NormalTok{ Region)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x=}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{y=}\StringTok{"Science Scores"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =} \DecValTok{493}\NormalTok{, }\DataTypeTok{linetype=}\StringTok{"dashed"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-10-1.pdf}

\hypertarget{exercise}{%
\subsection{Exercise}\label{exercise}}

Create a plot of \textbf{math} scores over countries with different colors based on region. You need to modify the R code below by replacing \texttt{geom\_boxplot} with:

\begin{itemize}
\tightlist
\item
  \texttt{geom\_point(aes(color\ =\ Region))}, and then
\item
  \texttt{geom\_violin(aes(color\ =\ Region))}.
\end{itemize}

How long did it take to create both plots? Which one is a better way to visualize this type of data?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat,}
       \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{reorder}\NormalTok{(CNT, math), }\DataTypeTok{y =}\NormalTok{ math, }\DataTypeTok{fill =}\NormalTok{ Region)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x=}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{y=}\StringTok{"Math Scores"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =} \DecValTok{490}\NormalTok{, }\DataTypeTok{linetype=}\StringTok{"dashed"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

We can also create histograms (or density plots) for a particular variable and split the plot into multiple plots by using a categorical, group variable. In the following example, we use \texttt{x\ =\ Region} in the mapping in order to identify different regions in the distribution of the science scores. In addition, we use \texttt{facet\_grid(.\ \textasciitilde{}\ sex)} to generate separate histograms by gender. Note that we also added \texttt{title\ =\ "Science\ Scores\ by\ Gender\ and\ Region"} as a title in the \texttt{labs} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat,}
       \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ science, }\DataTypeTok{fill =}\NormalTok{ Region)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{bins =} \DecValTok{50}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Science Scores"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Count"}\NormalTok{,}
       \DataTypeTok{title =} \StringTok{"Science Scores by Gender and Region"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(. }\OperatorTok{~}\StringTok{ }\NormalTok{sex) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-11-1.pdf}

If we are interested in visualizing multiple variables, plotting each variable individually can be time consuming. Therefore, we can use the \texttt{ggpairs} function from the \texttt{GGally} package to build a more complex, diagnostic plot for multiple variables.

In the following example, we plot reading, science, and math scores as well as gender (i.e., sex) in the same plot. Because our dataset is quite large, plotting all the data points would result in a highly complex plot where most data points would overlap on each other. Therefore, we will take a random sample of 500 cases from each region defined in the data, save this smaller dataset as \texttt{dat\_small}, and use this dataset inside the \texttt{ggpairs} function. We colorize each variable by region (using \texttt{mapping\ =\ aes(color\ =\ Region)}). The resulting plot shows density plots for the continuous variables (by region), a stacked bar chart for gender, and box plots for the continuous variables by region and gender.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Random sample of 500 students from each region}
\NormalTok{dat_small <-}\StringTok{ }\NormalTok{dat[,.SD[}\KeywordTok{sample}\NormalTok{(.N, }\KeywordTok{min}\NormalTok{(}\DecValTok{500}\NormalTok{,.N))], by =}\StringTok{ }\NormalTok{Region]}

\KeywordTok{ggpairs}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat_small,}
        \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ Region),}
        \DataTypeTok{columns =} \KeywordTok{c}\NormalTok{(}\StringTok{"reading"}\NormalTok{, }\StringTok{"science"}\NormalTok{, }\StringTok{"math"}\NormalTok{, }\StringTok{"sex"}\NormalTok{),}
        \DataTypeTok{upper =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{continuous =} \KeywordTok{wrap}\NormalTok{(}\StringTok{"cor"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{2.5}\NormalTok{))}
\NormalTok{        )}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-12-1.pdf}

\textbf{Interpretation:}

\begin{itemize}
\tightlist
\item
  What can we say about the regions based on the plots above?
\item
  Do you see any major gender differences for reading, science, or math?
\item
  What is the relationship among reading, science, or math?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{conditional-plots}{%
\section{Conditional plots}\label{conditional-plots}}

When we deal with continuous variables, an effective way to understand the relationship between the variables is to produce conditional plots, such as scatterplots, dotplots, and bubble charts. Simple scatterplots in R can be created using \texttt{plot(var1,\ var2,\ data\ =\ name\_of\_dataset)}. Using the extended capabilities of \texttt{ggplot2} via the \texttt{ggExtra} package, we can combine histograms and density plots with scatterplots and visualize them together.

In the following example, we first create a scatterplot of learning time per week and science scores using \texttt{ggplot}. We use \texttt{geom\_point} to draw a plot with points and \texttt{geom\_smooth(method\ =\ "loess")} to add a regression line with loess smoothing (i.e., \textbf{Lo}cally \textbf{E}stimated \textbf{S}catterplot \textbf{S}moothing). We save this plot as \texttt{p1} and then pass it to \texttt{ggMarginal} to transform the plot into a marginal scatterplot. Inside \texttt{ggMarginal}, we use \texttt{type\ =\ "histogram"} to create histograms for learning time per week and science scores on the x and y axes of the plot. Note that as the plot is created, you may see some warning messages, such as ``Removed 750 rows containing missing values'', because some variables have missing rows in the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat_small,}
             \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ learning, }\DataTypeTok{y =}\NormalTok{ science)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"loess"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Weekly Learning Time"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Science Scores"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}

\CommentTok{# Replace "histogram" with "boxplot" or "density" for other types}
\KeywordTok{ggMarginal}\NormalTok{(p1, }\DataTypeTok{type =} \StringTok{"histogram"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-14-1.pdf}

We can also distinguish male and female students in the plot and create a scatterplot of learning time and science scores with densities by gender. To achieve this, we add \texttt{colour\ =\ sex} into the mapping of \texttt{ggplot} and change the type of plot to \texttt{type\ =\ "density"} in \texttt{ggMarginal}. In addition, we use \texttt{groupColour\ =\ TRUE,\ groupFill\ =\ TRUE} inside \texttt{ggMarginal} to use separate colors for each gender in the density plots.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat_small,}
             \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ learning, }\DataTypeTok{y =}\NormalTok{ science,}
                           \DataTypeTok{colour =}\NormalTok{ sex)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"loess"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Weekly Learning Time"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Science Scores"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"bottom"}\NormalTok{,}
        \DataTypeTok{legend.title =} \KeywordTok{element_blank}\NormalTok{())}

\KeywordTok{ggMarginal}\NormalTok{(p2, }\DataTypeTok{type =} \StringTok{"density"}\NormalTok{, }\DataTypeTok{groupColour =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{groupFill =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-15-1.pdf}

\textbf{Interpretation:}

\begin{itemize}
\tightlist
\item
  What can we say about the relationship between weekly learning time and science scores?
\item
  Do you see any gender differences?
\end{itemize}

Now let's incorporate more variables into the plot. This time we are not going to use marginal plots. Instead, we will create a regular scatterplot but add other layers to represent additional variables. In the following example, we examine the relationship between students' weekly learning time (learning) and science scores (science) across regions (region) and gender (sex). Adding \texttt{fill\ =\ Region} into the mapping will allow us to draw regression lines by regions, while adding \texttt{aes(colour\ =\ sex)} into \texttt{geom\_point} will allow us to use different colors for male and female students in the plot.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat_small,}
       \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ learning, }\DataTypeTok{y =}\NormalTok{ science, }\DataTypeTok{fill =}\NormalTok{ Region)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =}\NormalTok{ sex)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"loess"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Weekly Learning Time"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Science Scores"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-16-1.pdf}

The resulting scatterplot is nice but it is hard to compare the results clearly between gender groups and regions. To improve the interpretability of the plot, we will use the faceting option. This will allow us to split the scatterplot into multiple plots based on gender and region. In the following example, we examine the relationship between students' learning time and science scores across regions and gender. We use \texttt{facet\_grid(sex\ \textasciitilde{}\ Region)} to split the plots into multiple rows based on gender and multiple columns based on region.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat_small,}
       \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ learning, }\DataTypeTok{y =}\NormalTok{ science)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"loess"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Weekly Learning Time"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Science Scores"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.title =} \KeywordTok{element_blank}\NormalTok{()) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(sex }\OperatorTok{~}\StringTok{ }\NormalTok{Region)}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-17-1.pdf}

\textbf{Interpretation:}

\begin{itemize}
\tightlist
\item
  Do you see any regional differences?
\item
  Is there any interaction between gender and region?
\end{itemize}

\hypertarget{exercise-1}{%
\subsection{Exercise}\label{exercise-1}}

Create a scatterplot of socio-economic status (\texttt{ESCS}) and math scores (\texttt{math}) across regions (\texttt{region}) and gender (\texttt{sex}). Use \texttt{geom\_smooth(method\ =\ "lm")} to draw linear regression lines (instead of loess smoothing). Do you think that the relationship between ESCS and math changes across gender and regions?

\hypertarget{plots-for-examining-correlations}{%
\section{Plots for examining correlations}\label{plots-for-examining-correlations}}

For a simple examination of the correlation between two continuous variables, we could just create a scatterplot matrix. In the following plot, we will create a scatterplot matrix of family wealth (\texttt{WEALTH}) and science scores (\texttt{science}) by gender (\texttt{sex}) and region (\texttt{region}). We will use region for facetting and gender for coloring the data points.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat_small,}
       \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ WEALTH, }\DataTypeTok{y =}\NormalTok{ science)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ sex)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{( }\OperatorTok{~}\StringTok{ }\NormalTok{Region) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Family Wealth"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Science Scores"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.title =} \KeywordTok{element_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-18-1.pdf}

A more effective way for identifying correlated variables in a dataset for further statistical analyses (also known as feature extraction) is to create a correlation matrix plot. The \texttt{ggcorr()} function from the \texttt{GGally} package provides a quick way to make a correlation matrix plot. In the following example, we will create a correlation matrix plot for science, math, reading, ICT possession at home, socio-economic status, family wealth, highest parental education, science self-efficacy, sense of belonging to school, and grade level.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggcorr}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat[,.(science, math, reading, ICTHOME, ESCS,}
\NormalTok{                     WEALTH, PARED, SCIEEFF, BELONG, grade)],}
       \DataTypeTok{method =} \KeywordTok{c}\NormalTok{(}\StringTok{"pairwise.complete.obs"}\NormalTok{, }\StringTok{"pearson"}\NormalTok{),}
       \DataTypeTok{label =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{label_size =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-19-1.pdf}

\hypertarget{plots-for-examining-means-by-group}{%
\section{Plots for examining means by group}\label{plots-for-examining-means-by-group}}

Let's assume that we want to see average science scores by gender and country. First, we need to find the average science scores by country and gender and save them in a new dataset. Below we calculate average science scores and N counts by both gender and country and save the dataset as \texttt{science\_summary}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{science_summary <-}\StringTok{ }\NormalTok{dat[, }
\NormalTok{                       .(}\DataTypeTok{Science =} \KeywordTok{mean}\NormalTok{(science, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{), }
                         \DataTypeTok{Freq =}\NormalTok{ .N),}
\NormalTok{                       by =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"sex"}\NormalTok{, }\StringTok{"CNT"}\NormalTok{)]}

\KeywordTok{head}\NormalTok{(science_summary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       sex       CNT  Science  Freq
## 1: Female Australia 498.0377  7163
## 2:   Male Australia 499.4419  7367
## 3:   Male    Brazil 400.8134 11068
## 4: Female    Brazil 396.2647 12073
## 5: Female    Canada 515.3443 10022
## 6:   Male    Canada 517.2765 10036
\end{verbatim}

Now, we can create a simple bar graph summarizing the average science performance by gender and country, using our new dataset.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ science_summary,}
       \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CNT, }\DataTypeTok{y =}\NormalTok{ Science, }\DataTypeTok{fill =}\NormalTok{ sex)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat =} \StringTok{"identity"}\NormalTok{, }\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{""}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Science Scores"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"Gender"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-21-1.pdf}

Despite their easiness and simplicity, bar graphs are not necessarily visually appealing. Thus, we will create a bubble chart to visualize the same information in a different way. A bubble chart is essentially a weighted scatterplot where a third variable determines the size of the dots in the plot. In the following bubble chart, we use \emph{Freq} (i.e., number of students from each country) to determine the size of the dots in the plot, using \texttt{size\ =\ Freq}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ science_summary,}
       \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CNT, }\DataTypeTok{y =}\NormalTok{ Science, }\DataTypeTok{size =}\NormalTok{ Freq, }\DataTypeTok{fill =}\NormalTok{ sex)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{shape =} \DecValTok{21}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Science Scores"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"Gender"}\NormalTok{,}
       \DataTypeTok{size =} \StringTok{"Frequency"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-22-1.pdf}

\textbf{Interpretation:}

\begin{itemize}
\tightlist
\item
  Which countries seem to have the highest numbers of students?
\item
  Which countries seem to have the larger achievement gap in science between male and female students?
\end{itemize}

We can also create a dot plot, which is very similar to the bubble chart when one of the variables is categorical, to convey the same information even more effectively. As you will see, this is a more polished version of the bubble chart with additional titles and subtitles.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ science_summary, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CNT, }\DataTypeTok{y =}\NormalTok{ Science, }\DataTypeTok{fill =}\NormalTok{ sex)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{group =}\NormalTok{ CNT)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{size =}\NormalTok{ Freq), }\DataTypeTok{shape =} \DecValTok{21}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =} \DecValTok{493}\NormalTok{, }\DataTypeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{y =} \StringTok{"PISA Science Scores"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"Gender"}\NormalTok{, }\DataTypeTok{size =} \StringTok{"Frequency"}\NormalTok{, }
        \DataTypeTok{title =} \StringTok{"Science Performance by Country and Gender"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{plot.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{18}\NormalTok{, }\DataTypeTok{margin =} \KeywordTok{margin}\NormalTok{(}\DataTypeTok{b =} \DecValTok{10}\NormalTok{)), }
    \DataTypeTok{plot.subtitle =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{10}\NormalTok{, }\DataTypeTok{color =} \StringTok{"darkslategrey"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-23-1.pdf}

\hypertarget{plots-for-ordinalcategorical-variables}{%
\section{Plots for ordinal/categorical variables}\label{plots-for-ordinalcategorical-variables}}

An \emph{alluvial plot} can be used to summarize relationships between multiple categorical variables. In the following example, we use region (Region), gender (sex), and a survey item regarding whether parents support educational efforts and achievements (ST123Q02NA). We first create a new dataset called \texttt{dat\_alluvial} to have frequency counts by region, gender, and our survey item. Because the survey item includes missing values, we label them as ``missing'' and then recode this variable as a factor with re-ordered levels.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat_alluvial <-}\StringTok{ }\NormalTok{dat[, }
\NormalTok{                    .(}\DataTypeTok{Freq =}\NormalTok{ .N), }
\NormalTok{                    by =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Region"}\NormalTok{, }\StringTok{"sex"}\NormalTok{, }\StringTok{"ST123Q02NA"}\NormalTok{)}
\NormalTok{                    ][,}
\NormalTok{                      ST123Q02NA }\OperatorTok{:}\ErrorTok{=}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(ST123Q02NA }\OperatorTok{==}\StringTok{ ""}\NormalTok{, }\StringTok{"Missing"}\NormalTok{, ST123Q02NA))}
\NormalTok{                      ]}
\KeywordTok{levels}\NormalTok{(dat_alluvial}\OperatorTok{$}\NormalTok{ST123Q02NA) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Strongly disagree"}\NormalTok{, }\StringTok{"Disagree"}\NormalTok{, }\StringTok{"Agree"}\NormalTok{,}
                                     \StringTok{"Strongly agree"}\NormalTok{, }\StringTok{"Missing"}\NormalTok{)}

\KeywordTok{head}\NormalTok{(dat_alluvial)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       Region    sex        ST123Q02NA Freq
## 1: Australia Female          Disagree  232
## 2: Australia Female Strongly disagree 2773
## 3: Australia Female    Strongly agree 5981
## 4: Australia   Male Strongly disagree 3209
## 5: Australia   Male    Strongly agree 5626
## 6: Australia   Male           Missing  186
\end{verbatim}

Unlike the previous visualizations, there is a new layer called \texttt{geom\_alluvium}, which allows creating an alluvial plot using the \texttt{ggplot} function. We use \texttt{aes(fill\ =\ sex)} inside \texttt{geom\_alluvium} to differentiate the frequencies by gender.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# StatStratum <- StatStratum}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat_alluvial,}
       \KeywordTok{aes}\NormalTok{(}\DataTypeTok{axis1 =}\NormalTok{ Region, }\DataTypeTok{axis2 =}\NormalTok{ ST123Q02NA, }\DataTypeTok{y =}\NormalTok{ Freq)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_discrete}\NormalTok{(}\DataTypeTok{limits =} \KeywordTok{c}\NormalTok{(}\StringTok{"Region"}\NormalTok{, }\StringTok{"Parents supporting}\CharTok{\textbackslash{}n}\StringTok{achievement"}\NormalTok{),}
                   \DataTypeTok{expand =} \KeywordTok{c}\NormalTok{(.}\DecValTok{1}\NormalTok{, }\FloatTok{.05}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_alluvium}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{ sex)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_stratum}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_text}\NormalTok{(}\DataTypeTok{stat =} \StringTok{"stratum"}\NormalTok{, }\DataTypeTok{label.strata =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Demographics"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Frequency"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"Gender"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-25-1.pdf}

\textbf{Interpretation:}

\begin{itemize}
\tightlist
\item
  Does parents' support for educational efforts and achievement vary by region and gender?
\end{itemize}

\hypertarget{exercise-2}{%
\subsection{Exercise}\label{exercise-2}}

Create an alluvial plot for the survey item (ST119Q01NA) of whether students want top grades in most or all courses by region (Region) and gender (sex). Below we create the summary dataset (dat\_alluvial2) for this plot. Use this dataset to draw the alluvial plot plot. How should we interpret the plot (e.g., for each region)?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat_alluvial2 <-}\StringTok{ }\NormalTok{dat[,}
\NormalTok{                     .(}\DataTypeTok{Freq =}\NormalTok{ .N),}
\NormalTok{                     by =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Region"}\NormalTok{, }\StringTok{"sex"}\NormalTok{, }\StringTok{"ST119Q01NA"}\NormalTok{)}
\NormalTok{                     ][,}
\NormalTok{                       ST119Q01NA }\OperatorTok{:}\ErrorTok{=}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(ST119Q01NA }\OperatorTok{==}\StringTok{ ""}\NormalTok{, }\StringTok{"Missing"}\NormalTok{, ST119Q01NA))]}

\KeywordTok{levels}\NormalTok{(dat_alluvial2}\OperatorTok{$}\NormalTok{ST119Q01NA) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Strongly disagree"}\NormalTok{, }\StringTok{"Disagree"}\NormalTok{, }\StringTok{"Agree"}\NormalTok{,}
                                      \StringTok{"Strongly agree"}\NormalTok{, }\StringTok{"Missing"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{interactive-plots-with-plotly}{%
\section{\texorpdfstring{Interactive plots with \texttt{plotly}}{Interactive plots with plotly}}\label{interactive-plots-with-plotly}}

Using the \texttt{plotly} package, we can make more interactive visualizations. The \texttt{ggplotly} function from the \texttt{plotly} package transforms a \texttt{ggplot2} plot into an interactive plot in the HTML format. In the following example, we first save a boxplot as \texttt{p3} and then insert this plot into the \texttt{plotly} function in order to generate an interactive plot. As we hover the pointer over the plot area, the plot shows the min, max, q1, q3, and median values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p3 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat,}
             \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CNT, }\DataTypeTok{y =}\NormalTok{ science, }\DataTypeTok{fill =}\NormalTok{ Region))}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(. }\OperatorTok{~}\StringTok{ }\NormalTok{sex)}\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Science Scores"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"Region"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}

\KeywordTok{ggplotly}\NormalTok{(p3)}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-26-1.pdf}

Similarly, we can transform our bubble chart into an interactive plot using \texttt{ggplotly()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p4 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ science_summary,}
       \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CNT, }\DataTypeTok{y =}\NormalTok{ Science, }\DataTypeTok{size =}\NormalTok{ Freq, }\DataTypeTok{fill =}\NormalTok{ sex)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{shape =} \DecValTok{21}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Science Scores"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"Gender"}\NormalTok{,}
       \DataTypeTok{size =} \StringTok{"Frequency"}\NormalTok{)}

\KeywordTok{ggplotly}\NormalTok{(p4)}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-27-1.pdf}

We can also use the \texttt{plot\_ly} function to create interactive visualizations, without using `\texttt{ggplot2}. In the following example, we create a scatterplot of reading scores and science scores where the color of the dots will be based on region and the size of the dots will be based on student weight in the PISA database. Because the resulting figure is interactive, we can click on the legend and hide some regions as we review the plot. In addition, we add a hover text (\texttt{text\ =\ \textasciitilde{}paste("Reading:\ ",\ reading,\ \textquotesingle{}\textless{}br\textgreater{}Science:\textquotesingle{},\ science)}) into the plot. As we hover on the plot, it will show us a label with reading and science scores.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot_ly}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat_small,}
        \DataTypeTok{x =} \OperatorTok{~}\NormalTok{reading, }\DataTypeTok{y =} \OperatorTok{~}\NormalTok{science, }\DataTypeTok{color =} \OperatorTok{~}\NormalTok{Region,}
        \DataTypeTok{size =} \OperatorTok{~}\NormalTok{W_FSTUWT,}
        \DataTypeTok{type =} \StringTok{"scatter"}\NormalTok{,}
        \DataTypeTok{text =} \OperatorTok{~}\KeywordTok{paste}\NormalTok{(}\StringTok{"Reading: "}\NormalTok{, reading, }\StringTok{'<br>Science:'}\NormalTok{, science))}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-28-1.pdf}

Lastly, we create a bar chart showing average science scores by region and gender. We will also include error bars in the plot. First we will create a new dataset \texttt{science\_region} with the mean and standard deviation values by gender and region. Then, we will use this summary dataset in \texttt{plot\_ly()} to draw a bar chart for females and save it as \texttt{p5}. Finally, we will add a new layer for males using \texttt{add\_trace}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{science_region <-}\StringTok{ }\NormalTok{dat[, .(}\DataTypeTok{Science =} \KeywordTok{mean}\NormalTok{(science, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{), }
                          \DataTypeTok{SD =} \KeywordTok{sd}\NormalTok{(science, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)),}
\NormalTok{                        by =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"sex"}\NormalTok{, }\StringTok{"Region"}\NormalTok{)]}

\NormalTok{p5 <-}\StringTok{ }\KeywordTok{plot_ly}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ science_region[}\KeywordTok{which}\NormalTok{(science_region}\OperatorTok{$}\NormalTok{sex }\OperatorTok{==}\StringTok{ 'Female'}\NormalTok{),], }
        \DataTypeTok{x =} \OperatorTok{~}\NormalTok{Region, }
        \DataTypeTok{y =} \OperatorTok{~}\NormalTok{Science, }
        \DataTypeTok{type =} \StringTok{'bar'}\NormalTok{, }
        \DataTypeTok{name =} \StringTok{'Female'}\NormalTok{,}
        \DataTypeTok{error_y =} \OperatorTok{~}\KeywordTok{list}\NormalTok{(}\DataTypeTok{array =}\NormalTok{ SD, }\DataTypeTok{color =} \StringTok{'black'}\NormalTok{))}

\KeywordTok{add_trace}\NormalTok{(p5, }\DataTypeTok{data =}\NormalTok{ science_region[}\KeywordTok{which}\NormalTok{(science_region}\OperatorTok{$}\NormalTok{sex }\OperatorTok{==}\StringTok{ 'Male'}\NormalTok{),], }
          \DataTypeTok{name =} \StringTok{'Male'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-29-1.pdf}

Check out the \href{https://plot.ly/r/}{plotly} website to see more interesting examples of interactive visualizations and dashboards.

\hypertarget{exercise-3}{%
\subsection{Exercise}\label{exercise-3}}

Replicate the science-by-region histogram below as a density plot and use\texttt{plotly} to make it interactive. You will need to replace \texttt{geom\_histogram(alpha\ =\ 0.5,\ bins\ =\ 50)} with \texttt{geom\_density(alpha\ =\ 0.5)}. Repeat the same process by changing \texttt{alpha\ =\ 0.5} to \texttt{alpha\ =\ 0.8}. Which version is better for examining the science score distribution?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat,}
       \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ science, }\DataTypeTok{fill =}\NormalTok{ Region)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{bins =} \DecValTok{50}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Science Scores"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Count"}\NormalTok{,}
       \DataTypeTok{title =} \StringTok{"Science Scores by Gender and Region"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(. }\OperatorTok{~}\StringTok{ }\NormalTok{sex) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{customizing-visualizations}{%
\section{Customizing visualizations}\label{customizing-visualizations}}

Although \texttt{ggplot2} has many ways to customize visualizations, sometimes making a plot ready for a publication or a presentation becomes quite tedious. Therefore, we recommend the \href{https://cran.r-project.org/web/packages/cowplot/index.html}{cowplot} package -- which is capable of quickly transforming plots created with \texttt{ggplot2} into publication-ready plots. The \texttt{cowplot} package provides a nice theme that requires a minimum amount of editing for changing sizes of axis labels, plot backgrounds, etc. In addition, we can add custom annotations to \texttt{ggplot2} plots using \texttt{cowplot} (see the \href{https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html}{cowplot vignette} for more details).

On of the plots that we created earlier was a bubble chart by gender and frequency.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ science_summary,}
       \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CNT, }\DataTypeTok{y =}\NormalTok{ Science, }\DataTypeTok{size =}\NormalTok{ Freq, }\DataTypeTok{fill =}\NormalTok{ sex)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{shape =} \DecValTok{21}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Science Scores"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"Gender"}\NormalTok{,}
       \DataTypeTok{size =} \StringTok{"Frequency"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-30-1.pdf}

After we load the \texttt{cowplot} package and remove \texttt{theme\_bw} from the plot, it will change as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"cowplot"}\NormalTok{)}

\NormalTok{plot1 <-}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ science_summary,}
         \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CNT, }\DataTypeTok{y =}\NormalTok{ Science, }\DataTypeTok{size =}\NormalTok{ Freq, }\DataTypeTok{fill =}\NormalTok{ sex)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{shape =} \DecValTok{21}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Science Scores"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"Gender"}\NormalTok{,}
       \DataTypeTok{size =} \StringTok{"Frequency"}\NormalTok{)}

\NormalTok{plot1}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-31-1.pdf}

The \texttt{cowplot} package removes the gray background, gridlines, and make the axes more visible. If we want to save the plot, we can export it using \texttt{save\_plot}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{save_plot}\NormalTok{(}\StringTok{"plot1.png"}\NormalTok{, plot1,}
          \DataTypeTok{base_aspect_ratio =} \FloatTok{1.6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Also, \texttt{cowplot} enables combining two or more plots into one graph via the function \texttt{plot\_grid}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot2 <-}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ science_summary,}
         \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CNT, }\DataTypeTok{y =}\NormalTok{ Science, }\DataTypeTok{fill =}\NormalTok{ sex)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat =} \StringTok{"identity"}\NormalTok{, }\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{""}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Science Scores"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"Gender"}\NormalTok{) }


\KeywordTok{plot_grid}\NormalTok{(plot1, plot2, }\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{big-data-in-r_files/figure-latex/ch4-33-1.pdf}

If you decide not to use the \texttt{cowplot} theme, you can just simply unload the package as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{detach}\NormalTok{(}\StringTok{"package:cowplot"}\NormalTok{, }\DataTypeTok{unload=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{lab-1}{%
\section{Lab}\label{lab-1}}

We want to examine the relationships between reading scores and technology-related variables in the \texttt{dat} dataset that we created earlier. Create at least two visualizations (either static or interactive) using some of the variables shown below:

\begin{itemize}
\tightlist
\item
  Region
\item
  sex
\item
  grade
\item
  HOMESCH
\item
  ENTUSE
\item
  ICTHOME
\item
  ICTSCH
\end{itemize}

You can focus on a particular country or region or use the entire dataset for your visualizations.

\hypertarget{modeling-big-data}{%
\chapter{Modeling big data}\label{modeling-big-data}}

\begin{figure}
\centering
\includegraphics{images/machine_learning.png}
\caption{Source: \url{http://tinyurl.com/y95rd2jx}}
\end{figure}

\hypertarget{introduction-to-machine-learning}{%
\section{Introduction to machine learning}\label{introduction-to-machine-learning}}

\begin{quote}
Machine learning is automating the automation -- \href{https://homes.cs.washington.edu/~pedrod/}{Dr.~Pedro Domingos}
\end{quote}

\textbf{Machine Learning (ML)} is an important aspect of modern business applications and research nowadays. Through advanced mathematical models, ML algorithms can figure out how to perform important tasks either intuitively or by generalizing from existing observations (i.e., sample data). This is often feasible and cost-effective where manual programming is not. ML algorithms utilize sample data -- also known as \emph{training data} -- to make decisions without being specifically programmed to make those decisions. As more data points become available, ML algorithms assist computer systems in progressively improving their performance so that more ambitious and complex problems can be tackled. As a result, ML has begun to be widely used in computer science and other fields, including educational measurement and psychometrics. Some ML applications include web search, spam filters for e-mails, recommender systems (e.g., Netflix and YouTube), credit scoring, fraud detection, stock trading, and drug design.

Some examples of ML in educational testing and psychometrics include automated essay scoring applications, personalized learning systems, intelligent tutoring systems, and learning analytics applications to inform instructors, students, and other stakeholders.

\hypertarget{focus-of-machine-learning}{%
\subsection{Focus of machine learning}\label{focus-of-machine-learning}}

As an inductive approach, ML focuses on making accurate predictions based on existing data, \textbf{NOT} necessarily hypothesis testing (see Figure \ref{fig:fig5-1}).

\begin{figure}
\includegraphics[width=1\linewidth]{images/deduction2} \caption{Deduction vs. induction (Source: <https://tinyurl.com/yxtt8afm>)}\label{fig:fig5-1}
\end{figure}

Also, ML aims to learn from the data to tell you how to utilize the variables for a prediction scenario, \textbf{NOT} to give you output for a program that you wrote (see Figure \ref{fig:fig5-2}).

\begin{figure}
\includegraphics[width=1\linewidth]{images/traditional_ml} \caption{Traditional programming vs. machine learning}\label{fig:fig5-2}
\end{figure}

\hypertarget{some-concepts-underlying-machine-learning}{%
\subsection{Some concepts underlying machine learning}\label{some-concepts-underlying-machine-learning}}

Here we want to introduce some important ML concepts, based on \href{https://homes.cs.washington.edu/~pedrod/}{Dr.~Pedro Domingos} of University of Washington titled \href{https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf}{``A Few Useful Things to Know about Machine Learning''}. According to Dr.~Domingos, all machine learning algorithms generally consist of combinations of three elements:

\textbf{(Statistical) Learning from data = Representation + Evaluation + Optimization}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Representation}: A \emph{classifier} is a system that inputs (typically) a vector of discrete and/or continuous feature values (i.e., predictors) and outputs a single discrete or continuous value (i.e., dependent or outcome variable). To build a ML application, a classifier must be represented in some formal language that the computer can handle. Then we should consider questions such as ``how do we present the input data?'', ``how do we select what features/variables to use?'', and so on. We will review some of these classifiers today -- such as decision trees, support vector machines, and logistic regression.
\item
  \textbf{Evaluation}: An \emph{evaluation} function is necessary for distinguishing good classifiers from bad ones. The evaluation function used internally by the algorithm may differ from the external one that we want the classifier to optimize. Common evaluation methods include accuracy/error rate, mean squared or absolute error (for continuous outcomes) and precision, accuracy, recall (i.e., sensitivity), and specificity (for categorical outcomes).
\item
  \textbf{Optimization}: An \emph{optimization} method is necessary for searching among the classifiers in the language for the highest-scoring (i.e., most precise) one. The choice of optimization technique is key to the efficiency of the learner. Some optimization methods include greedy search, gradient descent, and linear programming.
\end{enumerate}

\hypertarget{model-development}{%
\subsection{Model development}\label{model-development}}

There are several elements that impact the success of model development in ML:

\begin{itemize}
\tightlist
\item
  \textbf{Amount of data}: Although there are many sophisticated ML algorithms available to researchers and practitioners, they all rely on the same thing -- \emph{data}. Selecting clever ML algorithms that are capable of making the most of the available data and computing resources is important. However, without enough data, even the most sophisticated ML algorithms will return poor-quality results. Nowadays enormous amounts of data are available, but there is not enough time to process all of it.
\end{itemize}

\begin{quote}
You can have data without information, but you cannot have information without data. -- \href{https://en.wikipedia.org/wiki/Daniel_Keys_Moran}{Daniel Keys Moran}
\end{quote}

\begin{itemize}
\tightlist
\item
  \textbf{Data quality}: Data quality is the essence of ML applications. ML is not magic; it can't get something out of nothing. The better quality data we provide, the more reliable and precise results we can obtain.
\end{itemize}

\begin{quote}
More data beats clever algorithms, but better data beats more data. -- \href{https://en.wikipedia.org/wiki/Peter_Norvig}{Peter Norvig}
\end{quote}

\begin{itemize}
\tightlist
\item
  \textbf{Data wrangling}: Big data are often not in a form that is amenable to learning, but we can construct new features from the data -- which is typically where most of the effort in a ML project goes. Data wrangling is the most essential skill for building a successful ML model. The processes of gathering data, integrating it, cleaning it, and pre-processing it are very time-consuming. Furthermore, ML is not a one-time process of building data and running a model to learn from the data, but rather an iterative process of running the model, analyzing the results, modifying the data, tweaking the model, and repeating.
\end{itemize}

\begin{quote}
Data scientists spend 60\% of their time on cleaning and organizing data. -- \href{https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/\#7dfde9af6f63}{Gil Press}
\end{quote}

\begin{itemize}
\tightlist
\item
  \textbf{Feature engineering}: Feature engineering is the key to building a successful ML model. Feature engineering refers to selecting and/or creating the most useful variables in a big dataset for a given purpose (e.g., classification). If there are many independent features that correlate well with the outcome variable, then the learning process is easy. If, however, the outcome variable is a very complex function of the features, then learning doesn't occur very easily. Categorical features nearly always need some treatment where we can use one hot encoding (similar to dummy coding) to convert such features into a form that could be provided to ML algorithms to do a better job in prediction.
\end{itemize}

\begin{quote}
Applied machine learning is basically feature engineering. -- \href{https://en.wikipedia.org/wiki/Andrew_Ng}{Andrew Ng}
\end{quote}

\hypertarget{model-evaluation}{%
\subsection{Model evaluation}\label{model-evaluation}}

ML models that focus on classification problems are often evaluated based on classification accuracy, sensitivity, specificity, and precision (see Figure \ref{fig:fig5-3a}).

\begin{figure}
\includegraphics[width=1\linewidth]{images/confusion_matrix_combined} \caption{Confusion matrix for classification problems}\label{fig:fig5-3a}
\end{figure}

ML models that focus on regression problems are evaluated based on the fitted regression line and actual data points, as shown in Figure \ref{fig:fig5-3b}. Using the difference between observed data points (blue) and predicted data points (red), we can creata a summary index of error -- such as mean absolute error, mean squared error, and root mean squared error.

\begin{figure}
\includegraphics[width=0.95\linewidth]{images/regression} \caption{A demonstration of simple linear regression}\label{fig:fig5-3b}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Mean Absolute Error (MAE):}
\end{enumerate}

\[
MAE = \frac{1}{N} \sum_{j=1}^N |y_i-\hat{y}_i|
\]

where \(N\) is the number of observations, \(y_i\) is the observed value of the outcome variable for observation \(i\), and \(\hat{y}_i\) is the predicted value for the outcome variable for observation \(i\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Mean Squared Error (MSE):}
\end{enumerate}

\[
MSE = \frac{1}{N} \sum_{j=1}^N (y_i-\hat{y}_i)^2
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Root Mean Squared Error (MSE):}
\end{enumerate}

\[
RMSE = \sqrt{MSE}
\]

Figure \ref{fig:fig5-3c} shows a typical machine learning pipeline that illustrates the flow of the model development and evaluation elements.

\begin{figure}
\includegraphics[width=1\linewidth]{images/modeling_pipeline} \caption{A typical machine learning pipeline}\label{fig:fig5-3c}
\end{figure}

\hypertarget{key-issues}{%
\subsection{Key issues}\label{key-issues}}

In ML applications, \textbf{generalization} refers to how well the concepts learned by a machine learning model apply to specific examples (i.e., new data that the model hasn't seen yet). Therefore, the fundamental goal of ML is to build a model that can generalize beyond the examples seen in training data. Regardless of how many observations we have in training, the model will produce inaccurate results for at least some observations in the test data. This is primarily because we are very unlikely to see those exact examples from training data again when testing the model with new (or validation) data. That is, getting highly precise results in training data is easy, whereas generalizing the model beyond training data is hard. Therefore, most machine learning beginners would easily fall for the illusion of success with training data and then get immediately disappointed with the results from new data.

When we talk about how well a ML model learns from training data and generalizes to new data, there are two key issues: \textbf{overfitting} and \textbf{underfitting}.

\begin{itemize}
\item
  \textbf{Overfitting} refers to a model that models the training data too well. It happens when a ML model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. In the context of automated essay scoring, overfitting would occur when all the essays (including words, punctuation, word combinations) from the training data are used to maximize the accuracy of the essay scores. Because the model would be very specific to the words or phrases used by students in the training data, the same ML model would yield very poor results when the essay is given to a different group of students who write essays quite differently (e.g., English language learners). Overfitting typically occurs with ML models that implement nonparametric and nonlinear function to learn from the data (e.g., neural network models).
\item
  \textbf{Underfitting} refers to a ML model that can neither model the training data nor generalize to new data -- which means that our ML attempt was a complete failure. An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data. The obvious remedy to underfitting is to try alternate ML algorithms. Nevertheless, it does provide a good contrast to the problem of overfitting.
\end{itemize}

Both overfitting and underfitting may cause poor performance of ML algorithms; but by far the most common problem in ML applications is overfitting. There are two important techniques that we can use when evaluating ML algorithms to limit overfitting:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Use a resampling technique to estimate model accuracy}: The most popular resampling technique is \emph{k}-fold cross validation. This method allows us to train and test our model \emph{k}-times on different subsets of training data and build up an estimate of the performance of a ML model on unseen data. Using cross validation is a gold standard in ML applications for estimating model accuracy on unseen data (see Figure \ref{fig:fig5-4}).
\end{enumerate}

\begin{figure}
\includegraphics[width=1\linewidth]{images/k_fold} \caption{An illustration of *k*-fold cross validation}\label{fig:fig5-4}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Hold back a validation dataset}: If we already have new data (or very large data from which we can spare enough data), using a validation dataset is also an excellent practice.
\end{enumerate}

In conclusion, we ideally want to select a model at the sweet spot between underfitting and overfitting. As we use more data for training the model, we can review the performance of the ML algorithm over time. We can plot both the outcome on the training data and the outcome on the test data we have held back from the training process.

Over time, as the ML algorithm learns, the prediction error for the model on the training data goes down and so does the error on the test data. If we train the model for too long, the performance on the training data may continue to decrease because the model is overfitting and learning irrelevant details and noise in the training dataset. At the same time, the error for the test set starts to increase again as the model's ability to generalize decreases. The sweet spot is the point just before the error on the test data starts to increase where the model has good accuracy on both the training data and the unseen test data.

\hypertarget{types-of-machine-learning}{%
\section{Types of machine learning}\label{types-of-machine-learning}}

In general, ML applications can be categorized in two ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Supervised learning vs.~unsupervised learning
\item
  ML for classification problems vs.~ML for regression problems
\end{enumerate}

In \textbf{supervised learning}, ML algorithms are given \textbf{training data} categorized as input variables and output variables from which to learn patterns and make inferences on previously unseen data (\textbf{testing data}). The goal of supervised learning is for machines to replicate a mapping function we have identified for them (for example, which students passed or failed the test at the end of the semester). Provided enough examples, ML algorithms can learn to recognize and respond to patterns in data without explicit instructions. Supervised machine learning is typically used for \textbf{classification} tasks, in which we segment the data inputs into categories (e.g., for pass/fail decisions, strongly agree/agree/neutral/disagree/strongly disagree), and \textbf{regression} tasks, in which the output variable is a real value, such as a test score. The accuracy of supervised learning algorithms typically is easy to evaluate, because there is a known, ``ground truth''" (output variable) to which the algorithm is optimizing (see Figure \ref{fig:fig5-5}).

\begin{figure}
\includegraphics[width=1\linewidth]{images/supervised_ml} \caption{How supervised machine learning works}\label{fig:fig5-5}
\end{figure}

\textbf{Unsupervised machine learning} is an approach to training ML in which the algorithm is given \textbf{only input data}, from which it identifies patterns on its own. The goal of unsupervised learning is for algorithms to identify underlying patterns or structures in data to better understand it. Unsupervised learning is closer to how humans learn most things in life: through observation, experience, and analogy. Unsupervised learning is best used for clustering problems -- for example, grouping examinees based on their response times and engagement with the items during testing in order to detect anomalies. It is also useful for ``association'', in which ML algorithms independently discover rules in data; for example, students who tend to answer math items slowly also tend to answer science items slowly. The accuracy of unsupervised learning is harder to evaluate, as there is no predefined ground truth the algorithm is working toward (see Figure \ref{fig:fig5-6}).

\begin{figure}
\includegraphics[width=1\linewidth]{images/unsupervised_ml} \caption{How unsupervised machine learning works}\label{fig:fig5-6}
\end{figure}

Figure \ref{fig:fig5-7} below shows most widely used algorithms for both supervised and unsupervised ML applications. The last column in Figure \ref{fig:fig5-7} refers to ``reinforcement learning'' -- a more specific type of machine learning -- but we will not be covering reinforcement learning in this training session.

\begin{figure}
\includegraphics[width=1\linewidth]{images/types_ml} \caption{Widely used machine learning algorithms}\label{fig:fig5-7}
\end{figure}

\hypertarget{supervised-machine-learning---part-i}{%
\chapter{Supervised Machine Learning - Part I}\label{supervised-machine-learning---part-i}}

\hypertarget{decision-trees}{%
\section{Decision Trees}\label{decision-trees}}

Decision trees (also known as classification and regression trees -- CART) are an important type of algorithm for predictive modeling and machine learning. In general, the CART approach relies on \emph{stratifying} or \emph{segmenting} the prediction space into a number of simple regions. In order to make regression-based or classification-based predictions, we use the mean or the mode of the training observations in the region to which they belong.

A typical layout of a decision tree model looks like a binary tree. The tree has a root node that represents the starting point of the prediction. There are also decision nodes where we split the data into a smaller subset and leaf nodes where we make a decision. Each node represents a single input variable (i.e., predictor) and a split point on that variable. The leaf nodes of the tree contain an output variable (i.e., dependent variable) for which we make a prediction. Predictions are made by walking the splits of the tree until arriving at a leaf node and output the class value at that leaf node. Figure \ref{fig:fig6-1} shows an example of a decision tree model in the context of a binary dependent variable (accepting or not accepting a new job offer).

\begin{figure}
\includegraphics[width=1\linewidth]{images/decisiontree} \caption{An example of decision tree approach}\label{fig:fig6-1}
\end{figure}

Although decision trees are not highly competitive with the advanced supervised learning approaches, they are still quite popular in ML applications because they:

\begin{itemize}
\tightlist
\item
  are fast to learn and very fast for making predictions.
\item
  are often accurate for a broad range of problems.
\item
  do not require any special preparation for the data.
\item
  are highly interpretable compared to more complex ML methods (e.g., neural networks).
\item
  are very easy to explain to people as the logic of decision trees closely mirrors human decision-making.
\item
  can be displayed graphically, and thus are easily interpreted even by a non-expert.
\end{itemize}

In a decision tree model, either categorical and continuous variables can be used as the outcome variable depending on whether we want classification trees (categorical outcomes) or regression trees (continuous outcomes). Decision trees are particularly useful when predictors interact well with the outcome variable (and with each other).

\hypertarget{regression-trees}{%
\subsection{Regression trees}\label{regression-trees}}

In regression trees, the following two steps will allow us to create a decision tree model:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We divide the prediction space (with several predictors) into distinct and non-overlapping regions, using a \emph{top-down}, \emph{greedy} approach -- which is also known as \emph{recursive binary splitting}. We begin splitting at the top of the tree and then go down by successively splitting the prediction space into two new branches. This step is completed by dividing the prediction space into high-dimensional rectangles and minimizing the following equation:
\end{enumerate}

\[
RSS=\sum_{i: x_i \in R_1(j,s)}(y_i-\hat{y}_{R_1})^2 + \sum_{i: x_i \in R_2(j,s)}(y_i-\hat{y}_{R_2})^2
\]

where \(RSS\) is the residual sum of squares, \(y_i\) is the observed predicted variable for the observations \(i=(1,2,3, \dots, N)\) in the training data, \(j\) is the index for the \(j^{th}\) split, \(s\) is the cutpoint for a given predictor \(X_i\), \(\hat{y}_{R_1}\) is the mean response for the observations in the \(R_1(j,s)\) region of the training data and \(\hat{y}_{R_2}\) is the mean response for the observations in the \(R_2(j,s)\) region of the training data.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Once all the regions \(R_1, \dots, R_J\) have been created, we predict the response for a given observation using the mean of the observations in the region of the training data to which that observation belongs.
\end{enumerate}

\hypertarget{classification-trees}{%
\subsection{Classification trees}\label{classification-trees}}

A classification tree is very similar to a regression tree, except that the decision tree predicts a qualitative (i.e., categorical) variable rather than a quantitative (i.e., continuous and numerical) variable. The procedure for splitting the data in multiple branches is the same as the one we described for the regression tree above. The only difference is that instead of using the mean of the observations in the region of the training data, we assume that each observation belongs to the \emph{mode} class (i.e., most commonly occurring class) of the observations in the region of the training data. Also, rather than minimizing \(RSS\), we try to minimize the \emph{classification error rate}, which is the fraction of the training observations in a given region that do not belong to the most common class:

\[
E = 1 - max_k(\hat{p}_{mk})
\]

where \(\hat{p}_{mk}\) is the proportion of training observations in the \(m^{th}\) region that are from the \(k^{th}\) class. However, only classification error is \textbf{NOT} good enough to split decision trees. Therefore, there are two other indices for the same purpose:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{The Gini index}:
\end{enumerate}

\[
G = \sum_{k=1}^{K}\hat{p}_{mk}(1-\hat{p}_{mk})
\]

where \(K\) represents the number of classes. This is essentially a measure of total variance across the \(K\) classes. A small Gini index indicates that a node contains predominantly observations from a single class.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Entropy}:
\end{enumerate}

\[
Entropy = -\sum_{k=1}^{K}\hat{p}_{mk}\text{log}\hat{p}_{mk}
\]

Like the Gini index, the entropy will also take on a small value if the \(m^{th}\) node is pure.

When building a classification tree, either the Gini index or the entropy is typically used to evaluate the quality of a particular split, as they are more sensitive to the changes in the splits than the classification error rate. Typically, the Gini index is better for minimizing misclassification, while the Entropy is better for exploratory analysis.

\hypertarget{pruning-decision-trees}{%
\subsection{Pruning decision trees}\label{pruning-decision-trees}}

Sometimes decision trees end up having many branches and nodes, yielding a model that overfits the training data and poorly fits the validation or test data. To eliminate this overfitting problem, we may prefer to have a smaller and more interpretable tree with fewer splits at the cost of a little bias. One strategy to achieve this is to grow a very large tree and then prune it back in order to obtain a \emph{subtree}.

Given a subtree, we can estimate its error in the test or validation data. However, estimating the error for every possible subtree would be computationally too expensive. A more feasible way is to use \emph{cost complexity pruning} by getting a sequence of trees indexed by a nonnegative tuning parameter \(\alpha\) -- which also known as the complexity parameter (cp). The cp parameter controls a trade-off between the subtree's complexity and its fit to the training data. As the cp parameter increases from zero, branches in the decision tree get pruned in a nested and predictable fashion. To determine the ideal value for the cp parameter, we can try different values of cp in a validation set or use cross-validation (e.g., \emph{K}-fold approach). By checking the error (using either RSS, or Gini index, or Entropy depending on the prediction problem) for different sizes of decision trees, we can determine the ideal point to prune the tree.

\hypertarget{decision-trees-in-r}{%
\section{Decision trees in R}\label{decision-trees-in-r}}

In the following example, we will build a classification tree model, using the science scores from PISA 2015. Using a set of predictors in the \textbf{pisa} dataset, we will predict whether students are above or below the mean scale score for science. The average science score in PISA 2015 was 493 across all participating countries (see \href{https://www.oecd.org/pisa/pisa-2015-results-in-focus.pdf}{PISA 2015 Results in Focus} for more details). Using this score as a cut-off value, we will first create a binary variable called \texttt{science\_perf} where \texttt{science\_perf}= High if a student's science score is equal or larger than 493; otherwise \texttt{science\_perf}= Low.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa <-}\StringTok{ }\NormalTok{pisa[, science_perf }\OperatorTok{:}\ErrorTok{=}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(science }\OperatorTok{>=}\StringTok{ }\DecValTok{493}\NormalTok{, }\StringTok{"High"}\NormalTok{, }\StringTok{"Low"}\NormalTok{))]}
\end{Highlighting}
\end{Shaded}

In addition, we will subset the students from the United States and Canada and choose some variables (rather than the entire set of variables) to make our example relatively simple and manageable in terms of time. We will use the following variables in our model:

\begin{longtable}[]{@{}ll@{}}
\toprule
Label & Description\tabularnewline
\midrule
\endhead
WEALTH & Family wealth (WLE)\tabularnewline
HEDRES & Home educational resources (WLE)\tabularnewline
ENVAWARE & Environmental Awareness (WLE)\tabularnewline
ICTRES & ICT Resources (WLE)\tabularnewline
EPIST & Epistemological beliefs (WLE)\tabularnewline
HOMEPOS & Home possessions (WLE)\tabularnewline
ESCS & Index of economic, social and cultural status (WLE)\tabularnewline
reading & Students' reading score in PISA 2015\tabularnewline
math & Students' math score in PISA 2015\tabularnewline
\bottomrule
\end{longtable}

We call this new dataset \texttt{pisa\_small}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa_small <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(pisa, CNT }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Canada"}\NormalTok{, }\StringTok{"United States"}\NormalTok{), }
                     \DataTypeTok{select =} \KeywordTok{c}\NormalTok{(science_perf, WEALTH, HEDRES, ENVAWARE, ICTRES, }
\NormalTok{                                EPIST, HOMEPOS, ESCS, reading, math))}
\end{Highlighting}
\end{Shaded}

Before we begin the analysis, we need to install and load all the required packages.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{decision_packages <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"caret"}\NormalTok{, }\StringTok{"rpart"}\NormalTok{, }\StringTok{"rpart.plot"}\NormalTok{, }\StringTok{"randomForest"}\NormalTok{, }\StringTok{"modelr"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(decision_packages)}

\KeywordTok{library}\NormalTok{(}\StringTok{"caret"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"rpart"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"rpart.plot"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"randomForest"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"modelr"}\NormalTok{)}

\CommentTok{# Already installed packages that we will use}
\KeywordTok{library}\NormalTok{(}\StringTok{"data.table"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"dplyr"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next, we will split our dataset into a training dataset and a test dataset. We will train the decision tree on the training data and check its accuracy using the test data. In order to replicate the results later on, we need to set the seed -- which will allow us to fix the randomization. Next, we remove the missing cases, save it as a new dataset, and then use \texttt{createDataPartition()} from the \texttt{caret} package to create an index to split the dataset as 70\% to 30\% using p = 0.7.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Set the seed before splitting the data}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{442019}\NormalTok{)}

\CommentTok{# We need to remove missing cases}
\NormalTok{pisa_nm <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(pisa_small)}

\CommentTok{# Split the data into training and test}
\NormalTok{index <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(pisa_nm}\OperatorTok{$}\NormalTok{science_perf, }\DataTypeTok{p =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{train_dat <-}\StringTok{ }\NormalTok{pisa_nm[index, ]}
\NormalTok{test_dat  <-}\StringTok{ }\NormalTok{pisa_nm[}\OperatorTok{-}\NormalTok{index, ]}

\KeywordTok{nrow}\NormalTok{(train_dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 16561
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{nrow}\NormalTok{(test_dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7097
\end{verbatim}

Alternatively, we could simply create the index using random number generation with \texttt{sample.int()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(pisa_nm)}
\NormalTok{index <-}\StringTok{ }\KeywordTok{sample.int}\NormalTok{(n, }\DataTypeTok{size =} \KeywordTok{round}\NormalTok{(}\FloatTok{0.7} \OperatorTok{*}\StringTok{ }\NormalTok{n))}
\end{Highlighting}
\end{Shaded}

To build a decision tree model, we will use the \texttt{rpart} function from the \texttt{rpart} package. In the function, there are several elements:

\begin{itemize}
\tightlist
\item
  \texttt{formula\ =\ science\_perf\ \textasciitilde{}\ .} defines the dependent variable (i.e., science\_perf) and the predictors (and \texttt{\textasciitilde{}} is the separator). Because we use \texttt{science\_perf\ \textasciitilde{}\ .}, we use all variables in the dataset (except for science\_perf) as our predictors. We could also write the same formula as \texttt{science\_perf\ \textasciitilde{}\ math\ +\ reading\ +\ ESCS\ +\ ...\ +\ WEALTH} by specifying each variable individually.
\item
  \texttt{data\ =\ train\_dat} defines the dataset we are using for the analysis.
\item
  \texttt{method\ =\ "class"} defines what type of decision tree we are building. \texttt{method\ =\ "class"} defines a classification tree and \texttt{method\ =\ "anova"} defines a regression tree.
\item
  \texttt{control} is a list of control (i.e., tuning) elements for the decision tree algorithm. \texttt{minsplit} defines the minimum number of observations that must exist in a node (default = 20); \texttt{cp} is the complexity parameter to prune the subtrees that don't improve the model fit (default = 0.01, if \texttt{cp} = 0, then no pruning); \texttt{xval} is the number of cross-validations (default = 10, if \texttt{xval} = 0, then no cross validation).
\item
  \texttt{parms} is a list of optional parameters for the splitting function.\texttt{anova} splitting (i.e., regression trees) has no parameters. For \texttt{class} splitting (i.e., classification tree), the most important option is the split index -- which is either \texttt{"gini"} for the Gini index or \texttt{"information"} for the Entropy index. Splitting based on \texttt{information} can be slightly slower compared to the Gini index (see the \href{https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf}{vignette} for more information).
\end{itemize}

We will start building our decision tree model \texttt{df\_fit1} (standing for decision tree fit for model 1) with no pruning (i.e., \texttt{cp\ =\ 0}) and no cross-validation as we have a test dataset already (i.e., \texttt{xval\ =\ 0}). We will use the Gini index for the splitting.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt_fit1 <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ science_perf }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                 \DataTypeTok{data =}\NormalTok{ train_dat,}
                 \DataTypeTok{method =} \StringTok{"class"}\NormalTok{, }
                 \DataTypeTok{control =} \KeywordTok{rpart.control}\NormalTok{(}\DataTypeTok{minsplit =} \DecValTok{20}\NormalTok{, }
                                         \DataTypeTok{cp =} \DecValTok{0}\NormalTok{, }
                                         \DataTypeTok{xval =} \DecValTok{0}\NormalTok{),}
                \DataTypeTok{parms =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{split =} \StringTok{"gini"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The estimated model is very likely to have too many nodes because we set \texttt{cp\ =\ 0}. Due to having many nodes, first we will examine the results graphically, before we attempt to print the output. Although the \texttt{rpart} package can draw decision tree plots, they are very basic. Therefore, we will use the \texttt{rpart.plot} function from the \texttt{rpart.plot} package to draw a nicer decision tree plot. Let's see the results graphically using the default settings of the \texttt{rpart.plot} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rpart.plot}\NormalTok{(dt_fit1)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=1\linewidth]{big-data-in-r_files/figure-latex/ch6-6-1}

How does the model look like? It is \textbf{NOT} very interpretable, isn't it? We definitely need to prune the trees; otherwise the model yields a very complex model with many nodes -- which is very likely to overfit the data. In the following model, we use \texttt{cp\ =\ 0.005}. Remember that as we increase \texttt{cp}, the pruning for the model will also increase. The higher the cp value, the shorter the trees with possibly fewer predictors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt_fit2 <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ science_perf }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                 \DataTypeTok{data =}\NormalTok{ train_dat,}
                 \DataTypeTok{method =} \StringTok{"class"}\NormalTok{, }
                 \DataTypeTok{control =} \KeywordTok{rpart.control}\NormalTok{(}\DataTypeTok{minsplit =} \DecValTok{20}\NormalTok{, }
                                         \DataTypeTok{cp =} \FloatTok{0.005}\NormalTok{, }
                                         \DataTypeTok{xval =} \DecValTok{0}\NormalTok{),}
                \DataTypeTok{parms =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{split =} \StringTok{"gini"}\NormalTok{))}

\KeywordTok{rpart.plot}\NormalTok{(dt_fit2)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=1\linewidth]{big-data-in-r_files/figure-latex/ch6-7a-1}

We could also estimate the same model with the Entropy as the split criterion, \texttt{split\ =\ "information"}, and the results would be similar (not necessarily the tree itself, but its classification performance).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt_fit2 <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ science_perf }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                 \DataTypeTok{data =}\NormalTok{ train_dat,}
                 \DataTypeTok{method =} \StringTok{"class"}\NormalTok{, }
                 \DataTypeTok{control =} \KeywordTok{rpart.control}\NormalTok{(}\DataTypeTok{minsplit =} \DecValTok{20}\NormalTok{, }
                                         \DataTypeTok{cp =} \FloatTok{0.005}\NormalTok{, }
                                         \DataTypeTok{xval =} \DecValTok{0}\NormalTok{),}
                \DataTypeTok{parms =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{split =} \StringTok{"information"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Now our model is less complex compared compared to the previous model. In the above decision tree plot, each node shows:

\begin{itemize}
\tightlist
\item
  the predicted class (High or low)
\item
  the predicted probability of the second class (i.e., ``Low'')
\item
  the percentage of observations in the node
\end{itemize}

Let's play with the colors to make the trees even more distinct. Also, we will adjust which values should be shown in the nodes, using \texttt{extra\ =\ 8} (see other possible options \href{http://www.milbo.org/doc/prp.pdf}{HERE}). Each node in the new plot shows:

\begin{itemize}
\tightlist
\item
  the predicted class (High or low)
\item
  the predicted probability of the fitted class
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rpart.plot}\NormalTok{(dt_fit2, }\DataTypeTok{extra =} \DecValTok{8}\NormalTok{, }\DataTypeTok{box.palette =} \StringTok{"RdBu"}\NormalTok{, }\DataTypeTok{shadow.col =} \StringTok{"gray"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=1\linewidth]{big-data-in-r_files/figure-latex/ch6-8-1}

An alternative way to prune the model is to use the \texttt{prune()} function from the \texttt{rpart} package. In the following example, we will use our initial complex model \texttt{dt\_fit1} and prune it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt_fit1_prune <-}\StringTok{ }\KeywordTok{prune}\NormalTok{(dt_fit1, }\DataTypeTok{cp =} \FloatTok{0.005}\NormalTok{)}
\KeywordTok{rpart.plot}\NormalTok{(dt_fit1_prune, }\DataTypeTok{extra =} \DecValTok{8}\NormalTok{, }\DataTypeTok{box.palette =} \StringTok{"RdBu"}\NormalTok{, }\DataTypeTok{shadow.col =} \StringTok{"gray"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

which would yield the same model that we estimated. Now let's print the output of our model using \texttt{printcp()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{printcp}\NormalTok{(dt_fit2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Classification tree:
## rpart(formula = science_perf ~ ., data = train_dat, method = "class", 
##     parms = list(split = "gini"), control = rpart.control(minsplit = 20, 
##         cp = 0.005, xval = 0))
## 
## Variables actually used in tree construction:
## [1] math    reading
## 
## Root node error: 6461/16561 = 0.39013
## 
## n= 16561 
## 
##          CP nsplit rel error
## 1 0.7461693      0   1.00000
## 2 0.0153227      1   0.25383
## 3 0.0147036      3   0.22319
## 4 0.0080483      4   0.20848
## 5 0.0050000      6   0.19239
\end{verbatim}

In the output, \texttt{CP} refers to the complexity parameter, \texttt{nsplit} is the number of splits in the decision tree based on the complexity parameter, and \texttt{rel\ error} is the relative error (i.e., \(1 - R^2\)) of the solution. This is the error for predictions of the data that were used to estimate the model. The section of \texttt{Variables\ actually\ used\ in\ tree\ construction} shows which variables have been used in the final model. In our example, only math and reading have been used. What happened to the other variables?

In addition to \texttt{printcp()}, we can use \texttt{summary()} to print out more detailed results with all splits.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(dt_fit2)}
\end{Highlighting}
\end{Shaded}

We don't print the entire summary output here. Instead, we want to focus on a specific section in the output:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Variable importance}
\NormalTok{    math  reading ENVAWARE     ESCS    EPIST  HOMEPOS }
      \DecValTok{46}       \DecValTok{37}        \DecValTok{5}        \DecValTok{4}        \DecValTok{4}        \DecValTok{4}  
\end{Highlighting}
\end{Shaded}

Similarly, \texttt{varImp()} from the \texttt{caret} package also gives us a similar output:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{varImp}\NormalTok{(dt_fit2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              Overall
## ENVAWARE  582.079065
## EPIST     791.498637
## ESCS      427.818610
## HEDRES      5.287639
## HOMEPOS    17.914110
## math     5529.901785
## reading  5752.549285
## WEALTH      7.572725
## ICTRES      0.000000
\end{verbatim}

Both of these show the importance of the variables for our estimated decision tree model. The larger the values are, the more crucial they are for the model. In our example, math and reading seem to be highly important for the decision tree model, whereas ICTRES is the least important variable. The variables that were not very important for the model are those that were not included in the final model. These variables are possibly have very low correlations with our outcome variable, \texttt{science\_perf}.

We can use \texttt{rpart.rules} to print out the decision rules from the trees. By default, the output from this function shows the probability of the \textbf{second} class for each decision/split being made (i.e., the category ``low'' in our example) and what percent of the observations fall into this category.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rpart.rules}\NormalTok{(dt_fit2, }\DataTypeTok{cover =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  science_perf                                                   cover
##          0.03 when math >=        478 & reading >=        499     53%
##          0.21 when math <  478        & reading >=        539      1%
##          0.34 when math >=        478 & reading is 468 to 499      6%
##          0.40 when math is 458 to 478 & reading is 507 to 539      2%
##          0.68 when math <  458        & reading is 507 to 539      2%
##          0.70 when math >=        478 & reading <  468             3%
##          0.95 when math <  478        & reading <  507            33%
\end{verbatim}

Furthermore, we need to check the classification accuracy of the estimated decision tree with the \textbf{test} data. Otherwise, it is hard to justify whether or not the estimated decision tree would work accurately for prediction. Below we estimate the predicted classes (either high or low) from the test data by applying the estimated model.First we obtain model predictions using \texttt{predict()} and then turn the results into a data frame called \texttt{dt\_pred}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt_pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(dt_fit2, test_dat) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{as.data.frame}\NormalTok{()}

\KeywordTok{head}\NormalTok{(dt_pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         High        Low
## 1 0.97465045 0.02534955
## 2 0.05406386 0.94593614
## 3 0.05406386 0.94593614
## 4 0.66243386 0.33756614
## 5 0.97465045 0.02534955
## 6 0.05406386 0.94593614
\end{verbatim}

This dataset shows each observation's (i.e., students from the test data) probability of falling into either \emph{high} or \emph{low} categories based on the decision rules that we estimated. We will turn these probabilities into binary classifications, depending on whether or not they are \textgreater{}= \(50\%\). Then, we will compare these estimates with the actual classes in the test data (i.e., \texttt{test\_dat\$science\_perf}) in order to create a confusion matrix.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt_pred <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(dt_pred,}
  \DataTypeTok{science_perf =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(High }\OperatorTok{>=}\StringTok{ }\FloatTok{0.5}\NormalTok{, }\StringTok{"High"}\NormalTok{, }\StringTok{"Low"}\NormalTok{))}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(science_perf)}
  
\KeywordTok{confusionMatrix}\NormalTok{(dt_pred}\OperatorTok{$}\NormalTok{science_perf, test_dat}\OperatorTok{$}\NormalTok{science_perf)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction High  Low
##       High 4076  316
##       Low   252 2453
##                                           
##                Accuracy : 0.92            
##                  95% CI : (0.9134, 0.9262)
##     No Information Rate : 0.6098          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.8311          
##  Mcnemar's Test P-Value : 0.008207        
##                                           
##             Sensitivity : 0.9418          
##             Specificity : 0.8859          
##          Pos Pred Value : 0.9281          
##          Neg Pred Value : 0.9068          
##              Prevalence : 0.6098          
##          Detection Rate : 0.5743          
##    Detection Prevalence : 0.6189          
##       Balanced Accuracy : 0.9138          
##                                           
##        'Positive' Class : High            
## 
\end{verbatim}

The output shows that the overall accuracy is around \(92\%\), sensitivit is \(94\%\), and specificity is \(89\%\). For only two variables, this is very good. However, sometimes we do not have predictors that are highly correlated with the outcome variables. In such cases, the model tuning might take much longer.

Let's assume that we did \textbf{NOT} have reading and math in our dataset. We still want to predict \texttt{science\_perf} using the remaining variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt_fit3a <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ science_perf }\OperatorTok{~}\StringTok{ }\NormalTok{WEALTH }\OperatorTok{+}\StringTok{ }\NormalTok{HEDRES }\OperatorTok{+}\StringTok{ }\NormalTok{ENVAWARE }\OperatorTok{+}\StringTok{ }\NormalTok{ICTRES }\OperatorTok{+}\StringTok{ }\NormalTok{EPIST }\OperatorTok{+}\StringTok{ }
\StringTok{                   }\NormalTok{HOMEPOS }\OperatorTok{+}\NormalTok{ESCS,}
                 \DataTypeTok{data =}\NormalTok{ train_dat,}
                 \DataTypeTok{method =} \StringTok{"class"}\NormalTok{, }
                 \DataTypeTok{control =} \KeywordTok{rpart.control}\NormalTok{(}\DataTypeTok{minsplit =} \DecValTok{20}\NormalTok{, }
                                         \DataTypeTok{cp =} \FloatTok{0.001}\NormalTok{, }
                                         \DataTypeTok{xval =} \DecValTok{0}\NormalTok{),}
                \DataTypeTok{parms =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{split =} \StringTok{"gini"}\NormalTok{))}

\KeywordTok{rpart.plot}\NormalTok{(dt_fit3a, }\DataTypeTok{extra =} \DecValTok{8}\NormalTok{, }\DataTypeTok{box.palette =} \StringTok{"RdBu"}\NormalTok{, }\DataTypeTok{shadow.col =} \StringTok{"gray"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=1\linewidth]{big-data-in-r_files/figure-latex/ch6-12a-1}

Now, let's change cp to 0.005.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt_fit3b <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ science_perf }\OperatorTok{~}\StringTok{ }\NormalTok{WEALTH }\OperatorTok{+}\StringTok{ }\NormalTok{HEDRES }\OperatorTok{+}\StringTok{ }\NormalTok{ENVAWARE }\OperatorTok{+}\StringTok{ }\NormalTok{ICTRES }\OperatorTok{+}\StringTok{ }\NormalTok{EPIST }\OperatorTok{+}\StringTok{ }
\StringTok{                   }\NormalTok{HOMEPOS }\OperatorTok{+}\StringTok{ }\NormalTok{ESCS,}
                 \DataTypeTok{data =}\NormalTok{ train_dat,}
                 \DataTypeTok{method =} \StringTok{"class"}\NormalTok{, }
                 \DataTypeTok{control =} \KeywordTok{rpart.control}\NormalTok{(}\DataTypeTok{minsplit =} \DecValTok{20}\NormalTok{, }
                                         \DataTypeTok{cp =} \FloatTok{0.005}\NormalTok{, }
                                         \DataTypeTok{xval =} \DecValTok{0}\NormalTok{),}
                \DataTypeTok{parms =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{split =} \StringTok{"gini"}\NormalTok{))}

\KeywordTok{rpart.plot}\NormalTok{(dt_fit3b, }\DataTypeTok{extra =} \DecValTok{8}\NormalTok{, }\DataTypeTok{box.palette =} \StringTok{"RdBu"}\NormalTok{, }\DataTypeTok{shadow.col =} \StringTok{"gray"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=1\linewidth]{big-data-in-r_files/figure-latex/ch6-12b-1}

Since we also care about the accuracy, sensitivity, and specificity of these models, we can turn this experiment into a small function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{decision_check <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(cp) \{}
  \KeywordTok{require}\NormalTok{(}\StringTok{"rpart"}\NormalTok{)}
  \KeywordTok{require}\NormalTok{(}\StringTok{"dplyr"}\NormalTok{)}
  
\NormalTok{  dt <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ science_perf }\OperatorTok{~}\StringTok{ }\NormalTok{WEALTH }\OperatorTok{+}\StringTok{ }\NormalTok{HEDRES }\OperatorTok{+}\StringTok{ }\NormalTok{ENVAWARE }\OperatorTok{+}\StringTok{ }\NormalTok{ICTRES }\OperatorTok{+}\StringTok{ }\NormalTok{EPIST }\OperatorTok{+}\StringTok{ }
\StringTok{                   }\NormalTok{HOMEPOS }\OperatorTok{+}\StringTok{ }\NormalTok{ESCS,}
              \DataTypeTok{data =}\NormalTok{ train_dat,}
              \DataTypeTok{method =} \StringTok{"class"}\NormalTok{, }
              \DataTypeTok{control =} \KeywordTok{rpart.control}\NormalTok{(}\DataTypeTok{minsplit =} \DecValTok{20}\NormalTok{, }
                                           \DataTypeTok{cp =}\NormalTok{ cp, }
                                           \DataTypeTok{xval =} \DecValTok{0}\NormalTok{),}
              \DataTypeTok{parms =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{split =} \StringTok{"gini"}\NormalTok{))}
  
\NormalTok{  dt_pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(dt, test_dat) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{as.data.frame}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{science_perf =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(High }\OperatorTok{>=}\StringTok{ }\FloatTok{0.5}\NormalTok{, }\StringTok{"High"}\NormalTok{, }\StringTok{"Low"}\NormalTok{))) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{select}\NormalTok{(science_perf)}
  
\NormalTok{  cm <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(dt_pred}\OperatorTok{$}\NormalTok{science_perf, test_dat}\OperatorTok{$}\NormalTok{science_perf)}
  
\NormalTok{  results <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{cp =}\NormalTok{ cp, }
                        \DataTypeTok{Accuracy =} \KeywordTok{round}\NormalTok{(cm}\OperatorTok{$}\NormalTok{overall[}\DecValTok{1}\NormalTok{], }\DecValTok{3}\NormalTok{),}
                        \DataTypeTok{Sensitivity =} \KeywordTok{round}\NormalTok{(cm}\OperatorTok{$}\NormalTok{byClass[}\DecValTok{1}\NormalTok{], }\DecValTok{3}\NormalTok{),}
                        \DataTypeTok{Specificity =} \KeywordTok{round}\NormalTok{(cm}\OperatorTok{$}\NormalTok{byClass[}\DecValTok{2}\NormalTok{], }\DecValTok{3}\NormalTok{))}
  
  \KeywordTok{return}\NormalTok{(results)}
\NormalTok{\}}

\NormalTok{result <-}\StringTok{ }\OtherTok{NULL}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \KeywordTok{seq}\NormalTok{(}\DataTypeTok{from=}\FloatTok{0.001}\NormalTok{, }\DataTypeTok{to=}\FloatTok{0.08}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.005}\NormalTok{)) \{}
\NormalTok{  result <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(result, }\KeywordTok{decision_check}\NormalTok{(}\DataTypeTok{cp =}\NormalTok{ i))}
\NormalTok{\}}

\NormalTok{result <-}\StringTok{ }\NormalTok{result[}\KeywordTok{order}\NormalTok{(result}\OperatorTok{$}\NormalTok{Accuracy, result}\OperatorTok{$}\NormalTok{Sensitivity, result}\OperatorTok{$}\NormalTok{Specificity),]}
\NormalTok{result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               cp Accuracy Sensitivity Specificity
## Accuracy9  0.046    0.675       0.947       0.250
## Accuracy10 0.051    0.675       0.947       0.250
## Accuracy11 0.056    0.675       0.947       0.250
## Accuracy12 0.061    0.675       0.947       0.250
## Accuracy13 0.066    0.675       0.947       0.250
## Accuracy14 0.071    0.675       0.947       0.250
## Accuracy15 0.076    0.675       0.947       0.250
## Accuracy3  0.016    0.686       0.757       0.574
## Accuracy4  0.021    0.686       0.757       0.574
## Accuracy5  0.026    0.686       0.757       0.574
## Accuracy6  0.031    0.686       0.757       0.574
## Accuracy7  0.036    0.686       0.757       0.574
## Accuracy8  0.041    0.686       0.757       0.574
## Accuracy1  0.006    0.694       0.850       0.449
## Accuracy2  0.011    0.694       0.850       0.449
## Accuracy   0.001    0.705       0.835       0.502
\end{verbatim}

We can also visulize the results using \texttt{ggplot2}. First, we wil transform the \texttt{result} dataset into a long format and then use this new dataset (called \texttt{result\_long}) in \texttt{ggplot()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result_long <-}\StringTok{ }\KeywordTok{melt}\NormalTok{(}\KeywordTok{as.data.table}\NormalTok{(result),}
                    \DataTypeTok{id.vars =} \KeywordTok{c}\NormalTok{(}\StringTok{"cp"}\NormalTok{),}
                    \DataTypeTok{measure =} \KeywordTok{c}\NormalTok{(}\StringTok{"Accuracy"}\NormalTok{, }\StringTok{"Sensitivity"}\NormalTok{, }\StringTok{"Specificity"}\NormalTok{),}
                    \DataTypeTok{variable.name =} \StringTok{"Index"}\NormalTok{,}
                    \DataTypeTok{value.name =} \StringTok{"Value"}\NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ result_long,}
       \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ cp, }\DataTypeTok{y =}\NormalTok{ Value)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ Index), }\DataTypeTok{size =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Complexity Parameter"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Value"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=1\linewidth]{big-data-in-r_files/figure-latex/ch6-12d-1}

In the plot, we see that there is a trade-off between sensitivity and specificity. Depending on the situation, we may prefer higher sensitivity (e.g., correctly identifying those who have ``high'' science scores) or higher specificity (e.g., correctly identifying those who have ``low'' science scores). For example, if we want to know who is performing poorly in science (so that we can design additional instructional materials), we may want the model to identify ``low'' performers more accurately.

\hypertarget{cross-validation}{%
\subsection{Cross-validation}\label{cross-validation}}

As you may remember, we set \texttt{xval\ =\ 0} in our decision tree models because we did not want to run any cross-validation samples. However, cross-validations (e.g., \emph{K}-fold approach) are highly useful when we do not have a test or validation dataset, or our dataset is to small to split into training and test data. A typical way to use cross-validation in decision trees is to not specify a cp (i.e., complexity parameter) and perform cross validation. In the following example, we will assume that our dataset is not too big and thus we want to run 10 cross-validation samples (i.e., splits) as we build our decision tree model. Note that we use \texttt{cp\ =\ 0} this time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt_fit4 <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ science_perf }\OperatorTok{~}\StringTok{ }\NormalTok{WEALTH }\OperatorTok{+}\StringTok{ }\NormalTok{HEDRES }\OperatorTok{+}\StringTok{ }\NormalTok{ENVAWARE }\OperatorTok{+}\StringTok{ }\NormalTok{ICTRES }\OperatorTok{+}\StringTok{ }
\StringTok{                   }\NormalTok{EPIST }\OperatorTok{+}\StringTok{ }\NormalTok{HOMEPOS }\OperatorTok{+}\StringTok{ }\NormalTok{ESCS,}
                 \DataTypeTok{data =}\NormalTok{ train_dat,}
                 \DataTypeTok{method =} \StringTok{"class"}\NormalTok{, }
                 \DataTypeTok{control =} \KeywordTok{rpart.control}\NormalTok{(}\DataTypeTok{minsplit =} \DecValTok{20}\NormalTok{,}
                                         \DataTypeTok{cp =} \DecValTok{0}\NormalTok{,}
                                         \DataTypeTok{xval =} \DecValTok{10}\NormalTok{),}
                \DataTypeTok{parms =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{split =} \StringTok{"gini"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

In the results, we can evaluate the cross-validated error (i.e., X-val Relative Error) and choose the complexity parameter that would give us an acceptable value. Then, we can use this cp value and prune the trees. We use \texttt{plotcp()} function to visualize the cross-validation results.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{printcp}\NormalTok{(dt_fit4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Classification tree:
## rpart(formula = science_perf ~ WEALTH + HEDRES + ENVAWARE + ICTRES + 
##     EPIST + HOMEPOS + ESCS, data = train_dat, method = "class", 
##     parms = list(split = "gini"), control = rpart.control(minsplit = 20, 
##         cp = 0, xval = 10))
## 
## Variables actually used in tree construction:
## [1] ENVAWARE EPIST    ESCS     HEDRES   HOMEPOS  ICTRES   WEALTH  
## 
## Root node error: 6461/16561 = 0.39013
## 
## n= 16561 
## 
##            CP nsplit rel error  xerror      xstd
## 1  7.7233e-02      0   1.00000 1.00000 0.0097156
## 2  4.3646e-02      2   0.84553 0.84677 0.0093682
## 3  1.1918e-02      3   0.80189 0.80406 0.0092417
## 4  5.6493e-03      5   0.77805 0.78703 0.0091875
## 5  2.6312e-03      7   0.76675 0.78270 0.0091733
## 6  2.3216e-03      9   0.76149 0.78378 0.0091769
## 7  1.9347e-03     11   0.75685 0.78270 0.0091733
## 8  1.8573e-03     13   0.75298 0.78115 0.0091683
## 9  1.7025e-03     16   0.74741 0.78130 0.0091688
## 10 1.5477e-03     18   0.74400 0.78068 0.0091667
## 11 1.5220e-03     24   0.73425 0.77743 0.0091560
## 12 1.3930e-03     30   0.72512 0.77852 0.0091596
## 13 1.2382e-03     34   0.71955 0.77356 0.0091430
## 14 1.0834e-03     35   0.71831 0.77465 0.0091467
## 15 1.0525e-03     47   0.70392 0.77387 0.0091441
## 16 8.5126e-04     52   0.69865 0.77279 0.0091404
## 17 8.2547e-04     54   0.69695 0.77790 0.0091575
## 18 7.7387e-04     57   0.69447 0.77697 0.0091544
## 19 7.2228e-04     69   0.68488 0.77914 0.0091616
## 20 6.7069e-04     72   0.68271 0.77697 0.0091544
## 21 6.1910e-04     84   0.67466 0.78099 0.0091677
## 22 5.8041e-04    101   0.66398 0.78610 0.0091845
## 23 5.6751e-04    106   0.66104 0.78718 0.0091880
## 24 5.4171e-04    123   0.65036 0.79121 0.0092010
## 25 5.1592e-04    148   0.63612 0.79121 0.0092010
## 26 4.9012e-04    153   0.63287 0.79910 0.0092262
## 27 4.6432e-04    159   0.62993 0.79957 0.0092277
## 28 4.3337e-04    198   0.61059 0.80313 0.0092388
## 29 4.1273e-04    211   0.60331 0.80607 0.0092480
## 30 4.0241e-04    231   0.59403 0.80916 0.0092576
## 31 3.8694e-04    254   0.58180 0.81009 0.0092604
## 32 3.6114e-04    275   0.57282 0.81226 0.0092671
## 33 3.3166e-04    298   0.56369 0.81272 0.0092685
## 34 3.0955e-04    310   0.55905 0.82216 0.0092970
## 35 2.7086e-04    360   0.54341 0.82479 0.0093048
## 36 2.5796e-04    380   0.53707 0.83083 0.0093226
## 37 2.4764e-04    399   0.53134 0.83965 0.0093481
## 38 2.3216e-04    411   0.52825 0.84197 0.0093547
## 39 2.2111e-04    456   0.51602 0.84492 0.0093630
## 40 2.1668e-04    467   0.51323 0.84492 0.0093630
## 41 2.0637e-04    495   0.50317 0.84909 0.0093747
## 42 1.9347e-04    507   0.50070 0.85188 0.0093824
## 43 1.8573e-04    521   0.49760 0.85234 0.0093837
## 44 1.7197e-04    529   0.49574 0.85234 0.0093837
## 45 1.5477e-04    538   0.49420 0.87339 0.0094403
## 46 1.2898e-04    632   0.47810 0.87463 0.0094435
## 47 1.1608e-04    638   0.47733 0.87773 0.0094515
## 48 1.0318e-04    646   0.47640 0.88222 0.0094630
## 49 9.2865e-05    667   0.47423 0.88345 0.0094662
## 50 7.7387e-05    672   0.47377 0.89305 0.0094902
## 51 6.8789e-05    716   0.47036 0.89305 0.0094902
## 52 5.8041e-05    725   0.46974 0.89661 0.0094990
## 53 5.1592e-05    740   0.46881 0.90094 0.0095095
## 54 3.8694e-05    770   0.46727 0.90187 0.0095117
## 55 2.2111e-05    782   0.46680 0.90497 0.0095192
## 56 0.0000e+00    796   0.46618 0.90652 0.0095229
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plotcp}\NormalTok{(dt_fit4)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=1\linewidth]{big-data-in-r_files/figure-latex/ch6-14-1}

Next, we can modify our model as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt_fit5 <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ science_perf }\OperatorTok{~}\StringTok{ }\NormalTok{WEALTH }\OperatorTok{+}\StringTok{ }\NormalTok{HEDRES }\OperatorTok{+}\StringTok{ }\NormalTok{ENVAWARE }\OperatorTok{+}\StringTok{ }\NormalTok{ICTRES }\OperatorTok{+}\StringTok{ }
\StringTok{                   }\NormalTok{EPIST }\OperatorTok{+}\StringTok{ }\NormalTok{HOMEPOS }\OperatorTok{+}\StringTok{ }\NormalTok{ESCS,}
                 \DataTypeTok{data =}\NormalTok{ train_dat,}
                 \DataTypeTok{method =} \StringTok{"class"}\NormalTok{, }
                 \DataTypeTok{control =} \KeywordTok{rpart.control}\NormalTok{(}\DataTypeTok{minsplit =} \DecValTok{20}\NormalTok{, }
                                         \DataTypeTok{cp =} \FloatTok{0.0039}\NormalTok{,}
                                         \DataTypeTok{xval =} \DecValTok{0}\NormalTok{),}
                \DataTypeTok{parms =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{split =} \StringTok{"gini"}\NormalTok{))}

\KeywordTok{printcp}\NormalTok{(dt_fit5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Classification tree:
## rpart(formula = science_perf ~ WEALTH + HEDRES + ENVAWARE + ICTRES + 
##     EPIST + HOMEPOS + ESCS, data = train_dat, method = "class", 
##     parms = list(split = "gini"), control = rpart.control(minsplit = 20, 
##         cp = 0.0039, xval = 0))
## 
## Variables actually used in tree construction:
## [1] ENVAWARE EPIST    ESCS     HOMEPOS 
## 
## Root node error: 6461/16561 = 0.39013
## 
## n= 16561 
## 
##          CP nsplit rel error
## 1 0.0772326      0   1.00000
## 2 0.0436465      2   0.84553
## 3 0.0119177      3   0.80189
## 4 0.0056493      5   0.77805
## 5 0.0039000      7   0.76675
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rpart.plot}\NormalTok{(dt_fit5, }\DataTypeTok{extra =} \DecValTok{8}\NormalTok{, }\DataTypeTok{box.palette =} \StringTok{"RdBu"}\NormalTok{, }\DataTypeTok{shadow.col =} \StringTok{"gray"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=1\linewidth]{big-data-in-r_files/figure-latex/ch6-15-1}

Lastly, for the sake of brevity, we demonstrate a short regression tree example below where we predict math scores (a continuous variable) using the same set of variables. This time we use \texttt{method\ =\ "anova"} in the \texttt{rpart()} function to estimate a regression tree.

Let's begin with cross-validation and check how \(R^2\) changes depending on the number of splits.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rt_fit1 <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ math }\OperatorTok{~}\StringTok{ }\NormalTok{WEALTH }\OperatorTok{+}\StringTok{ }\NormalTok{HEDRES }\OperatorTok{+}\StringTok{ }\NormalTok{ENVAWARE }\OperatorTok{+}\StringTok{ }
\StringTok{                  }\NormalTok{ICTRES }\OperatorTok{+}\StringTok{ }\NormalTok{EPIST }\OperatorTok{+}\StringTok{ }\NormalTok{HOMEPOS }\OperatorTok{+}\StringTok{ }\NormalTok{ESCS,}
                 \DataTypeTok{data =}\NormalTok{ train_dat,}
                 \DataTypeTok{method =} \StringTok{"anova"}\NormalTok{, }
                 \DataTypeTok{control =} \KeywordTok{rpart.control}\NormalTok{(}\DataTypeTok{minsplit =} \DecValTok{20}\NormalTok{,}
                                         \DataTypeTok{cp =} \FloatTok{0.001}\NormalTok{,}
                                         \DataTypeTok{xval =} \DecValTok{10}\NormalTok{),}
                \DataTypeTok{parms =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{split =} \StringTok{"gini"}\NormalTok{))}

\KeywordTok{printcp}\NormalTok{(rt_fit1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Regression tree:
## rpart(formula = math ~ WEALTH + HEDRES + ENVAWARE + ICTRES + 
##     EPIST + HOMEPOS + ESCS, data = train_dat, method = "anova", 
##     parms = list(split = "gini"), control = rpart.control(minsplit = 20, 
##         cp = 0.001, xval = 10))
## 
## Variables actually used in tree construction:
## [1] ENVAWARE EPIST    ESCS     HEDRES   WEALTH  
## 
## Root node error: 104625310/16561 = 6317.6
## 
## n= 16561 
## 
##           CP nsplit rel error  xerror      xstd
## 1  0.1121549      0   1.00000 1.00014 0.0100818
## 2  0.0382061      1   0.88785 0.88819 0.0091919
## 3  0.0353050      2   0.84964 0.85964 0.0089652
## 4  0.0166934      3   0.81433 0.81758 0.0085677
## 5  0.0078301      4   0.79764 0.80105 0.0084293
## 6  0.0070306      5   0.78981 0.79455 0.0083594
## 7  0.0063780      6   0.78278 0.78877 0.0083343
## 8  0.0040553      7   0.77640 0.78437 0.0083043
## 9  0.0033408      8   0.77235 0.78091 0.0082947
## 10 0.0030034      9   0.76901 0.77942 0.0082741
## 11 0.0028744     10   0.76600 0.77759 0.0082582
## 12 0.0024670     11   0.76313 0.77469 0.0082327
## 13 0.0021466     12   0.76066 0.77141 0.0082005
## 14 0.0021460     13   0.75851 0.77185 0.0082082
## 15 0.0019919     14   0.75637 0.77166 0.0082192
## 16 0.0018103     15   0.75438 0.76997 0.0082037
## 17 0.0017435     17   0.75076 0.76856 0.0081993
## 18 0.0017275     18   0.74901 0.76836 0.0081954
## 19 0.0016846     19   0.74728 0.76787 0.0081916
## 20 0.0016463     20   0.74560 0.76770 0.0081872
## 21 0.0015453     21   0.74395 0.76759 0.0081813
## 22 0.0013210     22   0.74241 0.76538 0.0081718
## 23 0.0012386     23   0.74109 0.76290 0.0081676
## 24 0.0012244     25   0.73861 0.76170 0.0081598
## 25 0.0011261     27   0.73616 0.75927 0.0081399
## 26 0.0011075     28   0.73504 0.75895 0.0081377
## 27 0.0011003     29   0.73393 0.75948 0.0081432
## 28 0.0010564     30   0.73283 0.75924 0.0081413
## 29 0.0010195     31   0.73177 0.75704 0.0081125
## 30 0.0010000     32   0.73075 0.75675 0.0081169
\end{verbatim}

Then, we can adjust our model based on the suggestions from the previous plot. Note that we use \texttt{extra\ =\ 100} in the \texttt{rpart.plot()} function to show percentages (\emph{Note}: \texttt{rpart.plot} has different \emph{extra} options depending on whether it is a classification or regression tree).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rt_fit2 <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ math }\OperatorTok{~}\StringTok{ }\NormalTok{WEALTH }\OperatorTok{+}\StringTok{ }\NormalTok{HEDRES }\OperatorTok{+}\StringTok{ }\NormalTok{ENVAWARE }\OperatorTok{+}\StringTok{ }
\StringTok{                  }\NormalTok{ICTRES }\OperatorTok{+}\StringTok{ }\NormalTok{EPIST }\OperatorTok{+}\StringTok{ }\NormalTok{HOMEPOS }\OperatorTok{+}\StringTok{ }\NormalTok{ESCS,}
                 \DataTypeTok{data =}\NormalTok{ train_dat,}
                 \DataTypeTok{method =} \StringTok{"anova"}\NormalTok{, }
                 \DataTypeTok{control =} \KeywordTok{rpart.control}\NormalTok{(}\DataTypeTok{minsplit =} \DecValTok{20}\NormalTok{, }
                                         \DataTypeTok{cp =} \FloatTok{0.007}\NormalTok{,}
                                         \DataTypeTok{xval =} \DecValTok{0}\NormalTok{),}
                \DataTypeTok{parms =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{split =} \StringTok{"gini"}\NormalTok{))}

\KeywordTok{printcp}\NormalTok{(rt_fit2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Regression tree:
## rpart(formula = math ~ WEALTH + HEDRES + ENVAWARE + ICTRES + 
##     EPIST + HOMEPOS + ESCS, data = train_dat, method = "anova", 
##     parms = list(split = "gini"), control = rpart.control(minsplit = 20, 
##         cp = 0.007, xval = 0))
## 
## Variables actually used in tree construction:
## [1] ENVAWARE EPIST    ESCS    
## 
## Root node error: 104625310/16561 = 6317.6
## 
## n= 16561 
## 
##          CP nsplit rel error
## 1 0.1121549      0   1.00000
## 2 0.0382061      1   0.88785
## 3 0.0353050      2   0.84964
## 4 0.0166934      3   0.81433
## 5 0.0078301      4   0.79764
## 6 0.0070306      5   0.78981
## 7 0.0070000      6   0.78278
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rpart.plot}\NormalTok{(rt_fit2, }\DataTypeTok{extra =} \DecValTok{100}\NormalTok{, }\DataTypeTok{box.palette =} \StringTok{"RdBu"}\NormalTok{, }\DataTypeTok{shadow.col =} \StringTok{"gray"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=1\linewidth]{big-data-in-r_files/figure-latex/ch6-17a-1}

To evaluate the model accuracy, we cannot use the classification-based indices anymore because we built a regression tree, not a classification tree. Two useful measures that we can for evaluating regression trees are the mean absolute error (mae) and the root mean square error (rmse). The \texttt{modelr} package has several functions -- such as \texttt{mae()} and \texttt{rmse()} -- to evaluate regression-based models. Using the training and (more importantly) test data, we can evaluate the accuracy of the decision tree model that we estimated above.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Training data}
\KeywordTok{mae}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ rt_fit2, }\DataTypeTok{data =}\NormalTok{ train_dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 56.48637
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rmse}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ rt_fit2, }\DataTypeTok{data =}\NormalTok{ train_dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 70.3226
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Test data}
\KeywordTok{mae}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ rt_fit2, }\DataTypeTok{data =}\NormalTok{ test_dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 56.65823
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rmse}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ rt_fit2, }\DataTypeTok{data =}\NormalTok{ test_dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 70.41532
\end{verbatim}

We seem to have slightly less error with the training data than the test data. Is this finding suprising to you?

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{random-forests}{%
\section{Random Forests}\label{random-forests}}

Decision trees can sometimes be non-robust because a small change in the data may cause a significant change in the final estimated tree. Therefore, whenever a decision tree approach is not completely stable, an alternative method -- such as \textbf{random forests} -- can be more suitable for supervised ML applications. Unlike the decision tree approach where there is a single solution from the same sample, random forest builds multiple decision trees by splitting the data into multiple sub-samples and merges them together to get a more accurate and stable prediction.

The underlying mechanism of random forests is very similar to that of decision trees. However, random forests first build lots of bushy trees and then average them to reduce the overall variance. Figure \ref{fig:fig6-2} shows how a random forest would look like with three trees.

\begin{figure}
\includegraphics[width=1\linewidth]{images/randomforest} \caption{An example of random forests approach}\label{fig:fig6-2}
\end{figure}

Random forest adds additional randomness to the model, while growing the trees. Instead of searching for the most important feature (i.e., predictor) while splitting a node, it searches for the best feature among a random subset of features. That is, only a random subset of the features is taken into consideration by the algorithm for splitting a node. This results in a wide diversity that generally results in a better model. For example, if there is a strong predictor among a set of predictors, a decision tree would typically rely on this particular predictor to make predictions and build trees. However, random forests force each split to consider only a set of the predictors -- which would result in trees that utilize not only the strong predictor but also other predictors that are moderately correlated with the outcome variable.

Random forest has nearly the same tuning parameters as a decision tree. Also, like decision trees, random forests can be used for both classification and regression problems. However, there are some differences between the two approaches. Unlike in decision trees, it is easier to control and prevent overfitting in random forests. This is because random forests create random subsets of the features and build much smaller trees using these subsets. Afterwards, it combines the subtrees. It should be noted that this procedure makes random forests computationally slower, depending on how many trees random forest builds. Therefore, it may not be effective for \emph{real-time} predictions.

The random forest algorithm is used in a lot of different fields, like banking, stock market, medicine, and e-commerce. For example, random forests can be used to detect customers who will use the bank's services more frequently than others and repay their debt in time. It can also used to detect fraud customers who want to scam the bank. In educational testing, we can use random forests to analyze a student's assessment history (e.g., test scores, response times, demographic variables, grade level, and so on) to identify whether the student has any learning difficulties. Similarly, we can use examinee-related variables, test scores, and test administration date to identify whether an examinee is likely to re-take the test (e.g., TOEFL or GRE) in the future.

\hypertarget{random-forests-in-r}{%
\section{Random forests in R}\label{random-forests-in-r}}

In R, \texttt{randomForest} and \texttt{caret} packages can be used to apply the random forest algorithm to classification and regression problems. The use of the \texttt{randomForest()} function is similar to that of \texttt{rpart()}. The main elements that we need to define are:

\begin{itemize}
\tightlist
\item
  \textbf{formula}: A regression-like formula defining the dependent variable and the predictors -- it is the same as the one for \texttt{rpart()}.
\item
  \textbf{data}: The dataset that we use to train the model.
\item
  \textbf{importance}: If TRUE, then importance of the predictors is assessed in the model.
\item
  \textbf{ntree}: Number of trees to grow in the model; we often start with a large number and then reduce it as we adjust the model based on the results. A large number for \textbf{ntree} can significantly increase the estimation time for the model.
\end{itemize}

There are also other elements that we can change depending on whether it is a classification or regression model (see \texttt{?randomForest} for more details). In the following example, we will focus on the same classification problem that we used before for decision trees. We initially set \texttt{ntree\ =\ 1000} to get 1000 trees in total but we will evaluate whether we need all of these trees to have an accurate model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"randomForest"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"caret"}\NormalTok{)}

\NormalTok{rf_fit1 <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ science_perf }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                        \DataTypeTok{data =}\NormalTok{ train_dat,}
                        \DataTypeTok{importance =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{ntree =} \DecValTok{1000}\NormalTok{)}

\KeywordTok{print}\NormalTok{(rf_fit1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
##  randomForest(formula = science_perf ~ ., data = train_dat, importance = TRUE,      ntree = 1000) 
##                Type of random forest: classification
##                      Number of trees: 1000
## No. of variables tried at each split: 3
## 
##         OOB estimate of  error rate: 7.58%
## Confusion matrix:
##      High  Low class.error
## High 9464  636   0.0629703
## Low   619 5842   0.0958056
\end{verbatim}

In the output, we see the confusion matrix along with classification error and out-of-bag (OOB) error. OBB is a method of measuring the prediction error of random forests, finding the mean prediction error on each training sample, using only the trees that did not have in their bootstrap sample. The results show that the overall OBB error is around \(7.6\%\), while the classification error is \(6\%\) for the \emph{high} category and around \(10\%\) for the \emph{low} category.

Next, by checking the level error across the number of trees, we can determine the ideal number of trees for our model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(rf_fit1)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=1\linewidth]{big-data-in-r_files/figure-latex/ch6-19-1}

The plot shows that the error level does not go down any further after roughly 50 trees. So, we can run our model again by using \texttt{ntree\ =\ 50} this time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf_fit2 <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ science_perf }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                        \DataTypeTok{data =}\NormalTok{ train_dat,}
                        \DataTypeTok{importance =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{ntree =} \DecValTok{50}\NormalTok{)}

\KeywordTok{print}\NormalTok{(rf_fit2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
##  randomForest(formula = science_perf ~ ., data = train_dat, importance = TRUE,      ntree = 50) 
##                Type of random forest: classification
##                      Number of trees: 50
## No. of variables tried at each split: 3
## 
##         OOB estimate of  error rate: 7.95%
## Confusion matrix:
##      High  Low class.error
## High 9459  641  0.06346535
## Low   675 5786  0.10447299
\end{verbatim}

We can see the overall accuracy of model (\(92.12\%\)) as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(rf_fit2}\OperatorTok{$}\NormalTok{confusion)) }\OperatorTok{/}\StringTok{ }\KeywordTok{nrow}\NormalTok{(train_dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9205362
\end{verbatim}

As we did for the decision trees, we can check the importance of the predictors in the model, using \texttt{importance()} and \texttt{varImpPlot()}. With \texttt{importance()}, we will first import the importance measures, turn it into a data.frame, save the row names as predictor names, and finally sort the data by MeanDecreaseGini (or, you can also see the basic output using only \texttt{importance(rf\_fit2)})

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{importance}\NormalTok{(rf_fit2) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{as.data.frame}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Predictors =} \KeywordTok{row.names}\NormalTok{(.)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(MeanDecreaseGini))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        High       Low MeanDecreaseAccuracy MeanDecreaseGini Predictors
## 1 28.659419 33.636045            39.132181        3483.8309       math
## 2 36.009283 34.864208            47.182695        2747.9847    reading
## 3  1.737624  1.235376             1.906881         362.2179      EPIST
## 4  4.234494  6.218419             7.870379         292.6858   ENVAWARE
## 5  5.395840  3.214590             6.759172         281.1615       ESCS
## 6  6.820185  6.218776            11.009217         218.7035    HOMEPOS
## 7  6.795979  9.105067            10.887967         197.7090     WEALTH
## 8  5.246056  3.811507             6.574882         161.3102     ICTRES
## 9  7.454470  1.509708             5.713732         133.4999     HEDRES
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{varImpPlot}\NormalTok{(rf_fit2, }
           \DataTypeTok{main =} \StringTok{"Importance of Variables for Science Performance"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=1\linewidth]{big-data-in-r_files/figure-latex/ch6-21-1}

The output shows different importance measures for the predictors that we used in the model. \texttt{MeanDecreaseAccuracy} and \texttt{MeanDecreaseGini} represent the overall classification error rate (or, mean squared error for regression) and the total decrease in node impurities from splitting on the variable, averaged over all trees. In the output, math and reading are the two predictors that seem to influence the model performance substantially, whereas EPIST and HEDRES are the least important variables. \texttt{varImpPlot()} presents the same information visually.

Next, we check the confusion matrix to see the accuracy, sensitivity, and specificity of our model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf_pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(rf_fit2, test_dat) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{as.data.frame}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{science_perf =} \KeywordTok{as.factor}\NormalTok{(}\StringTok{`}\DataTypeTok{.}\StringTok{`}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(science_perf)}
  
\KeywordTok{confusionMatrix}\NormalTok{(rf_pred}\OperatorTok{$}\NormalTok{science_perf, test_dat}\OperatorTok{$}\NormalTok{science_perf)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction High  Low
##       High 4058  274
##       Low   270 2495
##                                           
##                Accuracy : 0.9233          
##                  95% CI : (0.9169, 0.9294)
##     No Information Rate : 0.6098          
##     P-Value [Acc > NIR] : <2e-16          
##                                           
##                   Kappa : 0.8389          
##  Mcnemar's Test P-Value : 0.8977          
##                                           
##             Sensitivity : 0.9376          
##             Specificity : 0.9010          
##          Pos Pred Value : 0.9367          
##          Neg Pred Value : 0.9024          
##              Prevalence : 0.6098          
##          Detection Rate : 0.5718          
##    Detection Prevalence : 0.6104          
##       Balanced Accuracy : 0.9193          
##                                           
##        'Positive' Class : High            
## 
\end{verbatim}

The results show that the accuracy is quite high (\(92\%\)). Similarly, sensitivity and specificity are also very high. This is not necessarily surprising because we already knew that the math and reading scores are highly correlated with the science performance. Also, our decision tree model yielded very similar results.

Finally, let's visualize the classification results using \texttt{ggplot2}. First, we will create a new dataset called \texttt{rf\_class} with the predicted and actual classifications (from the test data) based on the random forest model. Then, we will visualize the correct and incorrect classifications using a bar chart and a point plot with jittering.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf_class <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{actual =}\NormalTok{ test_dat}\OperatorTok{$}\NormalTok{science_perf,}
                      \DataTypeTok{predicted =}\NormalTok{ rf_pred}\OperatorTok{$}\NormalTok{science_perf) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Status =} \KeywordTok{ifelse}\NormalTok{(actual }\OperatorTok{==}\StringTok{ }\NormalTok{predicted, }\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{))}

\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ rf_class, }
       \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ predicted, }\DataTypeTok{fill =}\NormalTok{ Status)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Predicted Science Performance"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Actual Science Performance"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=1\linewidth]{big-data-in-r_files/figure-latex/ch6-23-1}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ rf_class, }
       \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ predicted, }\DataTypeTok{y =}\NormalTok{ actual, }
                     \DataTypeTok{color =}\NormalTok{ Status, }\DataTypeTok{shape =}\NormalTok{ Status)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Predicted Science Performance"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Actual Science Performance"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=1\linewidth]{big-data-in-r_files/figure-latex/ch6-24-1}

Like decision trees, random forests can also be used for cross-validation, using the package \texttt{rfUtilities} that utilizes the objects returned from the \texttt{randomForest()} function. Below we show how cross-validation would work for random forests (output is not shown). Using the \texttt{randomForest} object that we estimated earlier (i.e.,, \texttt{rf\_fit2}), we can run cross validations as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"rfUtilities"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"rfUtilities"}\NormalTok{)}

\NormalTok{rf_fit2_cv <-}\StringTok{ }\KeywordTok{rf.crossValidation}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ rf_fit2, }
  \DataTypeTok{xdata =}\NormalTok{ train_dat,}
  \DataTypeTok{p=}\FloatTok{0.10}\NormalTok{, }\CommentTok{# Proportion of data to test (the rest is training)}
  \DataTypeTok{n=}\DecValTok{10}\NormalTok{,   }\CommentTok{# Number of cross validation samples}
  \DataTypeTok{ntree =} \DecValTok{50}\NormalTok{)   }


\CommentTok{# Plot cross validation verses model producers accuracy}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)) }
\KeywordTok{plot}\NormalTok{(rf_fit2_cv, }\DataTypeTok{type =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"CV producers accuracy"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(rf_fit2_cv, }\DataTypeTok{type =} \StringTok{"model"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Model producers accuracy"}\NormalTok{)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)) }

\CommentTok{# Plot cross validation verses model oob}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)) }
\KeywordTok{plot}\NormalTok{(rf_fit2_cv, }\DataTypeTok{type =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{stat =} \StringTok{"oob"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"CV oob error"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(rf_fit2_cv, }\DataTypeTok{type =} \StringTok{"model"}\NormalTok{, }\DataTypeTok{stat =} \StringTok{"oob"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Model oob error"}\NormalTok{)    }
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\hypertarget{supervised-machine-learning---part-ii}{%
\chapter{Supervised Machine Learning - Part II}\label{supervised-machine-learning---part-ii}}

\hypertarget{support-vector-machines}{%
\section{Support Vector Machines}\label{support-vector-machines}}

The support vector machine (SVM) is a family of related techniques developed in the 80s in computer science. They can be used in either a classification or a regression framework, but are principally known for/applied to classification (of which they are considered one of the best classification techniques because of their flexibility). Following James et al. (2013), we will make the distinction here between maximal margin classifiers (basically a support vector classifier with a cost parameter of 0 and a separating hyperplane), support vector classifiers (or an SVM with a linear kernel), and support vector machines (which employ non-linear kernels).

\hypertarget{maximal-margin-classifier}{%
\subsection{Maximal Margin Classifier}\label{maximal-margin-classifier}}

\hypertarget{hyperplane}{%
\subsubsection{Hyperplane}\label{hyperplane}}

The concept of a hyperplane is a critical concept in SVM, therefore, we need to understand what exactly a hyperplane is to understand SVM. A \textbf{hyperplane} is a subspace whose dimension is one less than that of the ambient space. Specifically, in a \emph{p}-dimensional space, a \textbf{hyperplane} is a flat affline subspace of dimensional \emph{p - 1}, where affline refers to the fact that the subspace need not pass through the origin.

We define a hyperplane as

\[
\beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p = 0
\]

Where \(X_1, X_2, ..., X_p\) are predictors (or \emph{features}). Therefore, for any observation of \(X = (X_1, X_2, \dots, X_p)^T\) that \emph{satisfies} the above equation, the observation falls directly onto the hyperplane. However, a value of \(X\) does not need to fall onto the hyperplane, but could fall on either side of the hyperplane such that either

\[
\beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_pX_p > 0
\]

or

\[
\beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_pX_p < 0
\]

occurs. In that situation, the value of \(X\) lies on one of the two sides of the hyperplane and the hyperplane acts to split the \emph{p}-dimensional space into two halves.

Figure \ref{fig:hyper} shows the hyperplane \(.5 + 1X_1 + -4X_2 = 0\). If we plug a value of \(X_1\) and \(X_2\) into this equation, we know based on the sign alone if the points falls on one side of the hyperplane or if it falls directly onto the hyperplane. In Figure \ref{fig:hyper} all the points in the red region will have negative signs (i.e., if we plug in the values of \(X_1\) and \(X_2\) into the above equation the sign will be negative), while all the points in the blue region would be positive, whereas any points that would have no sign are represented by the black line (the hyperplane).

\begin{figure}
\includegraphics[width=1\linewidth]{big-data-in-r_files/figure-latex/hyper-1} \caption{The hyperplane, $.5 + 1X_1 + -4X_2 = 0$, is black line, the red points occur in the region where $.5 + 1X_1 + -4X_2 > 0$, while the blue points occur in the region where $.5 + 1X_1 + -4X_2 < 0$.}\label{fig:hyper}
\end{figure}

Wee can apply this idea of a hyperplane to classifying observations. We learned earlier how it important it is when applying machine learning techniques to split our data into training and testing data sets to avoid overfitting. We can split our \emph{n + m} by \emph{p} matrix of observations into an \emph{n} by \emph{p} \(\mathbf{X}\) matrix of training observations, which fall into one of two classes for \(Y = y_1, .., y_n\) where \(Y_i \in {-1, 1}\) and an \emph{m} by \emph{p} matrix \(\mathbf{X^*}\) of testing observations. Using just the training data, our goal is develop a model that will correctly classify our testing data using just a hyperplane and we will do this by creating a \textbf{separating hyperplane} (a hyperplane that will separate our classes).

Let's assume we have the training data in Figure \ref{fig:hyperex} and that the blue points correspond to one class (labelled as \(y = 1\)) and the red points correspond to the other class (\(y = -1\)). The separating hyper plane has the property that:

\[
\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{p1} > 0 \quad \text{if} \quad y_i = 1
\]
and
\[
\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{p1} < 0 \quad \text{if} \quad y_i = -1
\]
Or more succintly,
\[
y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{p1}) > 0
\]

Ideally, we would create a hyperplane that perfectly separates the classes based on \(X_1\) and \(X_2\). However, as Figure \ref{fig:hyperex} makes clear, we can create many separating hyperplanes of which 3 of these are shown. In fact, it's often the case that an infinite number of separating hyperplanes could be created when the classes are perfectly separable. What we need to do is to develop some kind of a criterion for selecting one of the many separating hyperplanes.

\begin{figure}
\includegraphics[width=1\linewidth]{big-data-in-r_files/figure-latex/hyperex-1} \caption{Candidate hyperplanes to separate the two classes.}\label{fig:hyperex}
\end{figure}

For any given hyperplane, we have two pieces of information available for each observation: 1) the side of the hyperplane it lies on (represented by its sign) and 2) the distance it is from the hyperplane. The natural criterion for selecting a separating hyperplane is to \textbf{maximize the distance} it is from from the training observations. Therefore, we compute the distance that each training observation is from a candidate hyperplane. The minimal such distance from the observation to the hyperplane is known as the \textbf{margin}. Then we will select the hyperplane with the largest margin (the \textbf{maximal margin hyperplane}) and classify observations based on which side of this hyperplane they fall (\textbf{maximal margin classifier}). The hope is that a classifier with a large margin on the training data will also have a large margin on the test observations and subsequently classify well.

Figure \ref{fig:mmc} depicts a maximal margin classifier. The red line corresponds to the maximal margin hyperplane and the distance between one of the dotted lines and the black line is the \textbf{margin}. The black and white points along the boundary of the margin are the \textbf{support vectors}. It is clear in Figure \ref{fig:mmc} that the maximal margin hyperplane depends only on these two support vectors. If they are moved, the maximal margin hyperplane moves, however, if any other observations are moved they would have no effect on this hyperplane \emph{unless} they crossed the boundary of the margin.

\begin{figure}
\includegraphics[width=0.5\linewidth]{images/mmc} \caption{Maximal margin hyperplane. Source: https://tinyurl.com/y493pww8}\label{fig:mmc}
\end{figure}

The problem in practice is that a separating hyperplane usually doesn't exist. Even if a separating hyperplane existed, we may not want to use the maximal margin hyperplane as it would perfectly classify all of the observations and may be too sensitive to individual observations and subsequently overfitting.

Figure \ref{fig:fig95} from James, et al. (2013) clearly illustrates this problem. The left figure shows the maximal margin hyperplane (solid) in a completely separable solution. The figure on the right shows that when a new observation is introduced that the maximal margin hyperplane (solid) shifts rather dramatically relative to its original location (dashed).

\begin{figure}
\includegraphics[width=1\linewidth]{images/fig95} \caption{The impact of adding one observations to the maximal margin hyperplane from James et al. (2013).}\label{fig:fig95}
\end{figure}

\hypertarget{support-vector-classifier}{%
\subsection{Support Vector Classifier}\label{support-vector-classifier}}

Our hope for a hyperplane is that it would be relatively insensitive to individual observations, while still classifying training observations well. That is, we would like to have what is termed a \textbf{soft margin classifier} or a \textbf{support vector classifier}. Essentially, we are willing to allow some observations to be on the incorrect side of the margin (classified correctly) or even the incorrect side of the hyperplane (incorrectly classified) if our classifier, overall, performs well.

We do this by introducing a tuning parameter, C, which determines the number and the severity of violations to the margin/hyperplane we are willing to tolerate. As C increases, our \textbf{tolerance} for violations will increase and subsequently our margin will widen. C, thus, represents a \textbf{bias-variance tradeoff}, when C is small bias should be low, but variance will likely be high, whereas when C is large, bias is likely high but our variance is typically small. C will be selected, optimally, through cross-validation (as we'll see later).

The observations that lie on the margin or violate the margin are the only ones that will affect the hyperplane and the classifier (similar to the maximal margin classifier). These observations are the \textbf{support vectors} and only they will affect the support vector classifier. When C is large, there will be many support vectors, whereas when C is small, the number of support vectors will be less.

Because the support vector classifier depends on only the on the support vectors (which could be very few) this means they are quite \textbf{robust to the observations that are far} from the hyperplane. This makes this technique similar to logistic regression.

\hypertarget{example}{%
\subsubsection{Example}\label{example}}

In our example, we'll try and classify whether someone scores at or above the mean on the science scale we created earlier. To do support vector classifiers (and SVMs) in R, we'll use the \texttt{e1071} package (though the \texttt{caret} package could be used, too).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check if e1071 is installed}
\CommentTok{# if not, install it}
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\NormalTok{(}\StringTok{"e1071"} \OperatorTok{%in%}\StringTok{ }\KeywordTok{installed.packages}\NormalTok{()[,}\StringTok{"Package"}\NormalTok{])) \{}
  \KeywordTok{install.packages}\NormalTok{(}\StringTok{"e1071"}\NormalTok{)}
  \KeywordTok{library}\NormalTok{(}\StringTok{"e1071"}\NormalTok{)}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
  \KeywordTok{library}\NormalTok{(}\StringTok{"e1071"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The \texttt{svm} function in the \texttt{e1071} package requires that the outcome variable is a factor. So, we'll do a mean split (at the OECD mean of 493) on the \texttt{science} scale and convert it to a factor.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa[, sci_class }\OperatorTok{:}\ErrorTok{=}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(science }\OperatorTok{>=}\StringTok{ }\DecValTok{493}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{-1}\NormalTok{))]}
\end{Highlighting}
\end{Shaded}

While, I'm coding this variable as 1 and -1 to be consistent with the notation above, it doesn't matter to the \texttt{svm} function. The only thing the \texttt{svm} function needs to perform classification and not regression is that the outcome is a factor. If the outcome has just two values, a 1 and -1, but is not a factor, \texttt{svm} will perform regression.

We will use the following variables in our model:

\begin{longtable}[]{@{}ll@{}}
\toprule
Label & Description\tabularnewline
\midrule
\endhead
WEALTH & Family wealth (WLE)\tabularnewline
HEDRES & Home educational resources (WLE)\tabularnewline
ENVAWARE & Environmental Awareness (WLE)\tabularnewline
ICTRES & ICT Resources (WLE)\tabularnewline
EPIST & Epistemological beliefs (WLE)\tabularnewline
HOMEPOS & Home possessions (WLE)\tabularnewline
ESCS & Index of economic, social and cultural status (WLE)\tabularnewline
reading & Reading score\tabularnewline
math & Math score\tabularnewline
\bottomrule
\end{longtable}

We'll subset the variables to make it easier and in order for the model fitting to be performed in a reasonable amount of time in R, we'll just subset the United States and Canada.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pisa_sub <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(pisa, CNT }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Canada"}\NormalTok{, }\StringTok{"United States"}\NormalTok{), }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(sci_class, WEALTH, HEDRES, ENVAWARE, ICTRES, EPIST, HOMEPOS, ESCS, reading, math))}
\end{Highlighting}
\end{Shaded}

To fit a support vector classier, we use the \texttt{svm} function. Before we get started, let's divide the data set into a training and a testing data set. We will use a 66/33 split, though other splits could be used (e.g., 50/50).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set a random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{442019}\NormalTok{)}

\CommentTok{# svm uses listwise deletion, so we should just drop}
\CommentTok{# the observations now}
\NormalTok{pisa_m <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(pisa_sub)}

\CommentTok{# select the rows that will go into the training data set.}
\NormalTok{train <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(pisa_m), }\DecValTok{2}\OperatorTok{/}\DecValTok{3} \OperatorTok{*}\StringTok{ }\KeywordTok{nrow}\NormalTok{(pisa_m))}

\CommentTok{# subset the data based on the rows that were selected to be in training data set.}
\NormalTok{train_dat <-}\StringTok{ }\NormalTok{pisa_m[train, ]}
\NormalTok{test_dat <-}\StringTok{ }\NormalTok{pisa_m[}\OperatorTok{-}\NormalTok{train, ]}
\end{Highlighting}
\end{Shaded}

To perform support vector classification, we pass the \texttt{svm} function the \texttt{kernel\ =\ "linear"} argument. We also need to specify our tolerance, which is represented by the \texttt{cost} argument. The \texttt{cost} parameter is essentially the inverse of the tolerance parameter, C, described above. When the \texttt{cost} value is low, the tolerance is high (i.e., the margin is wide and there are lots of support vectors) and when the \texttt{cost} value is high, the tolerance is low (i.e., narrower margin). By default \texttt{cost\ =\ 1} and we will tune this parameter via cross-validation momentarily. For now, we'll just fit the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svc_fit <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(sci_class }\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train_dat, }\DataTypeTok{kernel =} \StringTok{"linear"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can obtain basic information about our model using the \texttt{summary} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(svc_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## svm(formula = sci_class ~ ., data = train_dat, kernel = "linear")
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  1 
##       gamma:  0.1111111 
## 
## Number of Support Vectors:  2782
## 
##  ( 1390 1392 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  -1 1
\end{verbatim}

We see there are 2782 support vectors: 1390 in class -1 and 1392 in class 1. We can also plot our model but we need to specific the two features we want to plot (because our model has nine feature). Let's look at the model with \texttt{math} on the y-axis and \texttt{reading} on the x-axis.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(svc_fit, }\DataTypeTok{data =}\NormalTok{ train_dat, math }\OperatorTok{~}\StringTok{ }\NormalTok{reading)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=1\linewidth]{big-data-in-r_files/figure-latex/svcplot-1} \caption{Support vector classifier plot for all the training data.}\label{fig:svcplot}
\end{figure}

In this figure, the red points correspond to observations that belong to class 1 (below the mean on science), while the black points correspond to observations that belong to class -1 (at/above the mean on science); the Xs are the support vectors, while the Os are the non-support vector observations; the upper triangle (purple) are for class 1, while the lower triangle (blue) is for class -1. While the decision boundary looks jagged, it's just an artifact of the way it's drawn with this function. We can see that many observations are misclassified (i.e., some red points are in the lower triangle and some black points are in the upper triangle). However, there are a lot of observations shown in this figure and it is difficult to discern the nature of the misclassification.

As was discussed in the section on data visualization, with this many points on a figure it is difficult to evaluate patterns, not to mention that the figure is extremely slow to render. Therefore, let's take a random sample of 1,000 observations to get a better sense of our classifier. This is shown in Figure \ref{fig:svcplotran}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{ran_obs <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(train_dat), }\DecValTok{1000}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(svc_fit, }\DataTypeTok{data =}\NormalTok{ train_dat[ran_obs, ], math }\OperatorTok{~}\StringTok{ }\NormalTok{reading)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=1\linewidth]{big-data-in-r_files/figure-latex/svcplotran-1} \caption{Support vector classifier plot for all a random subsample (n = 1000) of training observations.}\label{fig:svcplotran}
\end{figure}

Notice that few points are crossing the hyperplane (i.e., are misclassified). This looks like the classier is doing pretty good.

Initially when we fit the support vector classifier we used the default cost parameter, but we really should select this parameter through tuning via cross-validation as we might be able to do an even better job at classifying. The \texttt{e1071} package includes a \texttt{tune} function which makes this easy and automatic. It performs the tuning via 10-folds cross-validation by default, which is probably a fine tradeoff (see James, et al.~2013 for a comparison of k-folds vs.~leave one out cross-validation). We need to provide the \texttt{tune} function with a range of cost values (which again corresponds to our tolerance to violate the margin and hyperplane).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tune_svc <-}\StringTok{ }\KeywordTok{tune}\NormalTok{(svm, sci_class }\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train_dat,}
                 \DataTypeTok{kernel=}\StringTok{"linear"}\NormalTok{,}
                 \DataTypeTok{ranges =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{cost =} \KeywordTok{c}\NormalTok{(.}\DecValTok{01}\NormalTok{, }\FloatTok{.1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

On my Macbook Pro (2.6 GHz Intel Core i7 and 16 GB RAM) it takes approximately 2 minutes run this. Without doing this subsetting, it will take quite a bit longer to do.

We can view the cross-validation errors by using the \texttt{summary} function on this object.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(tune_svc)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Parameter tuning of 'svm':
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost
##     1
## 
## - best performance: 0.07316727 
## 
## - Detailed performance results:
##    cost      error  dispersion
## 1  0.01 0.07342096 0.005135708
## 2  0.10 0.07354766 0.004985649
## 3  1.00 0.07316727 0.004952085
## 4  5.00 0.07329406 0.004879146
## 5 10.00 0.07335747 0.004887063
\end{verbatim}

And then select the best model and view it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best_svc <-}\StringTok{ }\NormalTok{tune_svc}\OperatorTok{$}\NormalTok{best.model}
\KeywordTok{summary}\NormalTok{(best_svc)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## best.tune(method = svm, train.x = sci_class ~ ., data = train_dat, 
##     ranges = list(cost = c(0.01, 0.1, 1, 5, 10)), kernel = "linear")
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  1 
##       gamma:  0.1111111 
## 
## Number of Support Vectors:  2782
## 
##  ( 1390 1392 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  -1 1
\end{verbatim}

Next, we write a function to evaluate our classifier that has one argument that takes a confusion matrix.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#' Evaluate classifier}
\CommentTok{#'}
\CommentTok{#' Evaluates a classifier (e.g. SVM, logistic regression)}
\CommentTok{#' @param tab a confusion matrix}
\NormalTok{eval_classifier <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(tab, }\DataTypeTok{print =}\NormalTok{ F)\{}
\NormalTok{  n <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(tab)}
\NormalTok{  TP <-}\StringTok{ }\NormalTok{tab[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}
\NormalTok{  FN <-}\StringTok{ }\NormalTok{tab[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\NormalTok{  FP <-}\StringTok{ }\NormalTok{tab[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}
\NormalTok{  TN <-}\StringTok{ }\NormalTok{tab[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\NormalTok{  classify.rate <-}\StringTok{ }\NormalTok{(TP }\OperatorTok{+}\StringTok{ }\NormalTok{TN) }\OperatorTok{/}\StringTok{ }\NormalTok{n}
\NormalTok{  TP.rate <-}\StringTok{ }\NormalTok{TP }\OperatorTok{/}\StringTok{ }\NormalTok{(TP }\OperatorTok{+}\StringTok{ }\NormalTok{FN)}
\NormalTok{  TN.rate <-}\StringTok{ }\NormalTok{TN }\OperatorTok{/}\StringTok{ }\NormalTok{(TN }\OperatorTok{+}\StringTok{ }\NormalTok{FP)}
\NormalTok{  object <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{accuracy =}\NormalTok{ classify.rate,}
                       \DataTypeTok{sensitivity =}\NormalTok{ TP.rate,}
                       \DataTypeTok{specificity =}\NormalTok{ TN.rate)}
\NormalTok{  object}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The confusion matrix is just a list of all possible outcomes (true positives, true negatives, false positives, and false negatives). A confusion matrix for our \texttt{best\_svc} can be created by:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# to create a confusion matrix this order is important!}
\CommentTok{# observed values first and predict values second!}
\NormalTok{svc_cm_train <-}\StringTok{ }\KeywordTok{table}\NormalTok{(train_dat}\OperatorTok{$}\NormalTok{sci_class,}
                      \KeywordTok{predict}\NormalTok{(best_svc)) }
\NormalTok{svc_cm_train}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     
##        -1    1
##   -1 5563  606
##   1   550 9053
\end{verbatim}

The top-left are the true negatives, the bottom-left are the false negatives, the top-right are the false positives, and the bottom-right are the true positives. We can request the accuracy (the \% of observations that were correctly classified), the sensitivity (the \% of observations that were in class 1 that were correctly identified), and specificity (the \% of observations that were in class -1 that were correctly identified) using the \texttt{eval\_classifier} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{eval_classifier}\NormalTok{(svc_cm_train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    accuracy sensitivity specificity
## 1 0.9267056   0.9427262   0.9017669
\end{verbatim}

Performance is pretty good overall. We see that class -1 (specificity) isn't classified as well as class 1 (sensitivity). These statistics are likely overly optimistic as we are evaluating our model using the training data (the same data that we used to build our model). How well does the model perform on the testing data?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svc_cm_test <-}\StringTok{ }\KeywordTok{table}\NormalTok{(test_dat}\OperatorTok{$}\NormalTok{sci_class,}
                     \KeywordTok{predict}\NormalTok{(best_svc, }\DataTypeTok{newdata =}\NormalTok{ test_dat))}
\NormalTok{svc_cm_test}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     
##        -1    1
##   -1 2780  281
##   1   278 4547
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{eval_classifier}\NormalTok{(svc_cm_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    accuracy sensitivity specificity
## 1 0.9291149   0.9423834   0.9081999
\end{verbatim}

Still impressively high! This is a very good classifier indeed. This is likely because \texttt{math} and \texttt{reading} are so highly correlated with \texttt{science} scores.

We can extract the coefficients from our model that make up our decision boundary.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta0 <-}\StringTok{ }\NormalTok{best_svc}\OperatorTok{$}\NormalTok{rho}
\NormalTok{beta <-}\StringTok{  }\KeywordTok{drop}\NormalTok{(}\KeywordTok{t}\NormalTok{(best_svc}\OperatorTok{$}\NormalTok{coefs) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(train_dat[best_svc}\OperatorTok{$}\NormalTok{index, }\DecValTok{-1}\NormalTok{]))}
\NormalTok{beta0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -1.220883
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       WEALTH       HEDRES     ENVAWARE       ICTRES        EPIST 
##   0.04398688  -0.24398165   0.36167882  -0.09803825   0.04652237 
##      HOMEPOS         ESCS      reading         math 
##   0.22005477  -0.15065808 188.02960807 196.93421586
\end{verbatim}

With more complicated SVMs with non-linear kernels, coefficients don't make any sense and generally are of little interest with applying these models.

\hypertarget{comparison-to-logistic-regression}{%
\subsubsection{Comparison to logistic regression}\label{comparison-to-logistic-regression}}

Support vector classifiers are quite similar to logistic regression. This has to do with them having similar loss functions (the functions used to estimate the parameters). In situations where the classes are well separated, SVM (more generally), tend to do better than logistic regression and when they are not well separated, logistic regression tends to do better (James, et al., 2013).

Let's compare logistic regression to the support vector classier. We'll begin by fitting the model

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lr_fit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(sci_class }\OperatorTok{~}\NormalTok{. , }\DataTypeTok{data =}\NormalTok{ train_dat, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

and then viewing the coefficients.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coef}\NormalTok{(lr_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  (Intercept)       WEALTH       HEDRES     ENVAWARE       ICTRES 
## -41.82682653   0.11666541  -0.26667828   0.30159987  -0.13594566 
##        EPIST      HOMEPOS         ESCS      reading         math 
##   0.05053261   0.20699211  -0.24568642   0.03917470   0.04651408
\end{verbatim}

How does it do relative to our best support vector classifier on the training and the testing data sets? For the training data set

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lr_cm_train <-}\StringTok{ }\KeywordTok{table}\NormalTok{(train_dat}\OperatorTok{$}\NormalTok{sci_class,}
                     \KeywordTok{round}\NormalTok{(}\KeywordTok{predict}\NormalTok{(lr_fit, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)))}
\NormalTok{lr_cm_train}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     
##         0    1
##   -1 5567  602
##   1   541 9062
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{eval_classifier}\NormalTok{(lr_cm_train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    accuracy sensitivity specificity
## 1 0.9275298   0.9436634   0.9024153
\end{verbatim}

and then for the testing data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lr_cm_test <-}\StringTok{ }\KeywordTok{table}\NormalTok{(test_dat}\OperatorTok{$}\NormalTok{sci_class,}
                     \KeywordTok{round}\NormalTok{(}\KeywordTok{predict}\NormalTok{(lr_fit, }\DataTypeTok{newdata =}\NormalTok{ test_dat, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)))}
\NormalTok{lr_cm_test}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     
##         0    1
##   -1 2780  281
##   1   275 4550
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{eval_classifier}\NormalTok{(lr_cm_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    accuracy sensitivity specificity
## 1 0.9294953   0.9430052   0.9081999
\end{verbatim}

Equivalent out to the hundredths place. Either model would be fine here.

\hypertarget{using-apache-spark-for-machine-learning}{%
\subsubsection{Using Apache Spark for machine learning}\label{using-apache-spark-for-machine-learning}}

Apache Spark is also capable of running support vector classifiers. It does this using the \texttt{ml\_linear\_svc} function. The amazing thing about this is that you can use it to run the entire data set (i.e., there is no need to subset out a portion of the countries). If we tried to do this with the \texttt{e1071} package it would be very impractical and take forever, but with Apache Spark it is feasible and reasonably quick (just a few minutes).

We'll again use the \texttt{sparklyr} package to interface with Spark and use the \texttt{dplyr} package to simplify interacting with Spark.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sparklyr)}
\KeywordTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

We first need to establish a connection with Spark and then copy a subsetted PISA data set to Spark.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sc <-}\StringTok{ }\KeywordTok{spark_connect}\NormalTok{(}\DataTypeTok{master =} \StringTok{"local"}\NormalTok{)}
\NormalTok{spark_sub <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(pisa, }
                    \DataTypeTok{select =} \KeywordTok{c}\NormalTok{(sci_class, WEALTH, HEDRES, ENVAWARE, ICTRES,}
\NormalTok{                               EPIST, HOMEPOS, ESCS, reading, math))}
\NormalTok{spark_sub <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(spark_sub) }\CommentTok{# can't handle missing data}
\NormalTok{pisa_tbl <-}\StringTok{ }\KeywordTok{copy_to}\NormalTok{(sc, spark_sub, }\DataTypeTok{overwrite =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now, we'll let Spark partition the data into a training and a test data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{partition <-}\StringTok{ }\NormalTok{pisa_tbl }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{sdf_partition}\NormalTok{(}\DataTypeTok{training =} \DecValTok{2}\OperatorTok{/}\DecValTok{3}\NormalTok{, }\DataTypeTok{test =} \DecValTok{1}\OperatorTok{/}\DecValTok{3}\NormalTok{, }\DataTypeTok{seed =} \DecValTok{442019}\NormalTok{)}
\NormalTok{pisa_training <-}\StringTok{ }\NormalTok{partition}\OperatorTok{$}\NormalTok{training}
\NormalTok{pisa_test <-}\StringTok{ }\NormalTok{partition}\OperatorTok{$}\NormalTok{test}
\end{Highlighting}
\end{Shaded}

We are ready to run the classifier in Spark. Unlike the \texttt{svm} function, the tolerance parameter is called \texttt{reg\_param}. This parameter should be optimally selected like it was for \texttt{svm}. By default the tolerance is 1e-06.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svc_spark <-}\StringTok{ }\NormalTok{pisa_training }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ml_linear_svc}\NormalTok{(sci_class }\OperatorTok{~}\StringTok{ }\NormalTok{.)}
\end{Highlighting}
\end{Shaded}

We then use the \texttt{ml\_predict} function to predict the classes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svc_pred <-}\StringTok{ }\KeywordTok{ml_predict}\NormalTok{(svc_spark, pisa_training) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(sci_class, predicted_label) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{collect}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Then print the confusion matrix and the criteria that we've been using to evaluate our models.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(svc_pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          predicted_label
## sci_class     -1      1
##        -1 145111  12353
##        1   10753 121967
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{eval_classifier}\NormalTok{(}\KeywordTok{table}\NormalTok{(svc_pred))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    accuracy sensitivity specificity
## 1 0.9203747   0.9189798   0.9215503
\end{verbatim}

Again, this is really good. How does it look on the testing data?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svc_pred_test <-}\StringTok{ }\KeywordTok{ml_predict}\NormalTok{(svc_spark, pisa_test) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(sci_class, predicted_label) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{collect}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(svc_pred_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          predicted_label
## sci_class    -1     1
##        -1 72577  6199
##        1   5438 60953
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{eval_classifier}\NormalTok{(}\KeywordTok{table}\NormalTok{(svc_pred_test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    accuracy sensitivity specificity
## 1 0.9198372   0.9180913   0.9213085
\end{verbatim}

Pretty impressive. We can also Apache Spark to fit logistic regression using the \texttt{ml\_logistic\_regression} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spark_lr <-}\StringTok{ }\NormalTok{pisa_training }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ml_logistic_regression}\NormalTok{(sci_class }\OperatorTok{~}\StringTok{ }\NormalTok{.)}
\end{Highlighting}
\end{Shaded}

And view the performance on the training and test data sets.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Training data }
\NormalTok{svc_pred_lr <-}\StringTok{ }\KeywordTok{ml_predict}\NormalTok{(spark_lr, pisa_training) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(sci_class, predicted_label) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{collect}\NormalTok{()}
\KeywordTok{table}\NormalTok{(svc_pred_lr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          predicted_label
## sci_class     -1      1
##        -1 146217  11247
##        1   11133 121587
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{eval_classifier}\NormalTok{(}\KeywordTok{table}\NormalTok{(svc_pred_lr))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    accuracy sensitivity specificity
## 1 0.9228765   0.9161166   0.9285742
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Test data}
\NormalTok{svc_pred_test_lr <-}\StringTok{ }\KeywordTok{ml_predict}\NormalTok{(spark_lr, pisa_test) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(sci_class, predicted_label) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{collect}\NormalTok{()}
\KeywordTok{table}\NormalTok{(svc_pred_test_lr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          predicted_label
## sci_class    -1     1
##        -1 73098  5678
##        1   5646 60745
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{eval_classifier}\NormalTok{(}\KeywordTok{table}\NormalTok{(svc_pred_test_lr))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    accuracy sensitivity specificity
## 1 0.9219933   0.9149584   0.9279222
\end{verbatim}

We could also use the logistic regression in R as it's pretty quick even with this large of a data set (in fact, it's slightly quicker).

Finally, it is quite common to evaluate these models using AUC. We can let Apache Spark do this for the test data sets.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# extract predictions}
\NormalTok{pred_svc <-}\StringTok{ }\KeywordTok{ml_predict}\NormalTok{(svc_spark, pisa_test)}
\NormalTok{pred_lr <-}\StringTok{ }\KeywordTok{ml_predict}\NormalTok{(spark_lr, pisa_test)}

\KeywordTok{ml_binary_classification_evaluator}\NormalTok{(pred_svc)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9795075
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ml_binary_classification_evaluator}\NormalTok{(pred_lr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9805051
\end{verbatim}

We want these values as close to 1 as a possible. These values are all quite large and corroborate that these are both good classifiers.

\hypertarget{support-vector-machine}{%
\subsection{Support Vector Machine}\label{support-vector-machine}}

SVM is an extension of support vector classifiers using \textbf{kernels} that allow for a non-linear boundary between the classes. Without getting into the weeds, to solve a support vector classifier problem all you need to know is the inner products of the observations. Assuming that \(x_i\) and \(x_i'\) are two observations and \(p\) is the number of predictors (features), their inner product is defined as:

\[
\langle x_i, x_i'\rangle = \begin{bmatrix} 
x_{i1} x_{i2} \dots x_{ip}
\end{bmatrix} 
\begin{bmatrix} 
x_{i1}' \\
x_{i2}' \\
\vdots \\
x_{ip}'
\end{bmatrix} = x_{i1}x_{i1}' + x_{i2}x_{i2}' + \dots x_{ip}x_{ip}'
\]

More succinctly, \(\langle x_i, x_i'\rangle = \sum_{i = 1}^p x_{ij}x_{ij}'\). We can replace the inner product with a more general form, \(K(x_i, x_i')\), where \(K\) is a kernel (a function that quantifies the similarity of two observations). When,

\[
K(x_i, x_i') = \sum_{i = 1}^p x_{ij}x_{ij}
\]

We have the linear kernel and this is the support vector classifier. However, we can use a more flexible kernel. Such as:

\[
K(x_i, x_i') = (1 + \sum_{i = 1}^p x_{ij}x_{ij})^d
\]

which is known as a \textbf{polynomial kernel} of degree \(d\) and when \(d > 1\) we have much more flexible decision boundary than we do for support vector classifiers (when \(d = 1\) we are back to the support vector classifier).

Another very common kernel is the \textbf{radial kernel}, which is given by:

\[
K(x_i, x_i') = \exp\left(-\gamma \sum_{i = 1}^p (x_{ij} - x_{ij})^2\right)
\]

where \(\gamma\) is a positive constant. Note, both \(d\) and \(\gamma\) are selected via tuning and cross-validation.

Both of these kernels are worth considering when the decision boundary is non-linear. Figure \ref{fig:james2} from James, et al. (2013) gives an example of a non-linear boundary. We see that the classes are not linearly separated and if we tried to use a linear decision boundary, we would end up with a very poor classifier. Therefore, we need to use a more flexible kernel. In both cases, we should expect that an SVM would greatly outperform both a support vector classifier and logistic regression.

\begin{figure}
\includegraphics[width=1\linewidth]{images/james2} \caption{Non-linear decision boundary with a polynomial kernel (left) and radial kernel (right) from James et al., 2013.}\label{fig:james2}
\end{figure}

\hypertarget{examples}{%
\subsubsection{Examples}\label{examples}}

We will continue trying to build the best classifier of whether someone scored in the upper or lower half on the science scale and again use the \texttt{svm} function in the \texttt{e1071} package. For brevity, we'll consider only the radial kernel. By default gamma is set to 1. We'll explicitly set it to 1 below and cost to 1.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svm_fit <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(sci_class }\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train_dat,}
               \DataTypeTok{cost =} \DecValTok{1}\NormalTok{,}
               \DataTypeTok{gamma =} \DecValTok{1}\NormalTok{, }
               \DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Again, we can request some basic information about our model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(svm_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## svm(formula = sci_class ~ ., data = train_dat, cost = 1, gamma = 1, 
##     kernel = "radial")
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  radial 
##        cost:  1 
##       gamma:  1 
## 
## Number of Support Vectors:  6988
## 
##  ( 3676 3312 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  -1 1
\end{verbatim}

This time we see we have 6988 support vectors, 3676 in class -1 and 3312 in class 1. Quite a bit more support vectors than the support vector classifier. Lets visually inspect this model by plotting it against the math and reading features on the same subset of test takers (Figure \ref{fig:svmplot}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(svm_fit, }\DataTypeTok{data =}\NormalTok{ train_dat[ran_obs, ], math }\OperatorTok{~}\StringTok{ }\NormalTok{reading)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=1\linewidth]{big-data-in-r_files/figure-latex/svmplot-1} \caption{Support vector classifier plot for all a random subsample (n = 1000) of training observations.}\label{fig:svmplot}
\end{figure}

We see that the decision boundary is now clearly no longer linear and we again see decent classification. Before we investigate the fit of the model, we should tune it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tune_svm <-}\StringTok{ }\KeywordTok{tune}\NormalTok{(svm, sci_class }\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train_dat,}
                 \DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{,}
                 \DataTypeTok{ranges =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{cost =} \KeywordTok{c}\NormalTok{(.}\DecValTok{01}\NormalTok{, }\FloatTok{.1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{),}
                               \DataTypeTok{gamma =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

We can see which model was selected

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(tune_svm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Parameter tuning of 'svm':
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost gamma
##   0.1   0.5
## 
## - best performance: 0.07583144 
## 
## - Detailed performance results:
##     cost gamma      error  dispersion
## 1   0.01   0.5 0.17670590 0.010347886
## 2   0.10   0.5 0.07583144 0.008233504
## 3   1.00   0.5 0.07754307 0.009223528
## 4   5.00   0.5 0.08375680 0.008098257
## 5  10.00   0.5 0.08718054 0.008157393
## 6   0.01   1.0 0.36425229 0.011955406
## 7   0.10   1.0 0.13162657 0.007210614
## 8   1.00   1.0 0.08242504 0.008590571
## 9   5.00   1.0 0.09402859 0.009848512
## 10 10.00   1.0 0.10074908 0.007984562
## 11  0.01   2.0 0.39113500 0.011126599
## 12  0.10   2.0 0.31409966 0.010609909
## 13  1.00   2.0 0.11469785 0.006880824
## 14  5.00   2.0 0.12363760 0.006591525
## 15 10.00   2.0 0.12465198 0.006523243
## 16  0.01   3.0 0.39113500 0.011126599
## 17  0.10   3.0 0.38257549 0.012981991
## 18  1.00   3.0 0.17562831 0.007488136
## 19  5.00   3.0 0.17277475 0.006502038
## 20 10.00   3.0 0.17309173 0.006145790
## 21  0.01   4.0 0.39113500 0.011126599
## 22  0.10   4.0 0.39107163 0.011072976
## 23  1.00   4.0 0.23960159 0.011545434
## 24  5.00   4.0 0.22641364 0.008709051
## 25 10.00   4.0 0.22641360 0.008779341
\end{verbatim}

And then select the best model and view it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best_svm <-}\StringTok{ }\NormalTok{tune_svm}\OperatorTok{$}\NormalTok{best.model}
\KeywordTok{summary}\NormalTok{(best_svm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## best.tune(method = svm, train.x = sci_class ~ ., data = train_dat, 
##     ranges = list(cost = c(0.01, 0.1, 1, 5, 10), gamma = c(0.5, 
##         1, 2, 3, 4)), kernel = "radial")
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  radial 
##        cost:  0.1 
##       gamma:  0.5 
## 
## Number of Support Vectors:  6138
## 
##  ( 3095 3043 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  -1 1
\end{verbatim}

Finally, we see how well this predicts on both the training observations

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svm_cm_train <-}\StringTok{ }\KeywordTok{table}\NormalTok{(train_dat}\OperatorTok{$}\NormalTok{sci_class,}
                     \KeywordTok{predict}\NormalTok{(best_svm))}
\NormalTok{svm_cm_train}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     
##        -1    1
##   -1 5620  549
##   1   519 9084
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{eval_classifier}\NormalTok{(svm_cm_train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    accuracy sensitivity specificity
## 1 0.9322851   0.9459544   0.9110066
\end{verbatim}

and finally the testing observations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svm_cm_test <-}\StringTok{ }\KeywordTok{table}\NormalTok{(test_dat}\OperatorTok{$}\NormalTok{sci_class,}
                     \KeywordTok{predict}\NormalTok{(best_svm, }\DataTypeTok{newdata =}\NormalTok{ test_dat))}
\NormalTok{svm_cm_test}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     
##        -1    1
##   -1 2781  280
##   1   291 4534
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{eval_classifier}\NormalTok{(svm_cm_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    accuracy sensitivity specificity
## 1 0.9275932   0.9396891   0.9085266
\end{verbatim}

Performance is very comparable to the support vector classifier and logistic regression implying there isn't much gain from the use of non-linear decision boundary.

\hypertarget{lab-2}{%
\subsection{Lab}\label{lab-2}}

For the lab, we'll try to build the best classifier for the ``Do you expect your child will go into a ?'' item. Using the following variables (and any variables that you think might be relevant in the codebook) and \textbf{data for just Mexico}, try and build the best classifier. Do the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Split the data into a training and a testing data set. Rather than using a 66/33 split, try a 50/50 or a 75/25 split.
\item
  Fit a decision tree \textbf{or} random forest
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Prune your model and plot your model (if using decision trees)
\item
  Determine the ideal number of trees (if using random forests)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Fit a support vector machine
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Consider different kernels (e.g., linear and radial)
\item
  Visually inspect your model by plotting it against a few features. Create a few different plots.
\item
  Tune the parameters.

  \begin{itemize}
  \tightlist
  \item
    How many support vectors do you have?
  \item
    Did you notice much difference in the error rates?
  \item
    Does your model have a high tolerance?
  \end{itemize}
\item
  (OPTIONAL): When fitting the support vector classifier, you could try and fit it using Apache Spark

  \begin{itemize}
  \tightlist
  \item
    If you do this, use the \texttt{ml\_binary\_classification\_evaluator} function to calculate AUC.
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Run a logistic regression
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Examine the coefficients table
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Evaluate the fit of your models using the \texttt{eval\_classifier} function on the testing data.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Which model(s) fits the best? Can you improve it?
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Record your accuracy, sensitivity, and specificity for all the models (decision tree or random forest and SVM) to share.
\end{enumerate}

The following table contains the list of variables you could consider (this were introduced earlier):

\begin{longtable}[]{@{}ll@{}}
\toprule
Label & Description\tabularnewline
\midrule
\endhead
DISCLISCI & Disciplinary climate in science classes (WLE)\tabularnewline
TEACHSUP & Teacher support in a science classes of students choice (WLE)\tabularnewline
IBTEACH & Inquiry-based science teaching an learning practices (WLE)\tabularnewline
TDTEACH & Teacher-directed science instruction (WLE)\tabularnewline
ENVAWARE & Environmental Awareness (WLE)\tabularnewline
JOYSCIE & Enjoyment of science (WLE)\tabularnewline
INTBRSCI & Interest in broad science topics (WLE)\tabularnewline
INSTSCIE & Instrumental motivation (WLE)\tabularnewline
SCIEEFF & Science self-efficacy (WLE)\tabularnewline
EPIST & Epistemological beliefs (WLE)\tabularnewline
SCIEACT & Index science activities (WLE)\tabularnewline
BSMJ & Student's expected occupational status (SEI)\tabularnewline
MISCED & Mother's Education (ISCED)\tabularnewline
FISCED & Father's Education (ISCED)\tabularnewline
OUTHOURS & Out-of-School Study Time per week (Sum)\tabularnewline
SMINS & Learning time (minutes per week) - \tabularnewline
TMINS & Learning time (minutes per week) - in total\tabularnewline
BELONG & Subjective well-being: Sense of Belonging to School (WLE)\tabularnewline
ANXTEST & Personality: Test Anxiety (WLE)\tabularnewline
MOTIVAT & Student Attitudes, Preferences and Self-related beliefs: Achieving motivation (WLE)\tabularnewline
COOPERATE & Collaboration and teamwork dispositions: Enjoy cooperation (WLE)\tabularnewline
PERFEED & Perceived Feedback (WLE)\tabularnewline
unfairteacher & Teacher Fairness (Sum)\tabularnewline
HEDRES & Home educational resources (WLE)\tabularnewline
HOMEPOS & Home possessions (WLE)\tabularnewline
ICTRES & ICT Resources (WLE)\tabularnewline
WEALTH & Family wealth (WLE)\tabularnewline
ESCS & Index of economic, social and cultural status (WLE)\tabularnewline
math & Students' math scores\tabularnewline
reading & Students' reading scores\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{unsupervised-machine-learning}{%
\chapter{Unsupervised machine learning}\label{unsupervised-machine-learning}}

\hypertarget{clustering}{%
\section{Clustering}\label{clustering}}

Clustering is a broad set of techniques for finding subgroups of observations within a data set. When we cluster observations, we want observations in the same group to be similar and observations in different groups to be dissimilar. Because there isn't a response variable, this is an unsupervised method, which implies that it seeks to find relationships between the observations without being trained by a response variable. Clustering allows us to identify which observations are alike, and potentially categorize them therein.

\hypertarget{distance-measures}{%
\section{Distance Measures}\label{distance-measures}}

The classification of observations into groups requires some methods for computing the distance or the (dis)similarity between each pair of observations. The result of this computation is known as a dissimilarity or distance matrix. There are many methods to calculate this distance information; the choice of distance measures is a critical step in clustering. It defines how the similarity of two elements (x, y) is calculated and it will influence the shape of the clusters.

The choice of distance measures is a critical step in clustering. It defines how the similarity of two elements (x, y) is calculated and it will influence the shape of the clusters. The classical methods for distance measures are Euclidean and Manhattan distances, which are defined as follow:

\textbf{Euclidean distance:}

\(d_{euc}(x,y) = \sqrt{\sum^n_{i=1}(x_i - y_i)^2}\)

\textbf{Manhattan distance:}

\(d_{man}(x,y) = \sum^n_{i=1}|(x_i - y_i)|\)

where \(x\) and \(y\) are two vectors of length \(n\).

The choice of distance measures is very important, as it has a strong influence on the clustering results. For most common clustering software, the default distance measure is the Euclidean distance. However, depending on the type of the data and the research questions, other dissimilarity measures might be preferred and you should be aware of the options.

\hypertarget{k-means-clustering}{%
\section{K-means clustering}\label{k-means-clustering}}

\textbf{K-means} clustering is the most commonly used unsupervised machine learning algorithm for partitioning a given data set into a set of k groups (i.e., \(k\) clusters), where \(k\) represents the number of groups pre-specified by the researcher. It classifies objects in multiple groups (i.e., clusters), such that objects within the same cluster are as similar as possible (i.e., high intra-class similarity), whereas objects from different clusters are as dissimilar as possible (i.e., low inter-class similarity). In K-means clustering, each cluster is represented by its center (i.e, centroid) which corresponds to the mean of points assigned to the cluster.

There are several k-means algorithms available. The standard algorithm is the Hartigan-Wong algorithm (1979), which defines the total within-cluster variation as the sum of squared distances Euclidean distances between items and the corresponding centroid:

\(W(C_k) = \sum_{x_i \in C_k}(x_i - \mu_k)^2\)

where:

\emph{\(X_i\) is a data point belonging to the cluster \(C_k\)
}\(\mu_k\) is the mean value of the points assigned to the cluster \(C_k\)

Each observation is assigned to a given cluster such that the sum of squares (SS) distance of the observation to their assigned cluster centers is minimized.

We can define the total within-cluster variation as follows:

\(SS_{total.within} = \sum^k_{k=1}W(C_k) = \sum^k_{k=1}\sum_{x_i \in C_k}(x_i - \mu_k)^2\)

The total \emph{within-cluster} sum of square measures the compactness (i.e., goodness) of the clustering and we want it to be as small as possible.

Generally, K-means algorithm can be used as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Specify the number of clusters (\(K\)) to be created.
\item
  Select randomly \(k\) objects from the data set as the initial cluster centers or means
\item
  Assign each observation to their closest centroid, based on the Euclidean distance between the object and the centroid
\item
  For each of the \(k\) clusters, update the cluster centroid by calculating the new mean values of all the data points in the cluster. The centroid of a \(K^{th}\) cluster is a vector of length \(p\) containing the means of all variables for the observations in the \(k^{th}\) cluster; \(p\) is the number of variables.
\item
  Iteratively minimize the total within sum of square (see Eq. 4) by iterating steps 3 and 4 until the cluster assignments stop changing or the maximum number of iterations is reached.
\end{enumerate}

\hypertarget{k-means-clustering-in-r}{%
\section{K-means clustering in R}\label{k-means-clustering-in-r}}

\emph{To be completed later\ldots{}}

Check out the following website for some examples in R: \url{https://uc-r.github.io/kmeans_clustering}

\hypertarget{summary-1}{%
\chapter{Summary}\label{summary-1}}

\hypertarget{topics-covered}{%
\section{Topics covered}\label{topics-covered}}

\hypertarget{exploratory-data-analysis}{%
\subsection{Exploratory data analysis}\label{exploratory-data-analysis}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data Wrangling

  \begin{itemize}
  \tightlist
  \item
    Subsetting, creating variables, reshaping, and summarizing
  \item
    \texttt{data.table}
  \item
    \texttt{dplyr}
  \item
    \texttt{sparklyr} and Apache Spark
  \end{itemize}
\item
  Data Visualization

  \begin{itemize}
  \tightlist
  \item
    Static visualizations
  \item
    \texttt{ggplot2}
  \item
    \texttt{ggplot2} add-ons \texttt{GGally}, \texttt{ggExtra}, and \texttt{ggalluvial}
  \item
    \texttt{cowplot} as an additional \texttt{ggplot2} theme
  \item
    Interactive visualizations
  \item
    \texttt{plotly}
  \end{itemize}
\end{enumerate}

\hypertarget{supervised-learning}{%
\subsection{Supervised learning}\label{supervised-learning}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Decision trees

  \begin{itemize}
  \tightlist
  \item
    Classification and regression trees
  \item
    \texttt{rpart}
  \item
    \texttt{rpart.plot}
  \end{itemize}
\item
  Random forests

  \begin{itemize}
  \tightlist
  \item
    Random forests for classification and regression
  \item
    \texttt{randomForest}
  \end{itemize}
\item
  Model building and evaluation

  \begin{itemize}
  \tightlist
  \item
    \texttt{modelr}
  \item
    \texttt{caret}
  \end{itemize}
\item
  Support Vector Machines

  \begin{itemize}
  \tightlist
  \item
    Maximal margin classifers
  \item
    Support vector classifiers
  \item
    Support vector machines with polynomial and radial kernels
  \item
    Logistic regression
  \item
    Tuning and evaluating the models
  \item
    \texttt{e1071}
  \item
    \texttt{sparklyr}
  \end{itemize}
\end{enumerate}

\hypertarget{methods-we-didnt-cover}{%
\section{Methods we didn't cover}\label{methods-we-didnt-cover}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Regression

  \begin{itemize}
  \tightlist
  \item
    Penalized regression

    \begin{itemize}
    \tightlist
    \item
      ridge, lasso, elastic net
    \item
      \texttt{glmnet}
    \end{itemize}
  \item
    Principal components and partial least squares (a supervised version of PC) regression

    \begin{itemize}
    \tightlist
    \item
      \texttt{pls}
    \end{itemize}
  \item
    Non-linear regression

    \begin{itemize}
    \tightlist
    \item
      Polynomials, splines (smoothing splines), generalized additive models
    \item
      \texttt{splines}
    \item
      \texttt{gam}
    \end{itemize}
  \end{itemize}
\item
  K-nearest-neighbors (KNN)

  \begin{itemize}
  \tightlist
  \item
    \texttt{caret}
  \item
    \texttt{class}
  \end{itemize}
\item
  Unsupervised learning

  \begin{itemize}
  \tightlist
  \item
    K-means clustering
  \item
    Hierarchical clustering
  \item
    \texttt{cluster}
  \item
    \texttt{factoextra}
  \end{itemize}
\end{enumerate}

\bibliography{book.bib,packages.bib}


\end{document}
